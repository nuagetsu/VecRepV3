{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b5cc2a-e484-4606-affb-9cd4c817723e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1:  19751\n",
      "index 2:  42855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15550/3353513199.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model/best_model_batch_greyscale_mnistSimpleCNN.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated values\n",
      "\n",
      "scaled NCC:  0.047356367111206055\n",
      "dot product value of model:  0.7805858850479126\n",
      "loss:  0.7332295179367065\n",
      "\n",
      "Model Method -- KNN-IOU score for k = 5\n",
      "vectorb len:  70000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 179\u001b[0m\n\u001b[1;32m    176\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_dataset)):\n\u001b[0;32m--> 179\u001b[0m     input1 \u001b[38;5;241m=\u001b[39m model(input_dataset[j])\n\u001b[1;32m    180\u001b[0m     input2 \u001b[38;5;241m=\u001b[39m model(input_dataset[index1])\n\u001b[1;32m    181\u001b[0m     dot_product_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(input1 \u001b[38;5;241m*\u001b[39m input2, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mMNIST_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 64]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)  \u001b[38;5;66;03m# [B, dimensions]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/modules/pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[1;32m    219\u001b[0m         ceil_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode,\n\u001b[1;32m    220\u001b[0m         return_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices,\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/nn/functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "No longer possible to compute matrix A given 28 dimension inputs\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from line_profiler import profile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import src.data_processing.BruteForceEstimator as bfEstimator\n",
    "import src.visualization.BFmethod as graphing\n",
    "import src.visualization.Metrics as metrics\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "\n",
    "k=5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self, dimensions=32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # [B, 32, 28, 28]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),  # [B, 32, 14, 14]\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),  # [B, 64, 14, 14]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),  # [B, 64, 7, 7]\n",
    "            \n",
    "            #should i do this\n",
    "            nn.AdaptiveAvgPool2d(1)  # [B, 64, 1, 1]\n",
    "        )\n",
    "        self.fc = nn.Linear(64, dimensions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # [B, 64]\n",
    "        x = self.fc(x)  # [B, dimensions]\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "    \n",
    "# ----------------------------------Image Input----------------------------------\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "full_dataset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "\n",
    "# ----------------------------------Preparing the Dataset----------------------------------\n",
    "\n",
    "def loss_fn(A, G):\n",
    "    return torch.norm(A - G, p='fro')  # Frobenius norm\n",
    "\n",
    "index1 = np.random.randint(len(full_dataset))\n",
    "index2 = np.random.randint(len(full_dataset))\n",
    "\n",
    "print(\"index 1: \", index1)\n",
    "print(\"index 2: \", index2)\n",
    "\n",
    "input1 = full_dataset[index1]\n",
    "input2 = full_dataset[index2]\n",
    "\n",
    "model = MNIST_CNN().cuda()\n",
    "model.load_state_dict(torch.load('model/best_model_batch_greyscale_mnistSimpleCNN.pt'))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_dataset = []\n",
    "input_images = [input1, input2] \n",
    "for i in range(len(input_images)):\n",
    "    image,label = input_images[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    img = torch.from_numpy(image_array)\n",
    "    img = img.unsqueeze(0).cuda().double()  #1x1xHxW\n",
    "    # 3 channel for RGB like input\n",
    "    # img = img.repeat(1, 3, 1, 1)  #1x3xHxW\n",
    "    input_dataset.append(img)\n",
    "    \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).cuda().float() for data in input_dataset]\n",
    "\n",
    "NCC_dataset = []\n",
    "for i in range(len(input_images)):\n",
    "    image,label = input_images[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    NCC_dataset.append(image_array)\n",
    "\n",
    "scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "NCC_scaled_value = scale(NCC_dataset[0].squeeze(0), NCC_dataset[1].squeeze(0))\n",
    "print(\"\\nCalculated values\")\n",
    "print(\"\\nscaled NCC: \",NCC_scaled_value)\n",
    "\n",
    "#----------------------Metric 1 - Difference in Values-----------------\n",
    "embedded_vector_image1 = model(input_dataset[0])\n",
    "embedded_vector_image2 = model(input_dataset[1])\n",
    "\n",
    "# print(\"embedded vector for image 1: \", embedded_vector_image1)\n",
    "# print(\"embedded vector for image 2: \", embedded_vector_image2)\n",
    "\n",
    "dot_product_value = torch.sum(embedded_vector_image1 * embedded_vector_image2, dim=1) \n",
    "\n",
    "print(\"dot product value of model: \", dot_product_value.item())\n",
    "\n",
    "NCC_scaled_value = torch.tensor(NCC_scaled_value).to(dot_product_value.device).float()\n",
    "if NCC_scaled_value.ndim == 0:\n",
    "    NCC_scaled_value = NCC_scaled_value.unsqueeze(0)\n",
    "\n",
    "train_loss_value = loss_fn(dot_product_value, NCC_scaled_value) \n",
    "print(\"loss: \", train_loss_value.item())\n",
    "\n",
    "#----------------------Metric 2 - KNNIoU-----------------\n",
    "print(f\"\\nModel Method -- KNN-IOU score for k = {k}\")\n",
    "\n",
    "input_dataset = []\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    image,label = full_dataset[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    img = torch.from_numpy(image_array)\n",
    "    img = img.unsqueeze(0).cuda().double()  #1x1xHxW\n",
    "    # 3 channel for RGB like input\n",
    "    # img = img.repeat(1, 3, 1, 1)  #1x3xHxW\n",
    "    input_dataset.append(img)\n",
    "\n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).cuda().float() for data in input_dataset]\n",
    "# print(\"len(input_dataset): \",len(input_dataset))\n",
    "\n",
    "kscores = []\n",
    "vectorc=[]\n",
    "vectorb = []\n",
    "NCC_dataset = []\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    image,label = full_dataset[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    NCC_dataset.append(image_array)\n",
    "\n",
    "for i in range(len(NCC_dataset)):\n",
    "    scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "    NCC_scaled_value = scale(NCC_dataset[index1].squeeze(0),NCC_dataset[i].squeeze(0))\n",
    "    vectorb.append(NCC_scaled_value)   \n",
    "\n",
    "print(\"vectorb len: \", len(vectorb))\n",
    "\n",
    "# print(\"img1: \", NCC_dataset[index1])\n",
    "# print(\"img2: \", NCC_dataset[index2])\n",
    "\n",
    "# -----------------------------For Plotting of images-----------------------------\n",
    "image1 = np.array(NCC_dataset[index1]).squeeze() \n",
    "image2 = np.array(NCC_dataset[index2]).squeeze()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5)) \n",
    "axes[0].imshow(image1, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(image2, cmap='gray', interpolation='nearest')\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for j in range(len(input_dataset)):\n",
    "    input1 = model(input_dataset[j])\n",
    "    input2 = model(input_dataset[index1])\n",
    "    dot_product_value = torch.sum(input1 * input2, dim=1)\n",
    "    vectorc.append(dot_product_value.detach().cpu().numpy().item())\n",
    "kscore, indices = metrics.get_k_neighbour_score(vectorb, vectorc, k)\n",
    "kscores.append(kscore)\n",
    "\n",
    "indices = set()\n",
    "top_values = sorted(enumerate(vectorb), key=lambda x: x[1], reverse=True)[:k+5]\n",
    "print(f\"\\nTop {k+5} values of Vector b\")\n",
    "for rank, (index, value) in enumerate(top_values, start=1):\n",
    "    print(f\"Rank {rank}: Value = {value}, Index = {index}\")\n",
    "    indices.add(index)\n",
    "    \n",
    "top_values = sorted(enumerate(vectorc), key=lambda x: x[1], reverse=True)[:k+5]\n",
    "print(f\"\\nTop {k+5} values of Vector c\")\n",
    "for rank, (index, value) in enumerate(top_values, start=1):\n",
    "    print(f\"Rank {rank}: Value = {value}, Index = {index}\")\n",
    "    indices.add(index)\n",
    "\n",
    "print(f\"Estimating K-Score for Image {index1}: K-Score = {kscore}\")\n",
    "\n",
    "kscores = []\n",
    "vectorc=[]\n",
    "vectorb = []\n",
    "\n",
    "for i in range(len(NCC_dataset)):\n",
    "    scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "    NCC_scaled_value = scale(NCC_dataset[index2].squeeze(0),NCC_dataset[i].squeeze(0))\n",
    "    vectorb.append(NCC_scaled_value)   \n",
    "        \n",
    "for j in range(len(input_dataset)):\n",
    "    input1 = model(input_dataset[j])\n",
    "    input2 = model(input_dataset[index2])\n",
    "    dot_product_value = torch.sum(input1 * input2, dim=1)\n",
    "    vectorc.append(dot_product_value.detach().cpu().numpy().item())\n",
    "kscore, indices = metrics.get_k_neighbour_score(vectorb, vectorc, k)\n",
    "kscores.append(kscore)\n",
    "          \n",
    "top_values = sorted(enumerate(vectorb), key=lambda x: x[1], reverse=True)[:k+5]\n",
    "print(f\"\\nTop {k+5} values of Vector b\")\n",
    "for rank, (index, value) in enumerate(top_values, start=1):\n",
    "    print(f\"Rank {rank}: Value = {value}, Index = {index}\")\n",
    "    \n",
    "top_values = sorted(enumerate(vectorc), key=lambda x: x[1], reverse=True)[:k+5]\n",
    "print(f\"\\nTop {k+5} values of Vector c\")\n",
    "for rank, (index, value) in enumerate(top_values, start=1):\n",
    "    print(f\"Rank {rank}: Value = {value}, Index = {index}\")\n",
    "          \n",
    "print(f\"Estimating K-Score for Image {index2}: K-Score = {kscore}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4519d56b-8e36-4c8b-bd04-2b63ddc58278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/test/lib/python3.12/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 1:  45869\n",
      "index 2:  29119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20004/944542761.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model/best_model_batch_greyscale_mnistSimpleCNN.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated values\n",
      "\n",
      "scaled NCC:  -0.19893121719360352\n",
      "dot product value of model:  0.3695003092288971\n",
      "loss:  0.5684314966201782\n",
      "\n",
      "Model Method -- KNN-IOU score for k = 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m NCC_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(full_dataset)):\n\u001b[0;32m--> 152\u001b[0m     image,label \u001b[38;5;241m=\u001b[39m full_dataset[i]\n\u001b[1;32m    153\u001b[0m     image_array \u001b[38;5;241m=\u001b[39m (image\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    154\u001b[0m     NCC_dataset\u001b[38;5;241m.\u001b[39mappend(image_array)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/datasets/mnist.py:143\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' \n",
    "No longer possible to compute matrix A given 28 dimension inputs\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from line_profiler import profile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import src.data_processing.BruteForceEstimator as bfEstimator\n",
    "import src.visualization.BFmethod as graphing\n",
    "import src.visualization.Metrics as metrics\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "\n",
    "k=5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self, dimensions=32):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # [B, 32, 28, 28]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),  # [B, 32, 14, 14]\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),  # [B, 64, 14, 14]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),  # [B, 64, 7, 7]\n",
    "            \n",
    "            #should i do this\n",
    "            nn.AdaptiveAvgPool2d(1)  # [B, 64, 1, 1]\n",
    "        )\n",
    "        self.fc = nn.Linear(64, dimensions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # [B, 64]\n",
    "        x = self.fc(x)  # [B, dimensions]\n",
    "        return F.normalize(x, p=2, dim=1)\n",
    "    \n",
    "# ----------------------------------Image Input----------------------------------\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "full_dataset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "\n",
    "# ----------------------------------Preparing the Dataset----------------------------------\n",
    "\n",
    "def loss_fn(A, G):\n",
    "    return torch.norm(A - G, p='fro')  # Frobenius norm\n",
    "\n",
    "index1 = np.random.randint(len(full_dataset))\n",
    "index2 = np.random.randint(len(full_dataset))\n",
    "\n",
    "print(\"index 1: \", index1)\n",
    "print(\"index 2: \", index2)\n",
    "\n",
    "input1 = full_dataset[index1]\n",
    "input2 = full_dataset[index2]\n",
    "\n",
    "model = MNIST_CNN().cuda()\n",
    "model.load_state_dict(torch.load('model/best_model_batch_greyscale_mnistSimpleCNN.pt'))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_dataset = []\n",
    "input_images = [input1, input2] \n",
    "for i in range(len(input_images)):\n",
    "    image,label = input_images[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    img = torch.from_numpy(image_array)\n",
    "    img = img.unsqueeze(0).cuda().double()  #1x1xHxW\n",
    "    # 3 channel for RGB like input\n",
    "    # img = img.repeat(1, 3, 1, 1)  #1x3xHxW\n",
    "    input_dataset.append(img)\n",
    "    \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).cuda().float() for data in input_dataset]\n",
    "\n",
    "NCC_dataset = []\n",
    "for i in range(len(input_images)):\n",
    "    image,label = input_images[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    NCC_dataset.append(image_array)\n",
    "\n",
    "scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "NCC_scaled_value = scale(NCC_dataset[0].squeeze(0), NCC_dataset[1].squeeze(0))\n",
    "print(\"\\nCalculated values\")\n",
    "print(\"\\nscaled NCC: \",NCC_scaled_value)\n",
    "\n",
    "#----------------------Metric 1 - Difference in Values-----------------\n",
    "embedded_vector_image1 = model(input_dataset[0])\n",
    "embedded_vector_image2 = model(input_dataset[1])\n",
    "\n",
    "# print(\"embedded vector for image 1: \", embedded_vector_image1)\n",
    "# print(\"embedded vector for image 2: \", embedded_vector_image2)\n",
    "\n",
    "dot_product_value = torch.sum(embedded_vector_image1 * embedded_vector_image2, dim=1) \n",
    "\n",
    "print(\"dot product value of model: \", dot_product_value.item())\n",
    "\n",
    "NCC_scaled_value = torch.tensor(NCC_scaled_value).to(dot_product_value.device).float()\n",
    "if NCC_scaled_value.ndim == 0:\n",
    "    NCC_scaled_value = NCC_scaled_value.unsqueeze(0)\n",
    "\n",
    "train_loss_value = loss_fn(dot_product_value, NCC_scaled_value) \n",
    "print(\"loss: \", train_loss_value.item())\n",
    "\n",
    "#----------------------Metric 2 - KNNIoU-----------------\n",
    "print(f\"\\nModel Method -- KNN-IOU score for k = {k}\")\n",
    "\n",
    "input_dataset = []\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    image,label = full_dataset[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    img = torch.from_numpy(image_array)\n",
    "    img = img.unsqueeze(0).cuda().double()  #1x1xHxW\n",
    "    # 3 channel for RGB like input\n",
    "    # img = img.repeat(1, 3, 1, 1)  #1x3xHxW\n",
    "    input_dataset.append(img)\n",
    "\n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).cuda().float() for data in input_dataset]\n",
    "# print(\"len(input_dataset): \",len(input_dataset))\n",
    "\n",
    "kscores = []\n",
    "vectorc=[]\n",
    "vectorb = []\n",
    "NCC_dataset = []\n",
    "\n",
    "for i in range(len(full_dataset)):\n",
    "    image,label = full_dataset[i]\n",
    "    image_array = (image.numpy() > 0.5).astype(np.uint8)\n",
    "    NCC_dataset.append(image_array)\n",
    "\n",
    "for i in range(len(NCC_dataset)):\n",
    "    scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "    NCC_scaled_value = scale(NCC_dataset[index1].squeeze(0),NCC_dataset[i].squeeze(0))\n",
    "    vectorb.append(NCC_scaled_value)   \n",
    "\n",
    "print(\"vectorb len: \", len(vectorb))\n",
    "\n",
    "# print(\"img1: \", NCC_dataset[index1])\n",
    "# print(\"img2: \", NCC_dataset[index2])\n",
    "\n",
    "# ===========================For Plotting of images ===========================\n",
    "image1 = np.array(NCC_dataset[index1]).squeeze() \n",
    "image2 = np.array(NCC_dataset[index2]).squeeze()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5)) \n",
    "axes[0].imshow(image1, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(image2, cmap='gray', interpolation='nearest')\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def print_images(indices, intersection_indices, input_images):\n",
    "    \"\"\"Displays multiple images side by side.\"\"\"\n",
    "    images_union = [np.array(input_images[i].squeeze()) for i in indices] \n",
    "    images_intersection = [np.array(input_images[j].squeeze()) for j in intersection_indices] \n",
    "    \n",
    "    print(f\"\\nPlotting images in intersection set: \")\n",
    "    if len(intersection_indices) == 1:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))  # Single Axes object\n",
    "        ax.imshow(images_intersection[0], cmap='gray', interpolation='nearest')\n",
    "        ax.set_title(f\"Image {intersection_indices[0]}\", fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, len(intersection_indices), figsize=(10, 5))  \n",
    "        for j, img in enumerate(images_intersection):\n",
    "            axes[j].imshow(img, cmap='gray', interpolation='nearest')\n",
    "            axes[j].set_title(f\"Image {intersection_indices[j]}\", fontsize=9)\n",
    "            axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig, axes = plt.subplots(1, len(indices), figsize=(10, 5))  \n",
    "    print(f\"\\nPlotting images in union set: \")\n",
    "    for j, img in enumerate(images_union):\n",
    "        axes[j].imshow(img, cmap='gray', interpolation='nearest')\n",
    "        axes[j].set_title(f\"Image {indices[j]}\", fontsize=9)  \n",
    "        axes[j].axis(\"off\") \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for j in range(len(input_dataset)):\n",
    "    input1 = model(input_dataset[j])\n",
    "    input2 = model(input_dataset[index1])\n",
    "    dot_product_value = torch.sum(input1 * input2, dim=1)\n",
    "    vectorc.append(dot_product_value.detach().cpu().numpy().item())\n",
    "kscore, indices, intersection_indices= metrics.get_k_neighbour_score(vectorb, vectorc, k)\n",
    "kscores.append(kscore)\n",
    "print(f\"Estimating K-Score for Image {index1}: K-Score = {kscore}\")\n",
    "\n",
    "for vec, vec_name in [(vectorb, \"b\"), (vectorc, \"c\")]:\n",
    "    top_values = sorted(enumerate(vec), key=lambda x: x[1], reverse=True)[:len(indices)]\n",
    "    print(f\"\\nTop {len(indices)} values of Vector {vec_name}\")\n",
    "    for rank, (i, val) in enumerate(top_values, 1):\n",
    "        print(f\"Rank {rank}: Value = {val}, Index = {i}\")\n",
    "\n",
    "indices = list(indices)\n",
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Index\": indices,\n",
    "    \"Vector b Value\": [vectorb[i] for i in indices],\n",
    "    \"Vector c Value\": [vectorc[i] for i in indices]\n",
    "}).sort_values(by=\"Index\") \n",
    "\n",
    "print(\"\\nComparison between vector c and vector b\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nImages to compare\")\n",
    "print_images(indices, intersection_indices, NCC_dataset)\n",
    "\n",
    "kscores = []\n",
    "vectorc=[]\n",
    "vectorb = []\n",
    "\n",
    "for i in range(len(NCC_dataset)):\n",
    "    scale = ImageProducts.scale_min(ImageProducts.ncc, -1)\n",
    "    NCC_scaled_value = scale(NCC_dataset[index2].squeeze(0),NCC_dataset[i].squeeze(0))\n",
    "    vectorb.append(NCC_scaled_value)   \n",
    "        \n",
    "for j in range(len(input_dataset)):\n",
    "    input1 = model(input_dataset[j])\n",
    "    input2 = model(input_dataset[index2])\n",
    "    dot_product_value = torch.sum(input1 * input2, dim=1)\n",
    "    vectorc.append(dot_product_value.detach().cpu().numpy().item())\n",
    "kscore, indices, intersection_indices = metrics.get_k_neighbour_score(vectorb, vectorc, k)\n",
    "kscores.append(kscore)\n",
    "print(f\"Estimating K-Score for Image {index2}: K-Score = {kscore}\")          \n",
    "for vec, vec_name in [(vectorb, \"b\"), (vectorc, \"c\")]:\n",
    "    top_values = sorted(enumerate(vec), key=lambda x: x[1], reverse=True)[:len(indices)]\n",
    "    print(f\"\\nTop {len(indices)} values of Vector {vec_name}\")\n",
    "    for rank, (i, val) in enumerate(top_values, 1):\n",
    "        print(f\"Rank {rank}: Value = {val}, Index = {i}\")\n",
    "\n",
    "indices = list(indices)\n",
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Index\": indices,\n",
    "    \"Vector b Value\": [vectorb[i] for i in indices],\n",
    "    \"Vector c Value\": [vectorc[i] for i in indices]\n",
    "}).sort_values(by=\"Index\") \n",
    "\n",
    "print(\"\\nComparison between vector c and vector b\")\n",
    "print(df.to_string(index=False))\n",
    "          \n",
    "print(\"\\nImages to compare\")\n",
    "print_images(indices, intersection_indices, NCC_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f4773-f97f-42c9-8747-69dda206c936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
