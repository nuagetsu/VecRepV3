{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108d7887-dcab-4935-b415-2920d5fe909f",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f29194-8193-42b5-9ac2-d9b2735ffdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/VecRepV3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../../VecRepV3\") \n",
    "sys.path.append(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c4123-7d8e-40b5-8a5f-3e657ad699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from line_profiler import profile\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "import src.data_processing.ImageCalculations as imgcalc\n",
    "import src.visualization.ImagePlots as imgplt\n",
    "import src.helpers.ModelUtilities as models\n",
    "import src.data_processing.Utilities as utils\n",
    "import src.helpers.FilepathUtils as Futils\n",
    "import src.data_processing.EmbeddingFunctions as embedfunc\n",
    "\n",
    "from src.visualization import SamplingMethod, BFmethod\n",
    "from src.data_processing.SampleEstimator import SampleEstimator\n",
    "from functools import partial\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers import get_logits_model, PolyphaseInvariantDown2D, LPS\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers.polydown import set_pool\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d4d46-36ff-470e-a012-a31201db2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------Image Input----------------------------------\n",
    "IMAGE_TYPES = [\"NbinMmax_ones\", \"Nbin\", \"triangles\", \"triangle_mean_subtracted\"]\n",
    "\n",
    "IMAGE_FILTERS = [\"unique\", \"Nmax_ones\", \"one_island\"]\n",
    "\n",
    "IMAGE_PRODUCT_TYPES = [\"ncc\", \"ncc_scaled\"]\n",
    "\n",
    "EMBEDDING_TYPES = [\"pencorr_D\"]\n",
    "\n",
    "dimensions = 64\n",
    "imageProductType = \"ncc_scaled_-1\"\n",
    "overwrite = {\"imgSet\": False, \"imgProd\": False, \"embedding\": False}\n",
    "weight = None\n",
    "embeddingType = f\"pencorr_{dimensions}\"\n",
    "k = 7\n",
    "\n",
    "model_directory = \"model\"\n",
    "\n",
    "testSample = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "print(testSample.shape) #Total unique images: 31 449 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47db3a0-060e-4538-a571-6b4e33d9ea4c",
   "metadata": {},
   "source": [
    "# Visualiation and Calculation using 2 random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f17c94-c8f0-421a-be0f-2ece071ead62",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = np.random.randint(len(testSample))\n",
    "index2 = np.random.randint(len(testSample))\n",
    "input1=testSample[index1]\n",
    "input2=testSample[index2]\n",
    "\n",
    "imgplt.plot_original_images(input1, input2, index1, index2)\n",
    "\n",
    "# ------------------------- Preprocessing Dataset ------------------------\n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "\n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "num = len(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f837a-2450-4b5a-9af0-919ddc7e2477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.SimpleCNN6(dimensions=dimensions, padding_mode='circular').to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                                              f'best_model_MNIST_{imageType}_{dimensions}d_convlayer5.pt')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbe338-44c9-487a-a5df-741663761fbe",
   "metadata": {},
   "source": [
    "### Visualisation of Loss score calculation -- Metric 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10228c-7eb7-4619-861e-768c94b38933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------Metric 1 - Loss Calculation-----------------\n",
    "NCC_scaled_value = imgcalc.get_NCC_score(input1, input2)\n",
    "print(\"\\nLoss Calculation\")\n",
    "print(\"\\nScaled NCC: \",NCC_scaled_value)\n",
    "\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "\n",
    "dot_product_value = imgcalc.get_dp_score(model_vectors[index1], model_vectors[index2])\n",
    "\n",
    "print(\"\\nDot product value of model embeddings: \", dot_product_value.item())\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(dot_product_value, NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "\n",
    "dot_product_value = imgcalc.get_dp_score(model_transformed[index1], model_transformed[index2])\n",
    "\n",
    "print(\"\\nDot product value of model transformed embeddings: \", dot_product_value.item())\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(dot_product_value, NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "\n",
    "input1_transformed_normalised =  model_transformed[index1]/np.linalg.norm(model_transformed[index1].detach().cpu().numpy())\n",
    "input2_transformed_normalised =  model_transformed[index2]/np.linalg.norm(model_transformed[index2].detach().cpu().numpy())\n",
    "dot_product_value = imgcalc.get_dp_score(input1_transformed_normalised, input2_transformed_normalised)\n",
    "\n",
    "print(\"\\nDot product value of model transformed normalised embeddings: \", dot_product_value.item())\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(dot_product_value, NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "\n",
    "dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "dot_product_value_Pencorr = dot_product_matrix[index1][index2]\n",
    "difference = abs(dot_product_value_Pencorr - dot_product_value)\n",
    "\n",
    "print(\"\\nDot product value of BF Method embeddings: \", dot_product_value_Pencorr)\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(torch.tensor(dot_product_value_Pencorr), NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "print(\"\\nDifference in values of BF Method and Model Method: \", difference.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8b6ff-d77b-42c2-b94c-ea7ccc985d9b",
   "metadata": {},
   "source": [
    "### Visualisation of KNN-IoU K score calculation -- Metric 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5c82c-09bb-4629-8477-c1f01a2c004b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------Metric 2 - KNNIoU-----------------\n",
    "print(\"----------------\\nBrute Force Method -- KNN-IOU score\\n----------------\")\n",
    "vectorb_bf1 = matrixG[index1]\n",
    "vectorc_bf1 = imgcalc.get_vectorc_brute(index1, matrixA)\n",
    "_, indices = imgplt.display_and_plot_results(vectorb_bf1, vectorc_bf1, \"Brute Force\", index1, k, testSample)\n",
    "\n",
    "top_values_b_1, _ = imgcalc.get_top_scores(vectorb_bf1, len(indices), vectorc_bf1)\n",
    "bottom_values_b_1, _ = imgcalc.get_bottom_scores(vectorb_bf1, len(indices), vectorc_bf1)\n",
    "    \n",
    "vectorb_bf2 = matrixG[index2]\n",
    "vectorc_bf2 = imgcalc.get_vectorc_brute(index2, matrixA)\n",
    "imgplt.display_and_plot_results(vectorb_bf2, vectorc_bf2, \"Brute Force\", index2, k, testSample)\n",
    "\n",
    "top_values_b_2, _ = imgcalc.get_top_scores(vectorb_bf2, len(indices), vectorc_bf2)\n",
    "bottom_values_b_2, _ = imgcalc.get_bottom_scores(vectorb_bf2, len(indices), vectorc_bf2)\n",
    "\n",
    "print(\"----------------\\nModel Method -- KNN-IOU score\\n----------------\")\n",
    "vectorb_model1 = matrixG[index1]\n",
    "vectorc_model1 = imgcalc.get_vectorc_model(index1, model, input_dataset)\n",
    "imgplt.display_and_plot_results(vectorb_model1, vectorc_model1, \"Model\", index1, k, testSample)\n",
    "\n",
    "vectorb_model2 = matrixG[index2]\n",
    "vectorc_model2 = imgcalc.get_vectorc_model(index2, model, input_dataset)\n",
    "imgplt.display_and_plot_results(vectorb_model2, vectorc_model2, \"Model\", index2, k, testSample)\n",
    "\n",
    "print(\"----------------\\nTransformed Model Method -- KNN-IOU score\\n----------------\")\n",
    "input2_transformed_1 =  model_transformed[index1]\n",
    "input2_transformed_2 =  model_transformed[index2]\n",
    "vectorc_transformed_1 = []\n",
    "vectorc_transformed_2 = []\n",
    "for j in range(len(input_dataset)):\n",
    "    input1_transformed =  model_transformed[j]\n",
    "    dot_product_transformed = torch.sum(input1_transformed * input2_transformed_1, dim=1)\n",
    "    vectorc_transformed_1.append(dot_product_transformed.detach().cpu().numpy().item())\n",
    "    \n",
    "    dot_product_transformed = torch.sum(input1_transformed * input2_transformed_2, dim=1)\n",
    "    vectorc_transformed_2.append(dot_product_transformed.detach().cpu().numpy().item())\n",
    "\n",
    "imgplt.display_and_plot_results(vectorb_model1, vectorc_transformed_1, \"Model\", index1, k, testSample)\n",
    "imgplt.display_and_plot_results(vectorb_model2, vectorc_transformed_2, \"Model\", index2, k, testSample)\n",
    "\n",
    "print(\"----------------\\nTransformed Normalised Model Method -- KNN-IOU score\\n----------------\")\n",
    "input2_transformed_normalised_1 =  model_transformed[index1]/np.linalg.norm(model_transformed[index1].detach().cpu().numpy())\n",
    "input2_transformed_normalised_2 =  model_transformed[index2]/np.linalg.norm(model_transformed[index2].detach().cpu().numpy())\n",
    "vectorc_transformed_normalised_1 = []\n",
    "vectorc_transformed_normalised_2 = []\n",
    "for j in range(len(input_dataset)):\n",
    "    input1_transformed_normalised =  model_transformed[j]/np.linalg.norm(model_transformed[j].detach().cpu().numpy())\n",
    "    dot_product_transformed_normalised = torch.sum(input1_transformed_normalised * input2_transformed_normalised_1, dim=1)\n",
    "    vectorc_transformed_normalised_1.append(dot_product_transformed_normalised.detach().cpu().numpy().item())\n",
    "    \n",
    "    dot_product_transformed_normalised = torch.sum(input1_transformed_normalised * input2_transformed_normalised_2, dim=1)\n",
    "    vectorc_transformed_normalised_2.append(dot_product_transformed_normalised.detach().cpu().numpy().item())\n",
    "\n",
    "imgplt.display_and_plot_results(vectorb_model1, vectorc_transformed_normalised_1, \"Model\", index1, k, testSample)\n",
    "imgplt.display_and_plot_results(vectorb_model2, vectorc_transformed_normalised_2, \"Model\", index2, k, testSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58c0d2-c85b-4ee8-a2e6-511e9b7ccb2f",
   "metadata": {},
   "source": [
    "### Visualisation of Relative Squared Difference of **original** matrix and vector embedding -- Metric 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca911d4d-5fbe-4d5d-b8ed-6bcc32f35e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------Before transforming output embedding matrices-----------------------------\n",
    "# ----------- Preprocessing dataset -------------\n",
    "dim = 64\n",
    "embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                 f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_matrix = imgcalc.get_matrix_embeddings(input_dataset, model_vectors)\n",
    "\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "\n",
    "######################################################\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, dot_product_matrix)\n",
    "print(\"Mean Squared Difference of Matrices -- Pencorr (A'A) and NCC (G):\", mean_squared_difference)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, model_matrix.detach().cpu().numpy())\n",
    "print(\"\\nMean Squared Difference of of Matrices -- Model and NCC (G):\", mean_squared_difference)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(dot_product_matrix, model_matrix.detach().cpu().numpy())\n",
    "print(\"Mean Squared Difference of of Matrices -- Model and Pencorr (A'A):\", mean_squared_difference)\n",
    "    \n",
    "print(f\"\\nPrinting matrices\")\n",
    "print(f\"\\nMatrix G: {matrixG}\")\n",
    "print(f\"\\nMatrix A'A (Pencorr): {dot_product_matrix}, {dot_product_matrix.shape}\")\n",
    "print(f\"\\nMatrix A'A (Model): {model_matrix.detach().cpu().numpy()}, {model_matrix.detach().cpu().numpy().shape}\")\n",
    "\n",
    "print(f\"\\nPrinting vectors\")\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "print(f\"Embedding of image {index1} for Model: {model_vectors[index1]}\")\n",
    "\n",
    "print(f\"\\nDifferences between vector embeddings\")\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_vectors[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Model: {mean_squared_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1343089-50e0-4e88-ad81-2f329599ec7c",
   "metadata": {},
   "source": [
    "### Visualisation of Relative Squared Difference of **transformed** matrix and vector embedding using Orthogonal Procrustes -- Metric 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4b7c8-62cc-434b-9f9c-e5cc8d1eb0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----------------After transforming output embedding matrices via Orthorgonal Procrustes ----------------\n",
    "print(f\"Difference between matrices\")\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "model_matrix_transformed = imgcalc.get_matrix_embeddings(input_dataset, model_transformed)\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, model_matrix_transformed.detach().cpu().numpy())\n",
    "print(\"\\nMean Squared Difference of Matrices -- Transformed Model and NCC (G):\", mean_squared_difference)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(dot_product_matrix, model_matrix_transformed.detach().cpu().numpy())\n",
    "print(\"Mean Squared Difference of Matrices -- Transformed Model and Pencorr (A'A):\", mean_squared_difference)\n",
    "\n",
    "print(f\"\\nPrinting matrices after transformation\")\n",
    "print(f\"\\nMatrix G: {matrixG}\")\n",
    "print(f\"\\nMatrix A'A (Pencorr): {dot_product_matrix}, {dot_product_matrix.shape}\")\n",
    "print(f\"\\nMatrix A'A (Transformed Model): {model_matrix_transformed}, error: {error_model}\")\n",
    "\n",
    "print(f\"\\nPrinting vectors \")\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "print(f\"Embedding of image {index1} for Transformed Model: {model_transformed[index1]}\")\n",
    "\n",
    "print(f\"\\nDifferences between vector embeddings\")\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_transformed[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Transformed Model: {mean_squared_difference}\")\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_vectors[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Model: {mean_squared_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10efc57-3c35-48b2-ac71-ba9aa06b83c3",
   "metadata": {},
   "source": [
    "### Visualisation of Relative Squared Difference of **transformed normalised** vector embedding using Orthogonal Procrustes -- Metric 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7d03f-f391-45e4-8c01-c1e92be360f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------After normalising the transformed output embedding matrices via Orthorgonal Procrustes ----------------\n",
    "print(f\"Difference between matrices\")\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "model_matrix_transformed = imgcalc.get_matrix_embeddings(input_dataset, model_transformed)\n",
    "\n",
    "print(f\"\\nPrinting vectors \")\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "print(f\"Embedding of image {index1} for Transformed Normalised Model: {model_transformed[index1]/np.linalg.norm(model_transformed[index1].detach().cpu().numpy())}\")\n",
    "\n",
    "print(f\"\\nDifferences between vector embeddings\")\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_transformed[index1]/np.linalg.norm(model_transformed[index1].detach().cpu().numpy()))\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Transformed Normalised Model: {mean_squared_difference}\")\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_transformed[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Transformed Model: {mean_squared_difference}\")\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_vectors[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Embeddings -- Pencorr (A) and Model: {mean_squared_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37986dc0-0fc2-4b5f-8be1-320d556f58a9",
   "metadata": {},
   "source": [
    "### Visualisation of original vector embeddings of similar images across dimensions -- Metric 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffc079-2a45-4fe5-855f-ca22bb279ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comparison with similar images before transforming output embedding matrices\n",
    "# ----------- Preprocessing dataset -------------\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "dimensions = [32, 64, 128, 256]\n",
    "for dim in dimensions:\n",
    "    print(f\"For dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                     f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    \n",
    "    mean_diff_dim_model = []\n",
    "    mean_diff_dim_pencorr = []\n",
    "    mean_values_dim_model = []\n",
    "    mean_values_dim_pencorr = []\n",
    "    for i in range(len(top_values_b_1)):\n",
    "        top_1_index = top_values_b_1[i][0]\n",
    "        # print(top_1_index)\n",
    "        # print(f\"\\nPrinting vectors for index {index1}\")\n",
    "        # print(f\"Embedding of image {index1} for Model: {model_vectors[index1]}\")    \n",
    "        # print(f\"Embedding of similar image {top_1_index} for Model: {model_vectors[top_1_index]}\")\n",
    "        # print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "        # print(f\"\\nEmbedding of similar image {top_1_index} for Pencorr (A'A): {matrixA[:,top_1_index]}\")\n",
    "\n",
    "        # print(f\"\\nDifferences between vector embeddings\")\n",
    "        mean_squared_difference = imgcalc.get_MSE(model_vectors[top_1_index].detach().cpu().numpy(), model_vectors[index1].detach().cpu().numpy())\n",
    "        # print(f\"Mean Squared Difference of Model between 2 similar images: {mean_squared_difference}\")\n",
    "        mean_diff_dim_model.append(mean_squared_difference)\n",
    "        mean_values_dim_model.append(abs(model_vectors[top_1_index].detach().cpu().numpy()).mean())\n",
    "        \n",
    "        mean_squared_difference = imgcalc.get_MSE(matrixA[:,top_1_index], matrixA[:,index1])\n",
    "        # print(f\"Mean Squared Difference of Pencorr (A'A) between 2 similar images: {mean_squared_difference}\")\n",
    "        mean_diff_dim_pencorr.append(mean_squared_difference)\n",
    "        mean_values_dim_pencorr.append(abs(matrixA[:,top_1_index]).mean())\n",
    "\n",
    "    indices = [x[0] for x in top_values_b_1]\n",
    "    print(indices)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
    "    axes[0].bar(range(len(indices)), mean_diff_dim_model, color='blue', alpha=0.7, label=\"Mean Diff Dim Model\")\n",
    "    axes[0].set_xticks(range(len(indices)))\n",
    "    axes[0].set_xticklabels(indices, rotation=45) \n",
    "    axes[0].set_ylabel(\"Mean Diff Dim Model\")\n",
    "    axes[0].set_title(\"Mean Diff Dim Model vs Top Values B 1\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].bar(range(len(indices)), mean_diff_dim_pencorr, color='red', alpha=0.7, label=\"Mean Diff Dim Pencorr\")\n",
    "    axes[1].set_xticks(range(len(indices)))\n",
    "    axes[1].set_xticklabels(indices, rotation=45) \n",
    "    axes[1].set_xlabel(\"Top Values B 1\")\n",
    "    axes[1].set_ylabel(\"Mean Diff Dim Pencorr\")\n",
    "    axes[1].set_title(\"Mean Diff Dim Pencorr vs Top Values B 1\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    imgplt.plot_score_distribution(mean_diff_dim_model, f\"MSE of Model for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_diff_dim_pencorr, f\"MSE of Pencorr for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_values_dim_model, f\"Average value of embeddings of Model for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_values_dim_pencorr, f\"Average value of embeddings of Pencorr for dimension {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42765d7d-b55f-4c48-8a4a-b915a66559de",
   "metadata": {},
   "source": [
    "### Visualisation of original vector embeddings of different images across dimensions -- Metric 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e0519-e10f-414d-ac48-dda6a10972b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comparison with different images\n",
    "# ----------- Preprocessing dataset -------------\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "dimensions = [32, 64, 128, 256]\n",
    "for dim in dimensions:\n",
    "    print(f\"For dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                     f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    \n",
    "    mean_diff_dim_model = []\n",
    "    mean_diff_dim_pencorr = []\n",
    "    mean_values_dim_model = []\n",
    "    mean_values_dim_pencorr = []\n",
    "    for i in range(len(bottom_values_b_1)):\n",
    "        top_1_index = bottom_values_b_1[i][0]\n",
    "        # print(top_1_index)\n",
    "        # print(f\"\\nPrinting vectors for index {index1}\")\n",
    "        # print(f\"Embedding of image {index1} for Model: {model_vectors[index1]}\")    \n",
    "        # print(f\"Embedding of similar image {top_1_index} for Model: {model_vectors[top_1_index]}\")\n",
    "        # print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "        # print(f\"\\nEmbedding of similar image {top_1_index} for Pencorr (A'A): {matrixA[:,top_1_index]}\")\n",
    "\n",
    "        # print(f\"\\nDifferences between vector embeddings\")\n",
    "        mean_squared_difference = imgcalc.get_MSE(model_vectors[top_1_index].detach().cpu().numpy(), model_vectors[index1].detach().cpu().numpy())\n",
    "        # print(f\"Mean Squared Difference of Model between 2 similar images: {mean_squared_difference}\")\n",
    "        mean_diff_dim_model.append(mean_squared_difference)\n",
    "        mean_values_dim_model.append(abs(model_vectors[top_1_index].detach().cpu().numpy()).mean())\n",
    "        \n",
    "        mean_squared_difference = imgcalc.get_MSE(matrixA[:,top_1_index], matrixA[:,index1])\n",
    "        # print(f\"Mean Squared Difference of Pencorr (A'A) between 2 similar images: {mean_squared_difference}\")\n",
    "        mean_diff_dim_pencorr.append(mean_squared_difference)\n",
    "        mean_values_dim_pencorr.append(abs(matrixA[:,top_1_index]).mean())\n",
    "\n",
    "    indices = [x[0] for x in bottom_values_b_1]\n",
    "    # print(indices)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
    "    axes[0].bar(range(len(indices)), mean_diff_dim_model, color='blue', alpha=0.7, label=\"Mean Diff Dim Model\")\n",
    "    axes[0].set_xticks(range(len(indices)))\n",
    "    axes[0].set_xticklabels(indices, rotation=45) \n",
    "    axes[0].set_ylabel(\"Mean Diff Dim Model\")\n",
    "    axes[0].set_title(\"Mean Diff Dim Model vs Top Values B 1\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].bar(range(len(indices)), mean_diff_dim_pencorr, color='red', alpha=0.7, label=\"Mean Diff Dim Pencorr\")\n",
    "    axes[1].set_xticks(range(len(indices)))\n",
    "    axes[1].set_xticklabels(indices, rotation=45) \n",
    "    axes[1].set_xlabel(\"Top Values B 1\")\n",
    "    axes[1].set_ylabel(\"Mean Diff Dim Pencorr\")\n",
    "    axes[1].set_title(\"Mean Diff Dim Pencorr vs Top Values B 1\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    imgplt.plot_score_distribution(mean_diff_dim_model, f\"MSE of Model for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_diff_dim_pencorr, f\"MSE of Pencorr for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_values_dim_model, f\"Average value of embeddings of Model for dimension {dim}\")\n",
    "    imgplt.plot_score_distribution(mean_values_dim_pencorr, f\"Average value of embeddings of Pencorr for dimension {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e2371-49c9-40b0-8193-055c6bafa89d",
   "metadata": {},
   "source": [
    "# Visualiation and Calculation across dataset, across dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74add7bd-995c-46df-83f2-4241203f7188",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric 1 & 2: Loss and K-Score\n",
    "### Visualisation of loss and k-score across dataset across dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735438e-3fb7-46a7-acc6-c2d66dae599b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------ Loss and Kscores of transformed embedding across dataset -----------------------------\n",
    "dimensions = [32, 64, 128, 256, 512]\n",
    "losses_transformed_normalised = []\n",
    "losses_transformed = []\n",
    "losses_original = []\n",
    "kscores_transformed_normalised = []\n",
    "kscores_transformed = []\n",
    "kscores_original = []\n",
    "losses_BFMethod = []\n",
    "kscores_BFMethod = []\n",
    "\n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "num = len(input_dataset)\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "\n",
    "for dim in dimensions: \n",
    "    print(f\"For dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                     f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "\n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "    model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "\n",
    "    kscore_transformed_normalised = []\n",
    "    kscore_transformed = []\n",
    "    kscore_original = []\n",
    "    \n",
    "    loss_original = []\n",
    "    loss_transformed = []\n",
    "    loss_transformed_normalised = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        vectorb = imgcalc.get_vectorb_model(i, model, testSample)\n",
    "\n",
    "        input2_original = model_vectors[i]\n",
    "        input2_transformed = model_transformed[i]\n",
    "        #since after transformation, its not a unit vector\n",
    "        input2_transformed_normalised =  model_transformed[i]/np.linalg.norm(model_transformed[i].detach().cpu().numpy())\n",
    "        vectorc_original = []\n",
    "        vectorc_transformed = []\n",
    "        vectorc_transformed_normalised = []\n",
    "\n",
    "        loss_value_transformed_normalised = []\n",
    "        loss_value_transformed = []\n",
    "        loss_value_original = []\n",
    "        for j in range(num):\n",
    "            input1_original = model_vectors[j]\n",
    "            #since after transformation, its not a unit vector\n",
    "            input1_transformed = model_transformed[j]\n",
    "            input1_transformed_normalised =  model_transformed[j]/np.linalg.norm(model_transformed[j].detach().cpu().numpy())\n",
    "\n",
    "            dot_product_orignal = torch.sum(input1_original * input2_original, dim=1)\n",
    "            dot_product_transformed = torch.sum(input1_transformed * input2_transformed, dim=1)\n",
    "            dot_product_transformed_normalised = torch.sum(input1_transformed_normalised * input2_transformed_normalised, dim=1)\n",
    "\n",
    "            vectorc_original.append(dot_product_orignal.detach().cpu().numpy().item())\n",
    "            vectorc_transformed.append(dot_product_transformed.detach().cpu().numpy().item())\n",
    "            vectorc_transformed_normalised.append(dot_product_transformed_normalised.detach().cpu().numpy().item())\n",
    "\n",
    "            NCC_scaled_value = imgcalc.get_NCC_score(testSample[i], testSample[j])\n",
    "\n",
    "            loss_value = imgcalc.get_loss_value(dot_product_orignal, NCC_scaled_value) \n",
    "            loss_value_original.append(loss_value)\n",
    "\n",
    "            loss_value = imgcalc.get_loss_value(dot_product_transformed, NCC_scaled_value) \n",
    "            loss_value_transformed.append(loss_value)\n",
    "\n",
    "            loss_value = imgcalc.get_loss_value(dot_product_transformed_normalised, NCC_scaled_value) \n",
    "            loss_value_transformed_normalised.append(loss_value)\n",
    "\n",
    "        average_loss_original = sum(loss_value_original) / len(loss_value_original)\n",
    "        average_loss_transformed = sum(loss_value_transformed) / len(loss_value_transformed)\n",
    "        average_loss_transformed_normalised = sum(loss_value_transformed_normalised) / len(loss_value_transformed_normalised)\n",
    "\n",
    "        loss_original.append(average_loss_original)\n",
    "        loss_transformed.append(average_loss_transformed)\n",
    "        loss_transformed_normalised.append(average_loss_transformed_normalised)\n",
    "\n",
    "        kscore, _, _ = imgcalc.get_kscore_and_sets(vectorb, vectorc_original, k)\n",
    "        kscore_original.append(kscore)\n",
    "\n",
    "        kscore, _, _ = imgcalc.get_kscore_and_sets(vectorb, vectorc_transformed, k)\n",
    "        kscore_transformed.append(kscore)\n",
    "\n",
    "        kscore, _, _ = imgcalc.get_kscore_and_sets(vectorb, vectorc_transformed_normalised, k)\n",
    "        kscore_transformed_normalised.append(kscore)\n",
    "            \n",
    "    kscore_BFMethod, loss_BFMethod, _ = imgcalc.kscore_loss_evaluation_brute(testSample, matrixA, matrixG, k)\n",
    "    \n",
    "    imgplt.plot_score_distribution(loss_BFMethod, \"Loss BFMethod\")\n",
    "    imgplt.plot_score_distribution(loss_original, \"Loss across dataset -- Model original\")\n",
    "    imgplt.plot_score_distribution(loss_transformed, \"Loss across dataset -- Model transformed\")\n",
    "    imgplt.plot_score_distribution(loss_transformed_normalised, \"Loss across dataset -- Model transformed normalised\")\n",
    "    imgplt.plot_score_distribution(kscore_BFMethod, \"K-Score BFMethod\")\n",
    "    imgplt.plot_score_distribution(kscore_original, \"K-score across dataset -- Model original\")\n",
    "    imgplt.plot_score_distribution(kscore_transformed, \"K-score across dataset -- Model transformed\")\n",
    "    imgplt.plot_score_distribution(kscore_transformed_normalised, \"K-score across dataset -- Model transformed normalised\")\n",
    "    \n",
    "    losses_transformed_normalised.append(np.mean(loss_transformed_normalised))\n",
    "    losses_transformed.append(np.mean(loss_transformed))\n",
    "    losses_original.append(np.mean(loss_original))\n",
    "    losses_BFMethod.append(np.mean(loss_BFMethod))\n",
    "    kscores_transformed_normalised.append(np.mean(kscore_transformed_normalised))\n",
    "    kscores_transformed.append(np.mean(kscore_transformed))\n",
    "    kscores_original.append(np.mean(kscore_original))\n",
    "    kscores_BFMethod.append(np.mean(kscore_BFMethod))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524b78b-5904-48f7-81c2-3ae5d6dca275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"BFMethod: Loss and Kscores\")\n",
    "print(\"Kscores: \",kscores_BFMethod, np.argmax(kscores_BFMethod))\n",
    "print(\"Losses: \",losses_BFMethod, np.argmin(losses_BFMethod))\n",
    "\n",
    "print(f\"\\nOriginal Model: Loss and Kscores\")\n",
    "print(\"Kscores: \",kscores_original, np.argmax(kscores_original))\n",
    "print(\"Losses: \",losses_original, np.argmin(losses_original))\n",
    "\n",
    "print(f\"\\nTransformed Model: Loss and Kscores\")\n",
    "print(\"Kscores: \",kscores_transformed, np.argmax(kscores_transformed))\n",
    "print(\"Losses: \",losses_transformed, np.argmin(losses_transformed))\n",
    "\n",
    "print(f\"\\nTransformed Normalized Model: Loss and Kscores\")\n",
    "print(\"Kscores: \",kscores_transformed_normalised, np.argmax(kscores_transformed_normalised))\n",
    "print(\"Losses: \",losses_transformed_normalised, np.argmin(losses_transformed_normalised))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b8f82-3e3e-450b-8c58-f75faae49b1f",
   "metadata": {},
   "source": [
    "## Metric 3: Relative Squared Difference between vector embeddings\n",
    "### Visualisation of Relative Squared Difference between vector embeddings, of original and transformed model using Orthogonal Procrustes, and Pencorr embeddings across dataset across dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffc108-949d-4bdf-9ee3-515a7614065a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------ MSE of embedding across dimensions across dataset -----------------------------\n",
    "MSD_transformed_normalised = []\n",
    "MSD_transformed = []\n",
    "MSD_original = []\n",
    "dimensions = [32, 64, 128, 256, 512]    \n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "num = len(input_dataset)\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "\n",
    "for dim in dimensions:    \n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                     f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "    \n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "    model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "    \n",
    "    # find a fair way to compare across the dimensions,\n",
    "        # maybe can experiment with the different angles \\\n",
    "    MSE_transformed_normalised = []\n",
    "    MSE_transformed = []\n",
    "    MSE_original = []\n",
    "    \n",
    "    print(f\"\\nMSE between vector embeddings of dimension {dim} for Pencorr (A) and Models:\")   \n",
    "    for i in range(num):\n",
    "        normalized_embedding = (model_transformed[i]/np.linalg.norm(model_transformed[i].detach().cpu().numpy())).detach().cpu().numpy()\n",
    "        mean_squared_difference = imgcalc.get_MSE(matrixA[:,i], normalized_embedding)\n",
    "        #print(f\"Original matrix of Index {i}: {mean_squared_difference}\")\n",
    "        MSE_transformed_normalised.append(mean_squared_difference)\n",
    "        \n",
    "        mean_squared_difference = imgcalc.get_MSE(matrixA[:,i], model_transformed[i].detach().cpu().numpy())\n",
    "        #print(f\"Transformed matrix of Index {i}: {mean_squared_difference}\")\n",
    "        MSE_transformed.append(mean_squared_difference)\n",
    "\n",
    "        mean_squared_difference = imgcalc.get_MSE(matrixA[:,i], model_vectors[i].detach().cpu().numpy())\n",
    "        #print(f\"Original matrix of Index {i}: {mean_squared_difference}\")\n",
    "        MSE_original.append(mean_squared_difference)\n",
    "    \n",
    "    MSD_transformed_normalised.append(np.mean(MSE_transformed_normalised))                                              \n",
    "    MSD_transformed.append(np.mean(MSE_transformed))\n",
    "    MSD_original.append(np.mean(MSE_original))\n",
    "    imgplt.plot_score_distribution(MSE_transformed_normalised, f\"MSE of Transformed Normalised Model\")                                              \n",
    "    imgplt.plot_score_distribution(MSE_transformed, f\"MSE of Transformed Model\")\n",
    "    imgplt.plot_score_distribution(MSE_original, f\"MSE of Original Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e91bc6-59be-40c7-97fd-4a82ea32d8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"RSD Original Model: \", MSD_original, np.argmin(MSD_original))\n",
    "print(\"RSD Transformed Model: \", MSD_transformed, np.argmin(MSD_transformed))\n",
    "print(\"RSD Transformed Normalised Model: \", MSD_transformed_normalised, np.argmin(MSD_transformed_normalised))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75057876-d4dc-4bd7-acb3-5ac86c287ba8",
   "metadata": {},
   "source": [
    "## Metric 4: F1 Score\n",
    "### Calculation of F1 Score across dimension using original vector embeddings and Pencorr embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a26ce3-312f-41a4-983b-5a4dea17d31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For model method F1 score calculation per dimension K\n",
    "dimensions = [32, 64, 128, 256, 512]\n",
    "f1_score = []\n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "num = len(input_dataset)\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "\n",
    "for dim in dimensions:    \n",
    "    print(f\"For dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    model = models.SimpleCNN6(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "                     f'best_model_MNIST_{imageType}_{dim}d_convlayer5.pt'), weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    model.eval()\n",
    "    f1_index = []\n",
    "    \n",
    "    tp_index = []\n",
    "    fp_index = []\n",
    "    tn_index = []\n",
    "    fn_index = []\n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    \n",
    "    #matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    \n",
    "    for i in range(num):\n",
    "        top_values_b_1, _ = imgcalc.get_top_scores(matrixG[i], k,  print_results=False)\n",
    "        top_1_index = top_values_b_1[-1][0]\n",
    "        indices = [x[0] for x in top_values_b_1]\n",
    "        #print(i, indices)\n",
    "        MSD_threshold = imgcalc.get_MSE(model_vectors[i].detach().cpu().numpy(), model_vectors[top_1_index].detach().cpu().numpy())\n",
    "        \n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        \n",
    "        for j in range(num):\n",
    "            mean_squared_difference = imgcalc.get_MSE(model_vectors[i].detach().cpu().numpy(), model_vectors[j].detach().cpu().numpy())\n",
    "            predicted = mean_squared_difference <= MSD_threshold\n",
    "            actual = j in indices\n",
    "\n",
    "            if predicted and actual:\n",
    "                TP += 1\n",
    "            elif predicted and not actual:\n",
    "                FP += 1\n",
    "            elif not predicted and actual:\n",
    "                FN += 1\n",
    "            elif not predicted and not actual:\n",
    "                TN += 1\n",
    "    \n",
    "        #print(TP, TN, FP, FN)\n",
    "        tp_index.append(TP)\n",
    "        fp_index.append(FP)\n",
    "        tn_index.append(TN)\n",
    "        fn_index.append(FN)\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1 = f1*100\n",
    "        f1_index.append(f1)\n",
    "\n",
    "    \n",
    "    average_f1_dim = np.mean(f1_index)\n",
    "    f1_score.append(average_f1_dim)\n",
    "    print(np.mean(tp_index), np.mean(fp_index), np.mean(tn_index), np.mean(fn_index))\n",
    "    print(f\"F1 Score: {average_f1_dim:.4f} for dimension {dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed5462-faa3-4055-b141-7f75ab55d805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For BFMEthod F1 score calculation per dimension K\n",
    "dimensions = [32, 64, 128, 256, 512]\n",
    "f1_score = []\n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "num = len(input_dataset)\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "\n",
    "for dim in dimensions:    \n",
    "    print(f\"For dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "\n",
    "    # model = models.SimpleCNN3(dimensions=dim, padding_mode='circular').to(device)\n",
    "    # model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), model_directory, \n",
    "    #                  f'best_model_{imageType}_{dim}d_convlayer2.pt'), weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "    # model.eval()\n",
    "    f1_index = []\n",
    "    \n",
    "    tp_index = []\n",
    "    fp_index = []\n",
    "    tn_index = []\n",
    "    fn_index = []\n",
    "    #model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    \n",
    "    matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "    \n",
    "    for i in range(num):\n",
    "        top_values_b_1, _ = imgcalc.get_top_scores(matrixG[i], k,  print_results=False)\n",
    "        top_1_index = top_values_b_1[-1][0]\n",
    "        indices = [x[0] for x in top_values_b_1]\n",
    "        #print(i, indices)\n",
    "        MSD_threshold = imgcalc.get_MSE(matrixA[:,i], matrixA[:,top_1_index])\n",
    "        \n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        \n",
    "        for j in range(num):\n",
    "            mean_squared_difference = imgcalc.get_MSE(matrixA[:,i], matrixA[:,j])\n",
    "            predicted = mean_squared_difference <= MSD_threshold\n",
    "            actual = j in indices\n",
    "\n",
    "            if predicted and actual:\n",
    "                TP += 1\n",
    "            elif predicted and not actual:\n",
    "                FP += 1\n",
    "            elif not predicted and actual:\n",
    "                FN += 1\n",
    "            elif not predicted and not actual:\n",
    "                TN += 1\n",
    "    \n",
    "        #print(TP, TN, FP, FN)\n",
    "        tp_index.append(TP)\n",
    "        fp_index.append(FP)\n",
    "        tn_index.append(TN)\n",
    "        fn_index.append(FN)\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1 = f1*100\n",
    "        f1_index.append(f1)\n",
    "\n",
    "    \n",
    "    average_f1_dim = np.mean(f1_index)\n",
    "    f1_score.append(average_f1_dim)\n",
    "    print(np.mean(tp_index), np.mean(fp_index), np.mean(tn_index), np.mean(fn_index))\n",
    "    print(f\"F1 Score: {average_f1_dim:.4f} for dimension {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6bc26-2904-4803-8437-8584968ea94a",
   "metadata": {},
   "source": [
    "# Obtaining the optimal dimension using 4 different metrics above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316dd64-4178-42f5-8155-523b73ea91af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dimensions = [32, 64, 128, 256, 512]\n",
    "\n",
    "\n",
    "metric1 = np.argmax(kscores_original)\n",
    "metric2 = np.argmin(losses_original)\n",
    "metric3A = np.argmin(MSD_original)\n",
    "metric4 = np.argmax(f1_score)\n",
    "\n",
    "metrics = [\n",
    "    dimensions[metric1],\n",
    "    dimensions[metric2],\n",
    "    dimensions[metric3A],\n",
    "    dimensions[metric4]\n",
    "]\n",
    "\n",
    "optimal_dim = Counter(metrics).most_common(1)[0][0]\n",
    "print(\"Optimal Dimension:\", optimal_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8dd42-a69a-44d9-8837-118c67a64551",
   "metadata": {},
   "source": [
    "### Using normalized embedding validation loss to determine optimal dimension.\n",
    "#### Existing strategies for the selection of the embedding dimension rely on performance maximization in downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3dfb6-12b7-4ee0-a86a-802a5128dd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_optimal_dim(losses, eps=0.05):  \n",
    "    \"\"\"\n",
    "    Finds the optimal dimension based on the embedding loss function.\n",
    "\n",
    "    :param losses: Dictionary {dimension: loss value}\n",
    "    :param eps: Threshold percentage (default 0.05)\n",
    "    :return: The optimal dimension\n",
    "    \"\"\"\n",
    "    L_inf = min(losses.values())\n",
    "\n",
    "    for d in sorted(losses.keys()): \n",
    "        if L_inf == 0 or (losses[d] - L_inf) / L_inf <= eps:  \n",
    "            return d  \n",
    "\n",
    "    return max(losses.keys()) \n",
    "\n",
    "losses = {8: 0.17436, 16: 0.10059, 32: 0.08990, 64: 0.09155, 128: 0.09328, 256: 0.0636}\n",
    "optimal_dim = find_optimal_dim(losses)\n",
    "print(\"Optimal Dimension:\", optimal_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dee692-cbc9-4eaf-9248-4068baf5bfd8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------- for debugging -----------------\n",
    "tolerance = 1e-6\n",
    "matrixG = imgcalc.get_matrixG(testSample, imageProductType)\n",
    "\n",
    "rank = np.linalg.matrix_rank(matrixG)\n",
    "print(\"Rank of the matrixG:\", rank)\n",
    "\n",
    "nDim = 64\n",
    "# original image product matrix not PSD, after transformation,\n",
    "matrixGprime = embedfunc.pencorr(matrixG, nDim) #reduce matrixG rank from image set size to nDim\n",
    "\n",
    "print(\"g pirme\")\n",
    "rank = np.linalg.matrix_rank(matrixGprime) #156\n",
    "''' \n",
    "given 753 unique training samples, rank of matrixG is by right 753 \n",
    "ndim 512: rank of g' = 120, 120 positive eigenvalues for g', 380 positive eigenvalues for g\n",
    "ndim 256: rank of g' = 120, 120 positive eigenvalues for g', 380 positive eigenvalues for g\n",
    "ndim 64: rank of g' = 64, 64 positive eigenvalues for g', 380 positive eigenvalues for g\n",
    "\n",
    "somewhere in the algorithm, it is coded such that pencorr striaghtaway finds the optimum rank of a matrix and minimises it\n",
    "once rank is minimised to its optimum, the number of positive eigenvalues is only that number.\n",
    "unless we set nDim < rank, we cannot change the number of positive eigenvalues.\n",
    "this overall limits the number of dimensions we can test. \n",
    "\n",
    "essentially, since rank of g' determines the vector space formed by the rows and columns (linearly independent rows or columns in matrix),\n",
    "the rank of a matrix is the number of non-zero eigenvalues of a matrix, which is why limiting the rank of the matrix to 120 only give\n",
    "120 non-zero elements in our embedding dimension. \n",
    "'''\n",
    "print(\"Rank of the matrixG':\", rank)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrixGprime)\n",
    "print(matrixGprime.shape) #753x753\n",
    "print(eigenvalues.shape) #753\n",
    "print(eigenvectors.shape) #753x753\n",
    "num_positive_eigenvalues = np.sum(eigenvalues > tolerance) \n",
    "print(\"Number of eigenvalues greater than tolerance:\", num_positive_eigenvalues)\n",
    "\n",
    "matrixA = imgcalc.get_matrixA(matrixG, embeddingType, weight)\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]} {matrixA.shape} {matrixA[:,index1].size}\")\n",
    "count_not_close_to_zero = np.sum(np.abs(matrixA[:,index1]) > tolerance)\n",
    "print(\"Number of elements not close to tolerance 1e-5:\", count_not_close_to_zero)\n",
    "\n",
    "count_to_zero = np.sum(np.abs(matrixA[:,index1]) > 0)\n",
    "print(\"Number of elements not close to 0:\", count_to_zero)\n",
    "\n",
    "print(\"g\")\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrixG)\n",
    "print(matrixG.shape) #753x753\n",
    "print(eigenvalues.shape) #753\n",
    "print(eigenvectors.shape)\n",
    "num_positive_eigenvalues = np.sum(eigenvalues > tolerance) #380 ????????\n",
    "print(\"Number of eigenvalues greater than tolerance:\", num_positive_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf648c-4b10-4041-8d4c-7fdceee6dac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
