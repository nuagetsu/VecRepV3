{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec884e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../\")\n",
    "sys.path.append(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6552a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from line_profiler import profile\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "\n",
    "import src.helpers.MetricUtilities as metrics\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "from functools import partial\n",
    "\n",
    "from mtree.mtree import MTree\n",
    "import mtree.mtree as mtree\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../\")\n",
    "sys.path.append(path)\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import src.helpers.MetricUtilities as metrics\n",
    "\n",
    "from src.helpers.MTreeUtilities import getKNearestNeighbours, getMTree, getMTreeFFT, getMTreeFFTNumba\n",
    "from src.data_processing.DatasetGetter import get_data, get_data_MStar, get_data_SARDet_100k, get_data_ATRNetSTARAll\n",
    "\n",
    "\n",
    "import mtree.mtree as mtree\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "def mtree_ncc_query_sample_size(max_node_size=12, image_size=32, k=7, runs=2, sample_sizes=[1]):\n",
    "    total_time_ncc = 0\n",
    "    total_time_mtree = 0\n",
    "    avg_times_ncc = []\n",
    "    avg_times_mtree = []\n",
    "\n",
    "    data = get_data_MStar(image_size)\n",
    "\n",
    "    print(f\"Average runtime of querying mtree and ncc for {k} NN over {runs} runs with image size {image_size} and max node size {max_node_size} and variable sample size\")\n",
    "    for i in range(len(sample_sizes)):\n",
    "        print(f\"NOW TRYING sample size: {sample_sizes[i]}\")\n",
    "        total_time_ncc = 0\n",
    "        total_time_mtree = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "        testSample = [item[0] for item in sampled_test_data]\n",
    "        \n",
    "\n",
    "        tree = getMTree(testSample, max_node_size)\n",
    "\n",
    "        # trans = transforms.Compose([transforms.Resize(img_sizes[i])])\n",
    "        # t_MNIST_data = trans(MNIST_data)\n",
    "\n",
    "        # for img in MNIST_data:\n",
    "        #     img = trans(img)\n",
    "\n",
    "        for _ in range(runs):\n",
    "            index1 = np.random.randint(len(data))\n",
    "            #input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "            unseen_image = data[index1][0]\n",
    "\n",
    "            start_time = time.time()\n",
    "            arr = []\n",
    "            for j in range(len(testSample)):\n",
    "                result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "                arr.append(result)\n",
    "            \n",
    "            unseen_img_arr = np.array(arr)\n",
    "            #print(unseen_img_arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time_ncc += end_time - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            imgs = getKNearestNeighbours(tree, unseen_image, k+1)\n",
    "            end_time = time.time()\n",
    "            total_time_mtree += end_time - start_time\n",
    "\n",
    "        avg_ncc = total_time_ncc / runs\n",
    "        avg_mtree = total_time_mtree / runs\n",
    "        avg_times_ncc.append(avg_ncc)\n",
    "        avg_times_mtree.append(avg_mtree)\n",
    "\n",
    "        # with open(\"test_mtree_query_sample_sizes_avg_times_ncc_.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_ncc)}\")\n",
    "        \n",
    "        # with open(\"test_mtree_query_sample_sizes_avg_times_mtree_.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_mtree)}\")\n",
    "\n",
    "        print(f\"Average runtime of ncc search: {avg_ncc:.6f} seconds\")\n",
    "        print(f\"Average runtime of mtree search: {avg_mtree:.6f} seconds\")\n",
    "    # plot_data_mtree_ncc(x_axis=\"Sample size\", title=f\"Average runtime of finding {k} neighbours with image size {image_size} against sample sizes\", filename=\"test_mtree_query_sample_sizes.png\", \n",
    "    #                     varied_arr=sample_sizes, data1=avg_times_ncc, data2=avg_times_mtree, max_node_size=max_node_size)\n",
    "\n",
    "\n",
    "def mtree_init_sample_size(max_node_size=12, image_size=32, k=7, runs=2, sample_sizes=[1]):\n",
    "    total_time_mtree_init = 0\n",
    "    avg_times_mtree_init = []\n",
    "\n",
    "    data = get_data_MStar(image_size)\n",
    "\n",
    "    print(f\"Average runtime of initialising mtree over {runs} runs with max node size {max_node_size} and image size {image_size} and variable sample sizes\")\n",
    "\n",
    "    for i in range(len(sample_sizes)):\n",
    "        print(f\"NOW TRYING sample size: {sample_sizes[i]}\")\n",
    "        total_time_mtree_init = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "        testSample = [item[0] for item in sampled_test_data]\n",
    "        #sampled_test_data[:,0]\n",
    "\n",
    "        for _ in range(runs):\n",
    "            start_time = time.time()\n",
    "            tree = getMTree(testSample, max_node_size)\n",
    "            end_time = time.time()\n",
    "            total_time_mtree_init += end_time - start_time\n",
    "            print(f\"Finished one run of mtree init...\")\n",
    "\n",
    "        \n",
    "        avg_mtree_init = total_time_mtree_init / runs\n",
    "        avg_times_mtree_init.append(avg_mtree_init)\n",
    "\n",
    "        \n",
    "        # with open(\"test_mtree_init_sample_sizes_avg_times_mtree_init.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_mtree_init)}\")\n",
    "\n",
    "        print(f\"Average runtime of mtree init: {avg_mtree_init:.6f} seconds\")\n",
    "    \n",
    "    # plot_data_mtree_init(x_axis=\"Sample size\", title=f\"Average runtime of initialising mtree with image size {image_size}, max node size {max_node_size} against sample sizes\", filename=\"test_mtree_init_sample_sizes.png\", \n",
    "    #                     varied_arr=sample_sizes, data=avg_times_mtree_init, max_node_size=max_node_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5946879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9466 samples\n",
    "# note MStar is 368 by 368...\n",
    "\n",
    "mtree_ncc_query_sample_size(max_node_size=15, image_size=368, k=7, runs=10, sample_sizes=[10, 100, 1000, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/SARDet_100k/SARDet_100K/Annotations/val.json\") as val_annotations:\n",
    "    mappings = json.load(val_annotations)\n",
    "\n",
    "print(mappings[\"images\"][0])\n",
    "print(len(mappings[\"images\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a10d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@inproceedings{li2024sardet100k,\n",
    "\ttitle={SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection}, \n",
    "\tauthor={Yuxuan Li and Xiang Li and Weijie Li and Qibin Hou and Li Liu and Ming-Ming Cheng and Jian Yang},\n",
    "\tyear={2024},\n",
    "\tbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},\n",
    "}\n",
    "\n",
    "@article{zhang2025rsar,\n",
    "  title={RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark},\n",
    "  author={Zhang, Xin and Yang, Xue and Li, Yuxuan and Yang, Jian and Cheng, Ming-Ming and Li, Xiang},\n",
    "  journal={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{dai2024denodet,\n",
    "\ttitle={DenoDet: Attention as Deformable Multi-Subspace Feature Denoising for Target Detection in SAR Images},\n",
    "\tauthor={Dai, Yimian and Zou, Minrui and Li, Yuxuan and Li, Xiang and Ni, Kang and Yang, Jian},\n",
    "\tjournal={arXiv preprint arXiv:2406.02833},\n",
    "\tyear={2024}\n",
    "}\n",
    "'''\n",
    "import json\n",
    "# TODO look at train/val/test.json format and extract out the category id and the id to class mapping at the v end.\n",
    "\n",
    "\n",
    "class CustomDatasetSARDet_100k(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        with open(\"../data/SARDet_100k/SARDet_100K/Annotations/test.json\") as annotations:\n",
    "            test_mapping = json.load(annotations)\n",
    "        with open(\"../data/SARDet_100k/SARDet_100K/Annotations/train.json\") as annotations:\n",
    "            train_mapping = json.load(annotations)\n",
    "        with open(\"../data/SARDet_100k/SARDet_100K/Annotations/val.json\") as annotations:\n",
    "            val_mapping = json.load(annotations)\n",
    "        \n",
    "        self.imgs_path = \"../data/SARDet_100k/SARDet_100K/JPEGImages/\"\n",
    "\n",
    "        test_path = self.imgs_path + \"test/\"\n",
    "        train_path = self.imgs_path + \"train/\"\n",
    "        val_path = self.imgs_path + \"val/\"\n",
    "\n",
    "        for i in range(len(test_mapping[\"images\"])):\n",
    "            img_path = test_path + test_mapping[\"images\"][i][\"file_name\"]\n",
    "            cat_id = test_mapping[\"annotations\"][i][\"category_id\"]\n",
    "            cat_name = test_mapping[\"categories\"][int(cat_id)][\"name\"]\n",
    "            self.data.append([img_path, cat_name])\n",
    "\n",
    "        for i in range(len(train_mapping[\"images\"])):\n",
    "            img_path = train_path + train_mapping[\"images\"][i][\"file_name\"]\n",
    "            cat_id = train_mapping[\"annotations\"][i][\"category_id\"]\n",
    "            cat_name = train_mapping[\"categories\"][int(cat_id)][\"name\"]\n",
    "            self.data.append([img_path, cat_name])\n",
    "        \n",
    "        for i in range(len(val_mapping[\"images\"])):\n",
    "            img_path = val_path + val_mapping[\"images\"][i][\"file_name\"]\n",
    "            cat_id = val_mapping[\"annotations\"][i][\"category_id\"]\n",
    "            cat_name = val_mapping[\"categories\"][int(cat_id)][\"name\"]\n",
    "            self.data.append([img_path, cat_name])\n",
    "\n",
    "        # file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        # self.data = []\n",
    "\n",
    "        # # with open(\"../data/SARDet_100k/SARDet_100K/mapping.json\") as annotations:\n",
    "        # #     mappings = json.load(annotations)\n",
    "\n",
    "        # for dir_path in file_list:\n",
    "        #     for img_path in glob.glob(dir_path + \"/*.png\"):\n",
    "        #         # self.data.append([img_path, mappings[img_path.split(\"/\")[-1]]])\n",
    "        #         self.data.append([img_path, \"yippee\"])\n",
    "        #     for img_path in glob.glob(dir_path + \"/*.jpg\"):\n",
    "        #         # self.data.append([img_path, mappings[img_path.split(\"/\")[-1]]])\n",
    "        #         self.data.append([img_path, \"yippee\"])\n",
    "        \n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        # class_id = self.class_map[data_path[1]]\n",
    "        class_id = data_path[1]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), class_id\n",
    "\n",
    "def get_data_SARDet_100k(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDatasetSARDet_100k(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83143d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('/home/jovyan/data/ATRNet-STAR/EOC_azimuth/test/Buick_Excelle_GT/KU_HH_15_80_325790.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"Root tag:\", root.tag)\n",
    "\n",
    "\n",
    "img_path = root.find('filename').text\n",
    "broad_class = root.find('object').find('class').text\n",
    "subclass = root.find('object').find('subclass').text\n",
    "subclass_type = \"_\".join(root.find('object').find('type').text.split(\"_\")[1:])\n",
    "\n",
    "print(img_path)\n",
    "print(broad_class)\n",
    "print(subclass)\n",
    "print(subclass_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd774e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "misc{liu2025atrnet,\n",
    "    title={{ATRNet-STAR}: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild}, \n",
    "    author={Yongxiang Liu and Weijie Li and Li Liu and Jie Zhou and Bowen Peng and Yafei Song and Xuying Xiong and Wei Yang and Tianpeng Liu and Zhen Liu and Xiang Li},\n",
    "    year={2025},\n",
    "    eprint={2501.13354},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.CV},\n",
    "    url={https://arxiv.org/abs/2501.13354},\n",
    "}\n",
    "\n",
    "@ARTICLE{li2025saratr,\n",
    "  author={Li, Weijie and Yang, Wei and Hou, Yuenan and Liu, Li and Liu, Yongxiang and Li, Xiang},\n",
    "  journal={IEEE Transactions on Image Processing}, \n",
    "  title={SARATR-X: Toward Building a Foundation Model for SAR Target Recognition}, \n",
    "  year={2025},\n",
    "  volume={34},\n",
    "  number={},\n",
    "  pages={869-884},\n",
    "  doi={10.1109/TIP.2025.3531988}}\n",
    "\n",
    "@ARTICLE{li2024predicting,\n",
    "  title = {Predicting gradient is better: Exploring self-supervised learning for SAR ATR with a joint-embedding predictive architecture},\n",
    "  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},\n",
    "  volume = {218},\n",
    "  pages = {326-338},\n",
    "  year = {2024},\n",
    "  issn = {0924-2716},\n",
    "  doi = {https://doi.org/10.1016/j.isprsjprs.2024.09.013},\n",
    "  url = {https://www.sciencedirect.com/science/article/pii/S0924271624003514},\n",
    "  author = {Li, Weijie and Yang, Wei and Liu, Tianpeng and Hou, Yuenan and Li, Yuxuan and Liu, Zhen and Liu, Yongxiang and Liu, Li}}\n",
    "\n",
    "Downloads last month\n",
    "167\n",
    "System\n",
    "'''\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetATRNetSTAR(Dataset):\n",
    "    def __init__(self, transform=None, p_left=0.8, p_right=0.2):\n",
    "        # Wall_poer real name is j poer. J for easier classname extraction... and cheetah and voleex\n",
    "        self.type_names = [\"Excelle_GT\", \"GL8\", \"CS75_Plus\", \"Starlight_4500\", \"Cheetah_CFA6473C\", \"8228-5\", \"Arrizo 5\", \"qq3\", \"Blazer_1998\", \"HOWO\", \"Duolika\", \"EQ6608LTV\", \"Forthing_Lingzhi\",\n",
    "        \"Tianjin_DFH2200B\", \"Tianjin_KR230\", \"J6P\", \"Jiabao_T51\", \"BJ1045V9JB5-54\", \"Wall_poer\", \"Wall_Voleex_C50\", \"EV160B\", \"CA7180A3E\", \"h5\", \"N1\", \"HLF25_II\", \"Junling\", \"Patriot\", \n",
    "        \"SY5033XJH\", \"MKC\", \"Proud_2009\", \"V80\", \"Outlander_2003\", \"ZL40F\", \"DeLong_M3000\", \"DeLong_X3000\", \"Aochi_1800\", \"Aochi_Hongrui\", \"Rongguang_V\", \"YZK6590XCA\", \"ZK6120HY1\"]\n",
    "        self.categories = {}\n",
    "        for i in range(len(self.type_names)):\n",
    "            self.categories[self.type_names[i]] = []\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.test = []\n",
    "        # Check if I can test the azimuths.... if not j try smt else idk\n",
    "        self.imgs_path = \"/home/jovyan/data/ATRNet-STAR/EOC_azimuth/\"\n",
    "        #file_list = [glob.glob(self.imgs_path + \"*\")]\n",
    "        file_list = [self.imgs_path + \"test_300/\"]\n",
    "        print(file_list)\n",
    "        for dir_path in file_list:\n",
    "            #print(dir_path)\n",
    "            for cat in glob.glob(dir_path + \"/*\"):\n",
    "                #print(cat)\n",
    "                for xml_file_path in glob.glob(cat + \"/*.xml\"):\n",
    "                    #print(xml_file_path)\n",
    "                    tree = ET.parse(xml_file_path)\n",
    "                    root = tree.getroot()\n",
    "                    img_path = root.find('filename').text\n",
    "                    broad_class = root.find('object').find('class').text\n",
    "                    subclass = root.find('object').find('subclass').text\n",
    "                    subclass_type = \"_\".join(root.find('object').find('type').text.split(\"_\")[1:])\n",
    "                    chip_data = [img_path, broad_class, subclass, subclass_type] \n",
    "                    # adding chipdata to the dictionary entry w that class name\n",
    "                    self.categories[subclass_type] = self.categories[subclass_type] + chip_data\n",
    "                print(self.categories)\n",
    "                    # self.data.append(chip_data)\n",
    "\n",
    "        print(self.categories)\n",
    "        self.data, self.test = self.split_data(p_left, p_right)\n",
    "\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data) + len(self.test)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        # class_id = self.class_map[data_path[1]]\n",
    "        broad_class = data_path[1]\n",
    "        subclass = data_path[2]\n",
    "        subclass_type = data_path[3]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), broad_class, subclass, subclass_type\n",
    "    \n",
    "    def split_data(self, p_left, p_right):\n",
    "        data = [[\"head\", \"head\", \"head\", \"head\"]]\n",
    "        test = [[\"head\", \"head\", \"head\", \"head\"]]\n",
    "        for key in self.categories:\n",
    "            entry = np.array(self.categories[key])\n",
    "            print(entry)\n",
    "            print(type(entry))\n",
    "            left_len = math.floor(p_left * len(entry))\n",
    "            right_len = math.ceil(p_right * len(entry))\n",
    "            sample_indices = np.array(random.sample(range(len(entry)), left_len))\n",
    "            print(sample_indices)\n",
    "            # sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "            mask = np.ones(len(entry), np.bool)\n",
    "            mask[sample_indices] = 0\n",
    "            # These are all wrong omg ded\n",
    "            left_partition = entry[sample_indices]\n",
    "            right_partition = entry[mask]\n",
    "            print(left_partition)\n",
    "            print(right_partition)\n",
    "            data += left_partition\n",
    "            test += right_partition\n",
    "            # other_indices = np.arange(len(entry))[mask]\n",
    "            # left_partition = Subset(entry, sample_indices)\n",
    "            # right_partition = Subset(entry, )\n",
    "            # other_data = data[mask]\n",
    "        \n",
    "        return data, test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_ATRNetSTAR(size, p_left=0.8, p_right=0.2):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDatasetATRNetSTAR(transform, p_left, p_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07abb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_ATRNetSTAR(100, 0.8, 0.2)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53085e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_SARDet_100k(300)\n",
    "print(len(data))\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "classes = [item[1] for item in sampled_test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00583247",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = np.random.randint(len(testSample))\n",
    "#index2 = np.random.randint(len(testSample))\n",
    "#input2=input_dataset[index2][0].squeeze().to('cpu')\n",
    "print(classes[index1])\n",
    "plt.imshow(testSample[index1], cmap='grey')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k=7\n",
    "data = get_data_MStar(368)\n",
    "print(len(data))\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "classes = [item[1] for item in sampled_test_data]\n",
    "\n",
    "# class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "tree = getMTree(testSample, 12)\n",
    "\n",
    "index1 = random.choice(sample_indices)\n",
    "#input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "unseen_image = data[index1][0]\n",
    "unseen_image_class = data[index1][1]\n",
    "\n",
    "arr = []\n",
    "for j in range(len(testSample)):\n",
    "    result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "    arr.append(result)\n",
    "\n",
    "unseen_img_arr = np.array(arr)\n",
    "#print(unseen_img_arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "\n",
    "num_same_class = 0\n",
    "\n",
    "print(f\"Unseen img class: {unseen_image_class}\")\n",
    "for i in imgProd_max_index:\n",
    "    img_class = classes[i]\n",
    "    print(f\"Index {i}, Class {img_class}\")\n",
    "    if (img_class == unseen_image_class):\n",
    "        num_same_class += 1\n",
    "\n",
    "print(num_same_class)\n",
    "\n",
    "\n",
    "imgs = getKNearestNeighbours(tree, unseen_image, k+1)\n",
    "\n",
    "\n",
    "def imgs_to_indices(img_arr, testSample):\n",
    "    ind_arr = []\n",
    "    for img in img_arr:\n",
    "        for i in range(len(testSample)):\n",
    "            if (metrics.distance(img, testSample[i]) < 0.00001):\n",
    "                ind_arr.append(i)\n",
    "                break\n",
    "    \n",
    "    return ind_arr\n",
    "\n",
    "ind_arr = imgs_to_indices(imgs, testSample)\n",
    "print(ind_arr)\n",
    "print(imgProd_max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def ncc(mainImg, tempImg) -> float:\n",
    "    \"\"\"\n",
    "    :param mainImg: Main image to be scanned\n",
    "    :param tempImg: Template image to be scanned over the main\n",
    "    :return: Max value of the ncc\n",
    "\n",
    "    Applies NCC of the template image over the main image and returns the max value obtained.\n",
    "    When the template image kernel exceeds the bounds, wraps to the other side of the main image\n",
    "    \"\"\"\n",
    "    if np.count_nonzero(mainImg) == 0:\n",
    "        if np.count_nonzero(tempImg) == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    mainImg = np.pad(mainImg, max(len(mainImg), len(mainImg[0])),\n",
    "                     'wrap')  # Padding the main image with wrapped values to simulate wrapping\n",
    "\n",
    "    mainImg = np.asarray(mainImg, np.single)  # Setting data types of array\n",
    "    tempImg = np.asarray(tempImg, np.single)\n",
    "\n",
    "    corr = cv2.matchTemplate(mainImg, tempImg, cv2.TM_CCORR_NORMED)\n",
    "\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(corr)\n",
    "\n",
    "    return max_val\n",
    "\n",
    "def ncc_scaled(mainImg, tempImg) -> float:\n",
    "    \"\"\"\n",
    "    :param mainImg: Main image to be scanned\n",
    "    :param tempImg: Template image to be scanned over the main\n",
    "    :return: Max value of the ncc, with scaled bounds of [-1,1]\n",
    "    \"\"\"\n",
    "    return ncc(mainImg, tempImg) * 2 - 1\n",
    "\n",
    "\n",
    "class CustomDatasetMStar(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"../data/mstar/Padded_imgs/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.JPG\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        #print(self.data)\n",
    "        self.class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        class_id = self.class_map[data_path[1]]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), class_id\n",
    "\n",
    "k=7\n",
    "data = get_data_MStar(16)\n",
    "print(len(data))\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "classes = [item[1] for item in sampled_test_data]\n",
    "\n",
    "# class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "tree = getMTreeFFTNumba(testSample, 12)\n",
    "tree_mst = getMTreeFFTNumba(testSample, 12, promote=mtree.MST_promotion, partition=mtree.MST_partition)\n",
    "\n",
    "index1 = random.choice(sample_indices)\n",
    "#input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "unseen_image = data[index1][0]\n",
    "unseen_image_class = data[index1][1]\n",
    "\n",
    "arr = []\n",
    "for j in range(len(testSample)):\n",
    "    result = ncc_scaled(testSample[j], unseen_image)\n",
    "    arr.append(result)\n",
    "\n",
    "unseen_img_arr = np.array(arr)\n",
    "#print(unseen_img_arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "\n",
    "num_same_class = 0\n",
    "\n",
    "print(f\"Unseen img class: {unseen_image_class}\")\n",
    "for i in imgProd_max_index:\n",
    "    img_class = classes[i]\n",
    "    print(f\"Index {i}, Class {img_class}\")\n",
    "    if (img_class == unseen_image_class):\n",
    "        num_same_class += 1\n",
    "\n",
    "print(num_same_class)\n",
    "\n",
    "\n",
    "imgs = getKNearestNeighbours(tree, unseen_image, k+1)\n",
    "imgs_mst = getKNearestNeighbours(tree_mst, unseen_image, k+1)\n",
    "\n",
    "def imgs_to_indices(img_arr, testSample):\n",
    "    ind_arr = []\n",
    "    for img in img_arr:\n",
    "        for i in range(len(testSample)):\n",
    "            if (metrics.dist_fft_numba(img, testSample[i]) < 0.0000001):\n",
    "                ind_arr.append(i)\n",
    "                break\n",
    "    \n",
    "    return ind_arr\n",
    "\n",
    "ind_arr = imgs_to_indices(imgs, testSample)\n",
    "ind_arr_mst = imgs_to_indices(imgs_mst, testSample)\n",
    "print(ind_arr_mst)\n",
    "print(ind_arr)\n",
    "print(imgProd_max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fft\n",
    "import math\n",
    "\n",
    "def ncc(mainImg, tempImg) -> float:\n",
    "    \"\"\"\n",
    "    :param mainImg: Main image to be scanned\n",
    "    :param tempImg: Template image to be scanned over the main\n",
    "    :return: Max value of the ncc\n",
    "\n",
    "    Applies NCC of the template image over the main image and returns the max value obtained.\n",
    "    When the template image kernel exceeds the bounds, wraps to the other side of the main image\n",
    "    \"\"\"\n",
    "    if np.count_nonzero(mainImg) == 0:\n",
    "        if np.count_nonzero(tempImg) == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    mainImg = np.pad(mainImg, max(len(mainImg), len(mainImg[0])),\n",
    "                     'wrap')  # Padding the main image with wrapped values to simulate wrapping\n",
    "\n",
    "    mainImg = np.asarray(mainImg, np.single)  # Setting data types of array\n",
    "    tempImg = np.asarray(tempImg, np.single)\n",
    "\n",
    "    corr = cv2.matchTemplate(mainImg, tempImg, cv2.TM_CCORR_NORMED)\n",
    "\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(corr)\n",
    "\n",
    "    return max_val\n",
    "\n",
    "def ncc_unnormed(mainImg, tempImg) -> float:\n",
    "    \"\"\"\n",
    "    :param mainImg: Main image to be scanned\n",
    "    :param tempImg: Template image to be scanned over the main\n",
    "    :return: Max value of the ncc\n",
    "\n",
    "    Applies NCC of the template image over the main image and returns the max value obtained.\n",
    "    When the template image kernel exceeds the bounds, wraps to the other side of the main image\n",
    "    \"\"\"\n",
    "    if np.count_nonzero(mainImg) == 0:\n",
    "        if np.count_nonzero(tempImg) == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    mainImg = np.pad(mainImg, max(len(mainImg), len(mainImg[0])),\n",
    "                     'wrap')  # Padding the main image with wrapped values to simulate wrapping\n",
    "\n",
    "    mainImg = np.asarray(mainImg, np.single)  # Setting data types of array\n",
    "    tempImg = np.asarray(tempImg, np.single)\n",
    "\n",
    "    corr = cv2.matchTemplate(mainImg, tempImg, cv2.TM_CCORR)\n",
    "\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(corr)\n",
    "\n",
    "    return max_val\n",
    "\n",
    "dataset = get_data_MStar(300)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "index1 = np.random.randint(len(data))\n",
    "index2 = np.random.randint(len(testSample))\n",
    "unseen_image = data[index1][0]\n",
    "\n",
    "\n",
    "test_img = testSample[index2]\n",
    "test_img2 = unseen_image\n",
    "m = test_img.shape[0]\n",
    "\n",
    "ncc_score_un = ncc_unnormed(test_img, test_img2)\n",
    "ncc_score = ncc(test_img, test_img2)\n",
    "print(ncc_score)\n",
    "\n",
    "\n",
    "def ncc_fft(mainImg, tempImg):\n",
    "    A = scipy.fft.fft2(mainImg)\n",
    "    B = scipy.fft.fft2(tempImg)\n",
    "\n",
    "    Z = scipy.fft.ifft2(np.conj(A) * B).real\n",
    "\n",
    "    auto_A = scipy.fft.ifft2(np.conj(A) * A).real\n",
    "    auto_B = scipy.fft.ifft2(np.conj(B) * B).real\n",
    "    auto_A = np.maximum(auto_A, 0.0001)\n",
    "    auto_B = np.maximum(auto_B, 0.0001)\n",
    "    auto_A_sqrt = np.sqrt(auto_A)\n",
    "    auto_B_sqrt = np.sqrt(auto_B)\n",
    "    denom = np.multiply(auto_A_sqrt, auto_B_sqrt).max() # (THIS WORKS.) (Basically uhm so the normalization amt should be the same throughout? I'm not v sure why this works...)\n",
    "\n",
    "    return np.divide(Z, denom).max()\n",
    "\n",
    "#print(ncc_fft(test_img, test_img2))\n",
    "\n",
    "def ncc_naive(mainImg, tempImg):\n",
    "    if np.count_nonzero(mainImg) == 0:\n",
    "        if np.count_nonzero(tempImg) == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    mainImg = np.pad(mainImg, max(len(mainImg), len(mainImg[0])),\n",
    "                     'wrap')  # Padding the main image with wrapped values to simulate wrapping\n",
    "\n",
    "    mainImg = np.asarray(mainImg, np.single)  # Setting data types of array\n",
    "    tempImg = np.asarray(tempImg, np.single)\n",
    "    m = tempImg.shape[0]\n",
    "    ncc_arr = np.ones((2*m, 2*m))\n",
    "    for i in range(2 * m):\n",
    "        for j in range(2 * m):\n",
    "            sum = 0\n",
    "            sum_norm_main = 0\n",
    "            sum_norm_temp = 0\n",
    "            for x in range(m):\n",
    "                for y in range(m):\n",
    "                    sum += mainImg[x + i][y + j] * tempImg[x][y]\n",
    "                    sum_norm_main += (mainImg[x+i][y+j])**2\n",
    "                    sum_norm_temp += (tempImg[x][y])**2\n",
    "            \n",
    "            ncc_arr[i][j] = (sum) / (math.sqrt(sum_norm_main * sum_norm_temp))\n",
    "    \n",
    "    return ncc_arr.max()\n",
    "\n",
    "def ncc_rfft(mainImg, tempImg):\n",
    "    A = scipy.fft.rfft2(mainImg)\n",
    "    B = scipy.fft.rfft2(tempImg)\n",
    "\n",
    "    Z = scipy.fft.irfft2(np.conj(A) * B).real\n",
    "\n",
    "    auto_A = scipy.fft.irfft2(np.conj(A) * A).real.max()\n",
    "    auto_B = scipy.fft.irfft2(np.conj(B) * B).real.max()\n",
    "    auto_A = np.maximum(auto_A, 0.0001)\n",
    "    auto_B = np.maximum(auto_B, 0.0001)\n",
    "    auto_A_sqrt = np.sqrt(auto_A)\n",
    "    auto_B_sqrt = np.sqrt(auto_B)\n",
    "    denom = np.multiply(auto_A_sqrt, auto_B_sqrt).max() # (THIS WORKS.) (Basically uhm so the normalization amt should be the same throughout? I'm not v sure why this works...)\n",
    "\n",
    "    return np.divide(Z, denom).max()\n",
    "\n",
    "print(ncc_rfft(test_img, test_img2))\n",
    "print(ncc_fft(test_img, test_img2))\n",
    "\n",
    "# for i in range(100):\n",
    "#     sample_indices = random.sample(range(len(data)), 100)\n",
    "#     sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "#     testSample = [item[0] for item in sampled_test_data]\n",
    "#     index1 = np.random.randint(len(data))\n",
    "#     index2 = np.random.randint(len(testSample))\n",
    "#     unseen_image = data[index1][0]\n",
    "\n",
    "\n",
    "#     test_img = testSample[index2]\n",
    "#     test_img2 = unseen_image\n",
    "#     m = test_img.shape[0]\n",
    "\n",
    "#     ncc_score = ncc(test_img, test_img2)\n",
    "#     ncc_fft_score = ncc_fft(test_img, test_img2)\n",
    "#     ncc_rfft_score = ncc_rfft(test_img, test_img2)\n",
    "#     if (abs(ncc_score - ncc_fft_score) > 0.00001 or abs(ncc_score - ncc_rfft_score) > 0.00001):\n",
    "#         print(ncc_score)\n",
    "#         print(ncc_fft_score)\n",
    "#         print(ncc_rfft_score)\n",
    "\n",
    "# print(ncc_naive(test_img, test_img2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d0587",
   "metadata": {},
   "source": [
    "## TEST NUMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "@nb.njit\n",
    "def ncc_fft_numba(mainImg, tempImg):\n",
    "    A = scipy.fft.fft2(mainImg)\n",
    "    B = scipy.fft.fft2(tempImg)\n",
    "\n",
    "    Z = scipy.fft.ifft2(np.conj(A) * B).real\n",
    "\n",
    "    auto_A = scipy.fft.ifft2(np.conj(A) * A).real\n",
    "    auto_B = scipy.fft.ifft2(np.conj(B) * B).real\n",
    "    auto_A = np.maximum(auto_A, 0.0001)\n",
    "    auto_B = np.maximum(auto_B, 0.0001)\n",
    "    auto_A_sqrt = np.sqrt(auto_A)\n",
    "    auto_B_sqrt = np.sqrt(auto_B)\n",
    "    denom = np.multiply(auto_A_sqrt, auto_B_sqrt).max()\n",
    "\n",
    "    return np.divide(Z, denom).max()\n",
    "\n",
    "@nb.njit\n",
    "def ncc_rfft_numba(mainImg, tempImg):\n",
    "    A = scipy.fft.rfft2(mainImg)\n",
    "    B = scipy.fft.rfft2(tempImg)\n",
    "\n",
    "    Z = scipy.fft.irfft2(np.conj(A) * B).real\n",
    "\n",
    "    auto_A = scipy.fft.irfft2((np.conj(A) * A).real)\n",
    "    auto_B = scipy.fft.irfft2((np.conj(B) * B).real)\n",
    "    auto_A = np.maximum(auto_A, 0.0001)\n",
    "    auto_B = np.maximum(auto_B, 0.0001)\n",
    "    auto_A_sqrt = np.sqrt(auto_A)\n",
    "    auto_B_sqrt = np.sqrt(auto_B)\n",
    "    denom = np.multiply(auto_A_sqrt, auto_B_sqrt).max()\n",
    "\n",
    "    return np.divide(Z, denom).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd353a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising\n",
    "\n",
    "ncc_score = ncc_fft_numba(test_img, test_img2)\n",
    "ncc_score = ncc_rfft_numba(test_img, test_img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "runs = 1000\n",
    "ttime = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    ncc_score = ncc_fft_numba(test_img, test_img2)\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"rocket-fft ncc fft runtime: {ttime / runs}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    ncc_score = ncc_fft(test_img, test_img2)\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"ncc fft runtime: {ttime / runs}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    ncc_score = ncc(test_img, test_img2)\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"cv2 ncc runtime: {ttime / runs}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    ncc_score = ncc_rfft_numba(test_img, test_img2)\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"ncc rfft rocket runtime: {ttime / runs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ca163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, vectorize, float64\n",
    "\n",
    "\n",
    "# @cuda.jit()\n",
    "# def linear_ncc_search(testSample, unseen_image, k, arr):\n",
    "#     i = cuda.grid(1)\n",
    "#     if (i < len(testSample)):\n",
    "#         arr[i] = ncc_fft_numba(testSample[i], unseen_image)\n",
    "\n",
    "@vectorize([float64(float64, float64)], cache=True)\n",
    "def f(x, y):\n",
    "    return ncc_fft_numba(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = get_data_MStar(32)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), 50)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "print(len(testSample))\n",
    "index1 = np.random.randint(len(data))\n",
    "#input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "unseen_image = data[index1][0]\n",
    "unseen_images = np.array([unseen_image for i in range(len(testSample))])\n",
    "\n",
    "\n",
    "k=7\n",
    "\n",
    "# x_host = np.ones(shape=(len(testSample)))\n",
    "# x_device = cuda.to_device(x_host)\n",
    "# threadsperblock = 256\n",
    "# blockspergrid = (x_device.size + (threadsperblock - 1)) // threadsperblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c183d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.perf_counter()\n",
    "# arr = [-1 for i in range(len(testSample))]\n",
    "# linear_ncc_search[blockspergrid, threadsperblock](testSample, unseen_image, k, arr)\n",
    "unseen_img_arr = f(testSample, unseen_images)\n",
    "#unseen_img_arr = np.array(x_device)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ed1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import prange\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def linear_ncc_psearch(testSample, unseen_image, arr):\n",
    "    for i in prange(len(testSample)):\n",
    "        arr[i] = ncc_fft_numba(testSample[i], unseen_image)\n",
    "\n",
    "    return arr\n",
    "\n",
    "@nb.njit()\n",
    "def linear_ncc_search(testSample, unseen_image, arr):\n",
    "    for i in range(len(testSample)):\n",
    "        arr[i] = ncc_fft_numba(testSample[i], unseen_image)\n",
    "\n",
    "    return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c114f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Measures free memory bef and aft allocation to device.\n",
    "print(cuda.select_device(0))\n",
    "print(cuda.current_context().get_memory_info()[0])\n",
    "arr = np.zeros(10**9, dtype = 'float32')\n",
    "d_ary = cuda.to_device(arr)\n",
    "print(cuda.current_context().get_memory_info()[0])\n",
    "print(d_ary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29354ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "ttime = 0\n",
    "image_size = 100\n",
    "sample_size = 9000\n",
    "\n",
    "# data = get_data_MStar(image_size)\n",
    "\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item[0] for item in sampled_test_data]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# #input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "# unseen_image = data[index1][0]\n",
    "\n",
    "data = get_data_MStar(image_size)\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "index1 = np.random.randint(len(data))\n",
    "unseen_image = data[index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974913a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSample = [item[0] for item in sampled_test_data]\n",
    "testSample = np.array(testSample)\n",
    "print(testSample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc_fft(testSample[0], testSample[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "runs = 100\n",
    "ttime = 0\n",
    "image_size = 16\n",
    "sample_size = 20000\n",
    "\n",
    "# data = get_data_SARDet_100k(image_size)\n",
    "\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item[0] for item in sampled_test_data]\n",
    "# classes = [item[1] for item in sampled_test_data]\n",
    "# testSampleIndexed = [(item, i) for item in testSample]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# # #input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "# unseen_image = data[index1][0]\n",
    "# unseen_image_class = data[index1][1]\n",
    "\n",
    "print(f\"Unseen image class: {unseen_image_class}\")\n",
    "# print()\n",
    "\n",
    "arr = np.ones(len(testSample))\n",
    "unseen_img_arr = linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "\n",
    "other = linear_ncc_search(testSample, unseen_image, arr)\n",
    "other_index = np.argpartition(other, -(k+1))[-(k+1):]\n",
    "# print()\n",
    "for i in range(len(imgProd_max_index)):\n",
    "    print(classes[imgProd_max_index[i]])\n",
    "\n",
    "for i in range(len(other_index)):\n",
    "    print(classes[other_index[i]])\n",
    "\n",
    "# data = get_data(image_size)\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item for item in sampled_test_data]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# unseen_image = data[index1]\n",
    "\n",
    "# for i in range(runs):\n",
    "#     start_time = time.perf_counter()\n",
    "#     arr = np.ones(len(testSample))\n",
    "#     unseen_img_arr = linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "#     imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "#     end_time = time.perf_counter()\n",
    "#     ttime += end_time - start_time\n",
    "\n",
    "# print(f\"time taken for parallelised linear ncc search with rocket fft over {runs} runs for image size {image_size} and sample size {sample_size} : {ttime / runs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f758028",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "ttime = 0\n",
    "image_size = 16\n",
    "sample_size = 100000\n",
    "# data = get_data_MStar(image_size)\n",
    "\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item[0] for item in sampled_test_data]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# unseen_image = data[index1][0]\n",
    "\n",
    "# data = get_data(image_size)\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item for item in sampled_test_data]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# unseen_image = data[index1]\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    arr = np.ones(len(testSample))\n",
    "    unseen_img_arr = linear_ncc_search(testSample, unseen_image, arr)\n",
    "    imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"time taken for linear ncc search with rocket fft over {runs} runs for image size {image_size} and sample size {sample_size} : {ttime / runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 10\n",
    "ttime = 0\n",
    "image_size = 16\n",
    "sample_size = 100\n",
    "data = get_data_SARDet_100k(image_size)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "index1 = np.random.randint(len(data))\n",
    "#input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "unseen_image = data[index1][0]\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "tree = getMTreeFFT(testSample, 12)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"time taken for mtree init with rocket fft over {1} runs for image size {image_size} and sample size {sample_size} : {end_time - start_time}\")\n",
    "\n",
    "for i in range(runs):\n",
    "    index1 = np.random.randint(len(data))\n",
    "    #input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "    unseen_image = data[index1][0]\n",
    "    start_time = time.perf_counter()\n",
    "    arr = np.ones(len(testSample))\n",
    "    img_arr = getKNearestNeighbours(tree, unseen_image, k)\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"time taken for mtree with rocket fft over {runs} runs for image size {image_size} and sample size {sample_size} : {ttime / runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1\n",
    "ttime = 0\n",
    "image_size = 16\n",
    "sample_size = 100\n",
    "\n",
    "data = get_data_MStar(image_size)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "index1 = np.random.randint(len(data))\n",
    "unseen_image = data[index1][0]\n",
    "\n",
    "# data = get_data(image_size)\n",
    "# sample_indices = random.sample(range(len(data)), sample_size)\n",
    "# sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "# testSample = [item for item in sampled_test_data]\n",
    "# index1 = np.random.randint(len(data))\n",
    "# unseen_image = data[index1]\n",
    "\n",
    "for i in range(runs):\n",
    "    start_time = time.perf_counter()\n",
    "    arr = np.ones(len(testSample))\n",
    "    unseen_img_arr = linear_ncc_search(testSample, unseen_image, arr)\n",
    "    imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "    end_time = time.perf_counter()\n",
    "    ttime += end_time - start_time\n",
    "\n",
    "print(f\"time taken for linear ncc search with rocket fft over {runs} runs for image size {image_size} and sample size {sample_size} : {ttime / runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e4379",
   "metadata": {},
   "source": [
    "## Accuracy test sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c74a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "runs = 100\n",
    "ttime = 0\n",
    "image_size = 16\n",
    "sample_size = 100000\n",
    "\n",
    "def imgs_to_indices(img_arr, testSample):\n",
    "    added = False\n",
    "    ind_arr = []\n",
    "    for img in img_arr:\n",
    "        added = False\n",
    "        for i in range(len(testSample)):\n",
    "            if (metrics.distance(img, testSample[i]) < 1e-7):\n",
    "                ind_arr.append(i)\n",
    "                added = True\n",
    "                break\n",
    "        if (not added):\n",
    "            print(\"NOT ADDED\")\n",
    "            print(f\"ncc: {ncc(img,img)}\")\n",
    "            print(f\"img prod ncc: {ImageProducts.ncc(img, img)}\")\n",
    "            print(f\"ncc fft: {ncc_fft_numba(img, img)}\")\n",
    "            print(metrics.distance(img, img))\n",
    "            for j in range(len(testSample)):\n",
    "                print(metrics.distance(img, testSample[j]))\n",
    "                print(f\"ncc against test samples: {ncc(img,testSample[j])}\")\n",
    "\n",
    "    return ind_arr\n",
    "\n",
    "data = get_data_SARDet_100k(image_size)\n",
    "\n",
    "\n",
    "for i in range(runs):\n",
    "    sample_indices = random.sample(range(len(data)), sample_size)\n",
    "    sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "    testSample = [item[0] for item in sampled_test_data]\n",
    "    testSampleIndexed = []\n",
    "    for i in range(len(testSample)):\n",
    "        testSampleIndexed.append((testSample[i], i))\n",
    "    index1 = np.random.choice(sample_indices)\n",
    "    unseen_image = data[index1][0]\n",
    "    unseen_image_indexed = [unseen_image, index1]\n",
    "    \n",
    "    arr = np.ones(len(testSample))\n",
    "    unseen_img_parr = linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "    imgProd_max_pindex = np.argpartition(unseen_img_parr, -(k+1))[-(k+1):]\n",
    "\n",
    "    # arr = []\n",
    "    # for j in range(len(testSample)):\n",
    "    #     result = ncc(testSample[j], unseen_image)\n",
    "    #     arr.append(result)\n",
    "\n",
    "    # unseen_img_arr_normal = np.array(arr)\n",
    "    # #print(unseen_img_arr)\n",
    "    # imgProd_max_index_normal = np.argpartition(unseen_img_arr_normal, -(k+1))[-(k+1):]\n",
    "\n",
    "    # arr = []\n",
    "    # for j in range(len(testSample)):\n",
    "    #     result = ncc_fft(testSample[j], unseen_image)\n",
    "    #     arr.append(result)\n",
    "\n",
    "    unseen_img_arr_fft = np.array(arr)\n",
    "    #print(unseen_img_arr)\n",
    "    imgProd_max_index_fft = np.argpartition(unseen_img_arr_fft, -(k+1))[-(k+1):]\n",
    "\n",
    "    # tree = getMTreeFFTNumba(testSample, 12)\n",
    "    # img_arr = getKNearestNeighbours(tree, unseen_image, k)\n",
    "\n",
    "    # ind_arr = imgs_to_indices(img_arr, testSample)\n",
    "    # print(len(img_arr))\n",
    "\n",
    "    normal_tree = getMTree(testSampleIndexed, 3, promote=mtree.MST_promotion, partition=mtree.MST_partition, d=metrics.dist_fft_numba_indexed)\n",
    "    # print(len(normal_tree))\n",
    "    img_arr_2 = getKNearestNeighbours(normal_tree, unseen_image_indexed, k=3)\n",
    "    print(len(img_arr_2))\n",
    "    img_arr_2 = [item[1] for item in img_arr_2]\n",
    "\n",
    "    #print(img_arr_2)\n",
    "    # print(len(img_arr_2))    \n",
    "    # ind_arr_2 = imgs_to_indices(img_arr_2, testSample)\n",
    "\n",
    "    # mtree_fft = getMTreeFFT(testSample, 12)\n",
    "    # img_arr_3 = getKNearestNeighbours(tree, unseen_image, k)\n",
    "    # ind_arr_3 = imgs_to_indices(img_arr_3, testSample)\n",
    "\n",
    "    # tree_mst = getMTreeFFTNumba(testSample, 12, promote=mtree.MST_promotion, partition=mtree.MST_partition)\n",
    "    # img_arr = getKNearestNeighbours(tree_mst, unseen_image, k)\n",
    "    # ind_arr_3 = imgs_to_indices(img_arr, testSample)\n",
    "\n",
    "    # tree_mst_normal = getMTree(testSample, 12, promote=mtree.MST_promotion, partition=mtree.MST_partition)\n",
    "    # img_arr = getKNearestNeighbours(tree_mst_normal, unseen_image, k)\n",
    "    # ind_arr_4 = imgs_to_indices(img_arr, testSample)\n",
    "\n",
    "\n",
    "    # imgProd_max_index = set(list(imgProd_max_index[1:].astype(int)))\n",
    "    # imgProd_max_index_normal = set(list(imgProd_max_index_normal[1:].astype(int)))\n",
    "    # imgProd_max_index_fft = set(list(imgProd_max_index_fft[1:].astype(int)))\n",
    "    #imgProd_max_pindex = set(list(imgProd_max_pindex[1:]))\n",
    "    # imgProd_max_index_normal = imgProd_max_index_normal[1:]\n",
    "    imgProd_max_pindex = set(imgProd_max_pindex[1:])\n",
    "\n",
    "    # ind_arr = set(ind_arr)\n",
    "    ind_arr_2 = set(list(img_arr_2))\n",
    "    print(len(ind_arr_2))\n",
    "    # ind_arr_3 = set(ind_arr_3)\n",
    "    # ind_arr_4 = set(ind_arr_4)\n",
    "\n",
    "    # problem is branch isnt even visited.... for some img...\n",
    "\n",
    "    # if (imgProd_max_index != imgProd_max_index_normal or imgProd_max_index_normal != imgProd_max_index_fft or imgProd_max_index_fft != imgProd_max_pindex or imgProd_max_pindex != ind_arr \n",
    "    #     or ind_arr != ind_arr_2 or ind_arr_2 != ind_arr_3):\n",
    "    # if (ind_arr != ind_arr_2 or ind_arr_2 != ind_arr_3 or ind_arr_3 != ind_arr_4):\n",
    "    # if (ind_arr != ind_arr_2 or ind_arr_2 != ind_arr_3):\n",
    "    if (ind_arr_2 != imgProd_max_pindex):\n",
    "        ind_arr_2 = list(ind_arr_2)\n",
    "        \n",
    "        # print(f\"ncc rocket fft: {imgProd_max_index}\")\n",
    "        # imgProd_max_index_normal = list(imgProd_max_index_normal)\n",
    "        # print(f\"ncc normal: {imgProd_max_index_normal}\")\n",
    "        # print(f\"ncc fft: {imgProd_max_index_fft}\")\n",
    "        imgProd_max_pindex = list(imgProd_max_pindex)\n",
    "        print(f\"ncc parallel rocket fft: {imgProd_max_pindex}\")\n",
    "        print(f\"mtree normal: {ind_arr_2}\")\n",
    "        # print(f\"mtree normal:{ind_arr_2}\")\n",
    "        # print(f\"mtree w fft: {ind_arr_3}\")\n",
    "        for j in ind_arr_2:\n",
    "            for i in range(len(imgProd_max_pindex)):\n",
    "                #print(metrics.distance(testSample[j], testSample[imgProd_max_index_normal[i]]))\n",
    "                print(f\"DISTANCE TO IMG from index {j}: {metrics.distance(testSample[j], unseen_image)}\")\n",
    "                print(f\"DISTANCE TO IMG W INDEX from index {j}: {metrics.dist_with_index((testSample[j], j), (unseen_image, -1))}\")\n",
    "                print(f\"DISTANCE TO IMG TEST from index {i}: {metrics.distance(testSample[imgProd_max_pindex[i]], unseen_image)}\")\n",
    "        # for i in range(len(img_arr)):\n",
    "        #     print(metrics.distance(img_arr[i], ))\n",
    "\n",
    "        # print(f\"mtree w mst normal: {ind_arr_4}\")\n",
    "\n",
    "\n",
    "# print(unseen_img_parr)\n",
    "# print(unseen_img_arr)\n",
    "# print(unseen_img_arr_fft)\n",
    "# print(unseen_img_arr_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688688ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "runs = 100\n",
    "ttime = 0\n",
    "image_size = 100\n",
    "sample_size = 500\n",
    "\n",
    "\n",
    "data = get_data_MStar(image_size)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "testSampleIndexed = []\n",
    "for i in range(len(testSample)):\n",
    "    testSampleIndexed.append((testSample[i], i))\n",
    "\n",
    "index1 = np.random.choice(sample_indices)\n",
    "unseen_image = data[index1][0]\n",
    "unseen_image_indexed = [unseen_image, index1]\n",
    "\n",
    "start = time.perf_counter()\n",
    "normal_tree = getMTree(testSampleIndexed, 3, d=metrics.dist_fft_numba_indexed)\n",
    "end = time.perf_counter()\n",
    "print(end - start)\n",
    "# print(len(normal_tree))\n",
    "img_arr_2 = getKNearestNeighbours(normal_tree, unseen_image_indexed, k=3)\n",
    "img_arr_2 = [item[1] for item in img_arr_2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5dcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 200\n",
    "sample_size = 100000\n",
    "\n",
    "data = get_data_SARDet_100k(image_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "# testSampleIndexed = []\n",
    "# for i in range(len(testSample)):\n",
    "#     testSampleIndexed.append((testSample[i], i))\n",
    "index1 = np.random.choice(sample_indices)\n",
    "# index1 = np.random.randint(len(testSample))\n",
    "# print(f\"UNSEEN IMAGE INDEX: {index1}\")\n",
    "#index1 = np.random.randint(len(data))\n",
    "unseen_image = data[index1][0]\n",
    "# unseen_image = testSample[index1]\n",
    "# unseen_image_indexed = [unseen_image, index1]\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones(len(testSample))\n",
    "\n",
    "k=7\n",
    "print(len(testSample))\n",
    "\n",
    "start = time.perf_counter()\n",
    "unseen_img_arr = linear_ncc_search(testSample, unseen_image, arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "end = time.perf_counter()\n",
    "print(f\"fft time: {end - start}\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "unseen_img_parr = linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "imgProd_max_pindex = np.argpartition(unseen_img_parr, -(k+1))[-(k+1):]\n",
    "end = time.perf_counter()\n",
    "print(f\"parallel time: {end-start}\")\n",
    "\n",
    "print(imgProd_max_index)\n",
    "print(imgProd_max_pindex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtree_query_times_max_node_size(image_size=32, k=7, runs=100, max_node_sizes=[], sample_size = 1000):\n",
    "\n",
    "    print(f\"Query times of mtree w {image_size}, {k} neighbours, variable mtree max node size, sample size {sample_size} over {runs} runs\")\n",
    "    data = get_data_SARDet_100k(image_size)\n",
    "    print(len(data))\n",
    "\n",
    "    # avgs_mtree = []\n",
    "    avgs_mtree_fft = []\n",
    "    min_time = 1000\n",
    "    min_node = 0\n",
    "\n",
    "    for i in range(len(max_node_sizes)):\n",
    "    \n",
    "        time_mtree_fft = 0\n",
    "        \n",
    "        sample_indices = random.sample(range(len(data)), sample_size)\n",
    "        testSample = Subset(data, sample_indices)\n",
    "        testSample = [item[0] for item in testSample]\n",
    "        print(len(testSample))\n",
    "        print(\"done with testSample\")\n",
    "        tree_fft = getMTreeFFTNumba(testSample, max_node_sizes[i])\n",
    "        \n",
    "\n",
    "        print(f\"Now testing with max nod size: {max_node_sizes[i]}\")\n",
    "        for j in range(runs):\n",
    "\n",
    "            index1 = np.random.randint(len(data))\n",
    "            unseen_image = data[index1][0]\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            knn = getKNearestNeighbours(tree_fft, unseen_image, k)\n",
    "            end_time = time.perf_counter()\n",
    "            time_mtree_fft += end_time - start_time\n",
    "    \n",
    "\n",
    "        avg_mtree_fft = time_mtree_fft / runs\n",
    "        avgs_mtree_fft.append(avg_mtree_fft)\n",
    "        if (avg_mtree_fft < min_time):\n",
    "            min_time = avg_mtree_fft\n",
    "            min_node = max_node_sizes[i]\n",
    "        print(f\"Avg time for mtree fft: {avg_mtree_fft}\")\n",
    "\n",
    "    print(avgs_mtree_fft)\n",
    "    return min_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_node_sizes = list(np.arange(61))[4:]\n",
    "min_node = mtree_query_times_max_node_size(image_size=32, k=7, runs=100, max_node_sizes=max_node_sizes, sample_size = 1000)\n",
    "print(min_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_max_node_query(x_axis=\"\", title=\"\", filename=\"\", varied_arr=[], data=[]):\n",
    "    data_mtree_init = []\n",
    "    for i in range(len(varied_arr)):\n",
    "        data_mtree_init.append([varied_arr[i], data[i]])\n",
    "\n",
    "    data_mtree_init = np.array(data_mtree_init)\n",
    "\n",
    "    x_mtree_init, y_mtree_init = data_mtree_init.T\n",
    "\n",
    "    plt.plot(x_mtree_init, y_mtree_init, label = f\"mtree query\", linestyle=\"-\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(\"Runtime (s)\")\n",
    "    plt.title(title)\n",
    "    # note its over 3 runs\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #plt.savefig(filename, bbox_inches='tight', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtree_data = [0.06648644635453821, 0.04985567561350763, 0.04870200272882357, 0.042445177359040825, 0.0425087088951841, 0.046353489940520375, 0.040480049555189905, 0.04304797149496153, 0.04135642212582752, 0.04262374066282064, 0.044578851461410524, 0.04061049841810018, 0.04110180689021945, 0.04106593079632148, 0.03709261615527794, 0.037639004660304634, 0.03677894547581673, 0.03674548069713637, 0.04001359798945486, 0.03869960339041427, 0.03689934399910271, 0.03658173124538735, 0.036888814736157655, 0.03928412785753608, 0.03513387402286753, 0.03632704545278102, 0.03477122830459848, 0.035498228406067936, 0.03584905448602513, 0.03439247708767652, 0.03489864443661645, 0.034392852096352726, 0.038850501228589567, 0.04059506134595722, 0.037090135156176984, 0.03257076226640493, 0.035022176750935614]\n",
    "varied_arr = list(np.arange(41))[4:]\n",
    "plot_data_max_node_query(x_axis=\"max node size\", title=\"Average runtime of finding 7 NN with sample size 100 against max node size\", filename=\"/home/jovyan/evaluation/results/MSTAR_query_max_node_size.png\", varied_arr=varied_arr, data=arr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7387afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mstar optimal max node size results for query on img size 32, ss 1000\n",
    "# 4 - 40\n",
    "arr1 =[0.07453360404120758, 0.0760427448619157, 0.057443349526729436, 0.045245827161706983, 0.04202748618321493, 0.04436554169282317, 0.0449096650374122, 0.049698567090090365, 0.04333758944878355, 0.041379778508562594, 0.03800240215845406, 0.04514048206154257, 0.039640117364469915, 0.03959023931762204, 0.03827136555686593, 0.03823503922438249, 0.03754547564079985, 0.04120285571785644, 0.03816691774642095, 0.035826291469857094, 0.036856996566057204, 0.03455932650016621, 0.037203651634044946, 0.03615273126168177, 0.03421859634341672, 0.0338427792978473, 0.03774892676388845, 0.033893928003963084, 0.03348122368799523, 0.03554718185449019, 0.03566529909381643, 0.03361380087211728, 0.03321378830587492, 0.03533396480837837, 0.03611931434832513, 0.03526111783226952, 0.03197259914362803]\n",
    "# 30smt to 60\n",
    "arr2= [0.0346488059964031, 0.03654287959448993, 0.03562711640959606, 0.032780495339538904, 0.034241919352207335, 0.03458906295709312, 0.03467396464431658, 0.0342395016993396, 0.033899460977409035, 0.03237264700001106, 0.03372701143147424, 0.03436243370873854, 0.03563438596436754, 0.03458592792507261, 0.03491504484321922, 0.03612813181476668, 0.03313937911298126, 0.03355619212146849, 0.03295315083116293, 0.0320811181794852, 0.032378274663351476, 0.03378636085428297, 0.0368100996594876, 0.03444660003529862, 0.03417618005303666, 0.03274708759970963, 0.03469003511127085, 0.03473432528786361, 0.03211943145142868, 0.03331151863792911, 0.03137900785310194]\n",
    "# 60 to 80\n",
    "arr3 = [0.03498226410476491, 0.03348756037419662, 0.03340420721564442, 0.03191877581179142, 0.032104695639573036, 0.0330363729596138, 0.0330572799523361, 0.03206634910544381, 0.03126085305120796, 0.030408310950733722, 0.03215049129677936, 0.030636014845222236, 0.030524236671626567, 0.03307789267739281, 0.032382142718415705, 0.034685473120771346, 0.03245834693312645, 0.030518119069747628, 0.03169271961785853, 0.03134439779445529, 0.03312397683737799]\n",
    "\n",
    "arr4 = arr1 + arr2 + arr3\n",
    "\n",
    "\n",
    "# imdb 4-40... for imdb optimal seems to be 39\n",
    "imdb_mtree_queires = [0.06648644635453821, 0.04985567561350763, 0.04870200272882357, 0.042445177359040825, 0.0425087088951841, 0.046353489940520375, 0.040480049555189905, 0.04304797149496153, 0.04135642212582752, 0.04262374066282064, 0.044578851461410524, 0.04061049841810018, 0.04110180689021945, 0.04106593079632148, 0.03709261615527794, 0.037639004660304634, 0.03677894547581673, 0.03674548069713637, 0.04001359798945486, 0.03869960339041427, 0.03689934399910271, 0.03658173124538735, 0.036888814736157655, 0.03928412785753608, 0.03513387402286753, 0.03632704545278102, 0.03477122830459848, 0.035498228406067936, 0.03584905448602513, 0.03439247708767652, 0.03489864443661645, 0.034392852096352726, 0.038850501228589567, 0.04059506134595722, 0.037090135156176984, 0.03257076226640493, 0.035022176750935614]\n",
    "# imdb 40-60\n",
    "imdb_mtree_queries2 = [0.034285021647810936, 0.03713188007473946, 0.034894764921627935, 0.03555949220666662, 0.035689420572016385, 0.03569600458955392, 0.033280161919537934, 0.03747501684119925, 0.03479402721859515, 0.03337046544300392, 0.03534285521600396, 0.03567452662857249, 0.03378638550639153, 0.0334895676583983, 0.034134891235735265, 0.03503445783164352, 0.034463033261708916, 0.03489655483979732, 0.0342508251266554, 0.03370528262341395, 0.034018566329032184]\n",
    "\n",
    "\n",
    "# SARDET 4-60\n",
    "sardet_mtree_queries = [0.05605921014677733, 0.05075702435569838, 0.04413527720142156, 0.041756415739655495, 0.03897050929255783, 0.03522332780994475, 0.036123363303486256, 0.03425650781719014, 0.03520769790979102, 0.028383985385298727, 0.03203317269915715, 0.031361144478432836, 0.03225434086751193, 0.029638385449070482, 0.02979010680690408, 0.02958951032022014, 0.02993883696850389, 0.028521171587053685, 0.026733811788726598, 0.02914088960038498, 0.027329431518446655, 0.028482252936810254, 0.02713657370302826, 0.028098205365240575, 0.02961433698888868, 0.029213892379775643, 0.028177589017432184, 0.02646509645273909, 0.028109403648413717, 0.028149100167211144, 0.026108818335924298, 0.028049338192213325, 0.027457277516368776, 0.029412257303483784, 0.02738230670336634, 0.02729105085367337, 0.028033825629390777, 0.02971143915085122, 0.02866856729844585, 0.026201729581225665, 0.02954051753738895, 0.026153243039734663, 0.02761902290629223, 0.026829968113452195, 0.027388408198021352, 0.028706976901739834, 0.028187253254000098, 0.026537614634726196, 0.02740072306478396, 0.028295907287392764, 0.029209212146233766, 0.027169538359157742, 0.02613771740347147, 0.026517155214678498, 0.02741806108504534, 0.02730743122054264, 0.02787747632712126]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3dbbaf",
   "metadata": {},
   "source": [
    "# ID Estimator Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ba1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ID estimator comparisons:\n",
    "# also functions that wouldve been nice oops\n",
    "sample_sizes = [100, 1000, 2000]\n",
    "\n",
    "def get_IMDB_data(image_size, sample_size):\n",
    "    IMDB_WIKI_data = get_data(image_size)\n",
    "    sample_indices = random.sample(range(len(IMDB_WIKI_data)), sample_size)\n",
    "    sampled_test_data = Subset(IMDB_WIKI_data, sample_indices)\n",
    "    testSample = np.array(sampled_test_data)\n",
    "    return testSample\n",
    "\n",
    "def get_MSTAR_data(image_size, sample_size):\n",
    "    data = get_data_MStar(image_size)\n",
    "    sample_indices = random.sample(range(len(data)), sample_size)\n",
    "    sampled_test_data = Subset(data, sample_indices)\n",
    "    testSample = np.array([item[0] for item in sampled_test_data])\n",
    "    return testSample\n",
    "\n",
    "def get_SARDET_data(image_size, sample_size):\n",
    "    data = get_data_SARDet_100k(image_size)\n",
    "    sample_indices = random.sample(range(len(data)), sample_size)\n",
    "    sampled_test_data = Subset(data, sample_indices)\n",
    "    testSample = np.array([item[0] for item in sampled_test_data])\n",
    "    return testSample\n",
    "\n",
    "def get_ATRNET_data(sample_size, list_path):\n",
    "    data = np.load(list_path)\n",
    "    all_data = data[\"testSample\"]\n",
    "    sample_indices = np.array(random.sample(range(len(all_data)), sample_size))\n",
    "    testSample = all_data[sample_indices]\n",
    "    return testSample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bf2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtree_ncc_query_times_sample_size(image_size=128, k=7, runs=100, max_node_size=12, list_data=\"path\", sample_sizes = []):\n",
    "\n",
    "    print(f\"Query times of diff methods based on kNN with ncc with image size {image_size}, {k} neighbours, mtree max node size {max_node_size} over {runs} runs\")\n",
    "\n",
    "    data_list = np.load(list_data)\n",
    "    data = data_list[\"testSample\"]\n",
    "\n",
    "            \n",
    "    avgs_ncc_pfft = []\n",
    "    avgs_ncc_fft = []\n",
    "    avgs_ncc_unoptim = []\n",
    "    avgs_mtree = []\n",
    "    avgs_mtree_fft = []\n",
    "\n",
    "    for i in range(len(sample_sizes)):\n",
    "        \n",
    "        time_ncc_pfft = 0\n",
    "        time_ncc_fft = 0\n",
    "        time_ncc_unoptim = 0\n",
    "        time_mtree = 0\n",
    "        time_mtree_fft = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        testSample = data[sample_indices]\n",
    "        print(\"done with testSample\")\n",
    "        # tree = getMTree(testSample, max_node_size)\n",
    "        tree_fft = getMTreeFFTNumba(testSample, max_node_size)\n",
    "        \n",
    "\n",
    "        print(f\"Now testing with sample size: {sample_sizes[i]}\")\n",
    "        for j in range(runs):\n",
    "\n",
    "            index1 = np.random.randint(len(data))\n",
    "            unseen_image = data[index1]\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            arr = np.ones(len(testSample))\n",
    "            unseen_img_arr = ImageProducts.linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "            end_time = time.perf_counter()\n",
    "            time_ncc_pfft += end_time - start_time\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            arr = np.ones(len(testSample))\n",
    "            unseen_img_arr = ImageProducts.linear_ncc_search(testSample, unseen_image, arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "            end_time = time.perf_counter()\n",
    "            time_ncc_fft += end_time - start_time\n",
    "\n",
    "            # start_time = time.perf_counter()\n",
    "            # arr = []\n",
    "            # for j in range(len(testSample)):\n",
    "            #     result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "            #     arr.append(result)\n",
    "            \n",
    "            # unseen_img_arr = np.array(arr)\n",
    "            # imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "            # time_ncc_unoptim += end_time - start_time\n",
    "\n",
    "            # start_time = time.perf_counter()\n",
    "            # knn = getKNearestNeighbours(tree, unseen_image, k)\n",
    "            # end_time = time.perf_counter()\n",
    "            # time_mtree += end_time - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            knn = getKNearestNeighbours(tree_fft, unseen_image, k)\n",
    "            end_time = time.perf_counter()\n",
    "            time_mtree_fft += end_time - start_time\n",
    "        \n",
    "        avg_ncc_pfft = time_ncc_pfft / runs\n",
    "        avgs_ncc_pfft.append(avg_ncc_pfft)\n",
    "        print(f\"Avg time for ncc pfft: {avg_ncc_pfft}\")\n",
    "\n",
    "        avg_ncc_fft = time_ncc_fft / runs\n",
    "        avgs_ncc_fft.append(avg_ncc_fft)\n",
    "        print(f\"Avg time for ncc fft: {avg_ncc_fft}\")\n",
    "\n",
    "        # avg_ncc_unoptim = time_ncc_unoptim / runs\n",
    "        # avgs_ncc_unoptim.append(avg_ncc_unoptim)\n",
    "        # print(f\"Avg time for ncc unoptim: {avg_ncc_unoptim}\")\n",
    "\n",
    "        # avg_mtree = time_mtree / runs\n",
    "        # avgs_mtree.append(avg_mtree)\n",
    "        # print(f\"Avg time for mtree: {avg_mtree}\")\n",
    "\n",
    "        avg_mtree_fft = time_mtree_fft / runs\n",
    "        avgs_mtree_fft.append(avg_mtree_fft)\n",
    "        print(f\"Avg time for mtree fft: {avg_mtree_fft}\")\n",
    "\n",
    "    print(avgs_ncc_pfft)\n",
    "    print(avgs_ncc_fft)\n",
    "    # print(avgs_ncc_unoptim)\n",
    "    # print(avgs_mtree)\n",
    "    print(avgs_mtree_fft)\n",
    "    with open(\"/home/jovyan/evaluation/results/test_query_sample_sizes.txt\", \"w\") as file:\n",
    "        file.write(f\"sample_sizes = {sample_sizes}\\n\")\n",
    "        file.write(f\"avgs_ncc_pfft = {avgs_ncc_pfft}\\n\")\n",
    "        file.write(f\"avgs_ncc_fft = {avgs_ncc_fft}\\n\")\n",
    "        # file.write(f\"avgs_ncc_unoptim = {avgs_ncc_unoptim}\\n\")\n",
    "        # file.write(f\"avgs_mtree = {avgs_mtree}\\n\")\n",
    "        file.write(f\"avgs_mtree_fft = {avgs_mtree_fft}\")\n",
    "\n",
    "\n",
    "\n",
    "def ncc_pfft_query_time(image_size=128, k=7, runs=100, sample_size=1000, testSample=[], unseen_image=[]):\n",
    "    avg_ncc_pfft = 0\n",
    "    time_ncc_pfft = 0\n",
    "    for j in range(runs):\n",
    "        start_time = time.perf_counter()\n",
    "        arr = np.ones(len(testSample))\n",
    "        unseen_img_arr = ImageProducts.linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "        imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "        end_time = time.perf_counter()\n",
    "        time_ncc_pfft += end_time - start_time\n",
    "    \n",
    "    avg_ncc_pfft = time_ncc_pfft / runs\n",
    "    return avg_ncc_pfft\n",
    "\n",
    "def ncc_fft_query_time(image_size=128, k=7, runs=100, sample_size=1000, testSample=[], unseen_image=[]):\n",
    "    time_ncc_fft = 0\n",
    "    for j in range(runs):\n",
    "        start_time = time.perf_counter()\n",
    "        arr = np.ones(len(testSample))\n",
    "        unseen_img_arr = ImageProducts.linear_ncc_search(testSample, unseen_image, arr)\n",
    "        imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "        end_time = time.perf_counter()\n",
    "        time_ncc_fft += end_time - start_time\n",
    "    \n",
    "    avg_ncc_fft = time_ncc_fft / runs\n",
    "    return avg_ncc_fft\n",
    "\n",
    "def mtree_fft_query_time(image_size=128, k=7, runs=100, max_node_size=25, sample_size=1000, testSample=[], unseen_image=[]):\n",
    "    time_mtree_fft = 0\n",
    "    tree_fft = getMTreeFFTNumba(testSample, max_node_size)\n",
    "    for j in range(runs):\n",
    "        start_time = time.perf_counter()\n",
    "        knn = getKNearestNeighbours(tree_fft, unseen_image, k)\n",
    "        end_time = time.perf_counter()\n",
    "        time_mtree_fft += end_time - start_time\n",
    "    \n",
    "    avg_mtree_fft = time_mtree_fft / runs\n",
    "    return avg_mtree_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_percentage_improvements_mtree_ncc(sample_sizes=[], runs=10, k=7):\n",
    "    print(f\"Measuring percentage improvement of mtree over ncc for IMDB, MSTAR, SARDET, ATRNET over {runs} runs for {k} neighbours over variable sample sizes and image size 128\")\n",
    "    list_path = \"/home/jovyan/data/ATRNet-STAR_annotations/list_data_all.npz\"\n",
    "    p_incs_imdb = []\n",
    "    p_incs_MSTAR = []\n",
    "    p_incs_SARDET = []\n",
    "    p_incs_ATRNET = []\n",
    "    IMDB_WIKI_data = get_data(128)\n",
    "    MSTAR_data = get_data_MStar(128)\n",
    "    SARDET_data = get_data_SARDet_100k(128)\n",
    "    ATRNET_data = np.load(list_path)\n",
    "    all_data = ATRNET_data[\"testSample\"]\n",
    "\n",
    "    for i in range(len(sample_sizes)):\n",
    "        print(f\"Now doing sample size {sample_sizes[i]}\")\n",
    "        \n",
    "        sample_indices = random.sample(range(len(IMDB_WIKI_data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(IMDB_WIKI_data, sample_indices)\n",
    "        testSample_imdb = np.array(sampled_test_data)\n",
    "        unseen_image_imdb = IMDB_WIKI_data[np.random.randint(len(IMDB_WIKI_data))]\n",
    "\n",
    "        \n",
    "        sample_indices = random.sample(range(len(MSTAR_data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(MSTAR_data, sample_indices)\n",
    "        testSample_MSTAR = np.array([item[0] for item in sampled_test_data])\n",
    "        unseen_image_MSTAR = MSTAR_data[np.random.randint(len(MSTAR_data))][0]\n",
    "\n",
    "        \n",
    "        sample_indices = random.sample(range(len(SARDET_data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(SARDET_data, sample_indices)\n",
    "        testSample_SARDET = np.array([item[0] for item in sampled_test_data])\n",
    "        unseen_image_SARDET = SARDET_data[np.random.randint(len(SARDET_data))][0]\n",
    "\n",
    "        sample_indices = np.array(random.sample(range(len(all_data)), sample_sizes[i]))\n",
    "        testSample_ATRNET = all_data[sample_indices]\n",
    "        unseen_image_ATRNET = all_data[np.random.randint(len(all_data))]\n",
    "    \n",
    "\n",
    "        avg_ncc_fft_time_imdb = ncc_fft_query_time(image_size=128, k=k, runs=runs, sample_size=sample_sizes[i], testSample=testSample_imdb, unseen_image=unseen_image_imdb)\n",
    "        avg_ncc_fft_time_MSTAR = ncc_fft_query_time(image_size=128, k=k, runs=runs, sample_size=sample_sizes[i], testSample=testSample_MSTAR, unseen_image=unseen_image_MSTAR)\n",
    "        avg_ncc_fft_time_SARDET = ncc_fft_query_time(image_size=128, k=k, runs=runs, sample_size=sample_sizes[i], testSample=testSample_SARDET, unseen_image=unseen_image_SARDET)\n",
    "        avg_ncc_fft_time_ATRNET = ncc_fft_query_time(image_size=128, k=k, runs=runs, sample_size=sample_sizes[i], testSample=testSample_ATRNET, unseen_image=unseen_image_ATRNET)\n",
    "        print(f\"TIMES NCC: {avg_ncc_fft_time_imdb}, {avg_ncc_fft_time_MSTAR}, {avg_ncc_fft_time_SARDET}, {avg_ncc_fft_time_ATRNET}\")\n",
    "\n",
    "        avg_mtree_fft_time_imdb = mtree_fft_query_time(image_size=128, k=k, runs=runs, max_node_size=39, sample_size=sample_sizes[i], testSample=testSample_imdb, unseen_image=unseen_image_imdb)\n",
    "        avg_mtree_fft_time_MSTAR = mtree_fft_query_time(image_size=128, k=k, runs=runs, max_node_size=39, sample_size=sample_sizes[i], testSample=testSample_MSTAR, unseen_image=unseen_image_MSTAR)\n",
    "        avg_mtree_fft_time_SARDET = mtree_fft_query_time(image_size=128, k=k, runs=runs, max_node_size=39, sample_size=sample_sizes[i], testSample=testSample_SARDET, unseen_image=unseen_image_SARDET)\n",
    "        avg_mtree_fft_time_ATRNET = mtree_fft_query_time(image_size=128, k=k, runs=runs, max_node_size=25, sample_size=sample_sizes[i], testSample=testSample_ATRNET, unseen_image=unseen_image_ATRNET)\n",
    "        print(f\"TIMES MTREE: {avg_mtree_fft_time_imdb}, {avg_mtree_fft_time_MSTAR}, {avg_mtree_fft_time_SARDET}, {avg_mtree_fft_time_ATRNET}\")\n",
    "\n",
    "        p_inc_imdb = ((avg_ncc_fft_time_imdb - avg_mtree_fft_time_imdb) / avg_mtree_fft_time_imdb) * 100\n",
    "        p_incs_imdb.append(p_inc_imdb)\n",
    "        print(f\"increase for imdb: {p_inc_imdb}\")\n",
    "\n",
    "        p_inc_MSTAR = ((avg_ncc_fft_time_MSTAR - avg_mtree_fft_time_MSTAR) / avg_mtree_fft_time_MSTAR) * 100\n",
    "        p_incs_MSTAR.append(p_inc_MSTAR)\n",
    "        print(f\"increase for MSTAR: {p_inc_MSTAR}\")\n",
    "\n",
    "        p_inc_SARDET = ((avg_ncc_fft_time_SARDET - avg_mtree_fft_time_SARDET) / avg_mtree_fft_time_SARDET) * 100\n",
    "        p_incs_SARDET.append(p_inc_SARDET)\n",
    "        print(f\"increase for SARDET: {p_inc_SARDET}\")\n",
    "\n",
    "        p_inc_ATRNET = ((avg_ncc_fft_time_ATRNET - avg_mtree_fft_time_ATRNET) / avg_mtree_fft_time_ATRNET) * 100\n",
    "        p_incs_ATRNET.append(p_inc_ATRNET)\n",
    "        print(f\"increase for ATRNET: {p_inc_ATRNET}\")\n",
    "    \n",
    "    print(f\"{p_incs_imdb}\")\n",
    "    print(f\"{p_incs_MSTAR}\")\n",
    "    print(f\"{p_incs_SARDET}\")\n",
    "    print(f\"{p_incs_ATRNET}\")\n",
    "\n",
    "    with open(\"/home/jovyan/evaluation/results/ID_estimator_comparisons.txt\", \"w\") as file:\n",
    "        file.write(f\"sample_sizes = {sample_sizes}\\n\")\n",
    "        file.write(f\"p_incs_imdb = {p_incs_imdb}\\n\")\n",
    "        file.write(f\"p_incs_MSTAR = {p_incs_MSTAR}\\n\")\n",
    "        file.write(f\"p_incs_SARDET = {p_incs_SARDET}\\n\")\n",
    "        file.write(f\"p_incs_ATRNET = {p_incs_ATRNET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [100, 1000, 2000]\n",
    "query_percentage_improvements_mtree_ncc(sample_sizes=sample_sizes, runs=100, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtree_ncc_query_times_sample_size(image_size=128, k=7, runs=100, max_node_size=12, list_data=\"path\", sample_sizes = [], filename=\"\"):\n",
    "\n",
    "    print(f\"Query times of diff methods based on kNN with ncc with image size {image_size}, {k} neighbours, mtree max node size {max_node_size} over {runs} runs\")\n",
    "\n",
    "    # data_list = np.load(list_data)\n",
    "    # data = data_list[\"testSample\"]\n",
    "    data = get_data_MStar(image_size)\n",
    "\n",
    "            \n",
    "    avgs_ncc_pfft = []\n",
    "    avgs_ncc_fft = []\n",
    "    avgs_ncc_unoptim = []\n",
    "    avgs_mtree = []\n",
    "    avgs_mtree_fft = []\n",
    "\n",
    "    for i in range(len(sample_sizes)):\n",
    "        \n",
    "        time_ncc_pfft = 0\n",
    "        time_ncc_fft = 0\n",
    "        time_ncc_unoptim = 0\n",
    "        time_mtree = 0\n",
    "        time_mtree_fft = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(data, sample_indices)\n",
    "        testSample = np.array([item[0] for item in sampled_test_data])\n",
    "        print(\"done with testSample\")\n",
    "        # tree = getMTree(testSample, max_node_size)\n",
    "        tree_fft = getMTreeFFTNumba(testSample, max_node_size)\n",
    "        \n",
    "\n",
    "        print(f\"Now testing with sample size: {sample_sizes[i]}\")\n",
    "        for j in range(runs):\n",
    "\n",
    "            index1 = np.random.randint(len(data))\n",
    "            unseen_image = data[index1][0]\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            arr = np.ones(len(testSample))\n",
    "            unseen_img_arr = ImageProducts.linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "            end_time = time.perf_counter()\n",
    "            time_ncc_pfft += end_time - start_time\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            arr = np.ones(len(testSample))\n",
    "            unseen_img_arr = ImageProducts.linear_ncc_search(testSample, unseen_image, arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "            end_time = time.perf_counter()\n",
    "            time_ncc_fft += end_time - start_time\n",
    "\n",
    "            # start_time = time.perf_counter()\n",
    "            # arr = []\n",
    "            # for j in range(len(testSample)):\n",
    "            #     result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "            #     arr.append(result)\n",
    "            \n",
    "            # unseen_img_arr = np.array(arr)\n",
    "            # imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "            # time_ncc_unoptim += end_time - start_time\n",
    "\n",
    "            # start_time = time.perf_counter()\n",
    "            # knn = getKNearestNeighbours(tree, unseen_image, k)\n",
    "            # end_time = time.perf_counter()\n",
    "            # time_mtree += end_time - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            knn = getKNearestNeighbours(tree_fft, unseen_image, k)\n",
    "            end_time = time.perf_counter()\n",
    "            time_mtree_fft += end_time - start_time\n",
    "        \n",
    "        avg_ncc_pfft = time_ncc_pfft / runs\n",
    "        avgs_ncc_pfft.append(avg_ncc_pfft)\n",
    "        print(f\"Avg time for ncc pfft: {avg_ncc_pfft}\")\n",
    "\n",
    "        avg_ncc_fft = time_ncc_fft / runs\n",
    "        avgs_ncc_fft.append(avg_ncc_fft)\n",
    "        print(f\"Avg time for ncc fft: {avg_ncc_fft}\")\n",
    "\n",
    "        # avg_ncc_unoptim = time_ncc_unoptim / runs\n",
    "        # avgs_ncc_unoptim.append(avg_ncc_unoptim)\n",
    "        # print(f\"Avg time for ncc unoptim: {avg_ncc_unoptim}\")\n",
    "\n",
    "        # avg_mtree = time_mtree / runs\n",
    "        # avgs_mtree.append(avg_mtree)\n",
    "        # print(f\"Avg time for mtree: {avg_mtree}\")\n",
    "\n",
    "        avg_mtree_fft = time_mtree_fft / runs\n",
    "        avgs_mtree_fft.append(avg_mtree_fft)\n",
    "        print(f\"Avg time for mtree fft: {avg_mtree_fft}\")\n",
    "\n",
    "    print(avgs_ncc_pfft)\n",
    "    print(avgs_ncc_fft)\n",
    "    # print(avgs_ncc_unoptim)\n",
    "    # print(avgs_mtree)\n",
    "    print(avgs_mtree_fft)\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(f\"sample_sizes = {sample_sizes}\\n\")\n",
    "        file.write(f\"avgs_ncc_pfft = {avgs_ncc_pfft}\\n\")\n",
    "        file.write(f\"avgs_ncc_fft = {avgs_ncc_fft}\\n\")\n",
    "        # file.write(f\"avgs_ncc_unoptim = {avgs_ncc_unoptim}\\n\")\n",
    "        # file.write(f\"avgs_mtree = {avgs_mtree}\\n\")\n",
    "        file.write(f\"avgs_mtree_fft = {avgs_mtree_fft}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now testing for mstar\n",
    "mtree_ncc_query_times_sample_size(image_size=128, k=7, runs=30, max_node_size=15, list_data=\"path\", sample_sizes = [6000, 7000, 8000, 9000], filename=\"/home/jovyan/evaluation/results/MSTAR_test_query_sample_sizes_3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b605d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(128)\n",
    "\n",
    "sample_indices = random.sample(range(len(data)), 1000)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "testSample = np.array(sampled_test_data)\n",
    "\n",
    "index1 = np.random.randint(len(data))\n",
    "unseen_image = data[index1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769141ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "k = 7\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "arr = np.ones(len(testSample))\n",
    "unseen_img_arr = ImageProducts.linear_ncc_psearch(testSample, unseen_image, arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "end_time = time.perf_counter()\n",
    "print(end_time - start_time)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "arr = np.ones(len(testSample))\n",
    "unseen_img_arr = ImageProducts.linear_ncc_search(testSample, unseen_image, arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):][1:]\n",
    "end_time = time.perf_counter()\n",
    "print(f\"for non parallel: {end_time - start_time}\")\n",
    "# my_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ff399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "print(numba.config.NUMBA_NUM_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad78e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_data(image_size):\n",
    "\n",
    "    data = get_data(image_size)\n",
    "\n",
    "\n",
    "    testSample = [item for item in data]\n",
    "    print(\"Done w testSample\")\n",
    "\n",
    "\n",
    "    filename = \"/home/jovyan/data/annotations/imdb_list_data_all_128.npz\"\n",
    "    np.savez(filename, testSample=testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done w testSample\n"
     ]
    }
   ],
   "source": [
    "save_list_data(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ddd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
