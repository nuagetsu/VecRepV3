{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec884e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../\")\n",
    "sys.path.append(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6552a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from line_profiler import profile\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "import src.data_processing.ImageCalculations as imgcalc\n",
    "import src.visualization.ImagePlots as imgplt\n",
    "import src.helpers.ModelUtilities as models\n",
    "import src.helpers.TCNNModelUtilities as tcnnmodels\n",
    "import src.data_processing.Utilities as utils\n",
    "import src.helpers.FilepathUtils as Futils\n",
    "import src.data_processing.EmbeddingFunctions as embedfunc\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "import src.helpers.MetricUtilities as metrics\n",
    "\n",
    "from src.visualization import BFmethod\n",
    "from functools import partial\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers import get_logits_model, PolyphaseInvariantDown2D, LPS\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers.polydown import set_pool\n",
    "\n",
    "from mtree.mtree import MTree\n",
    "import mtree.mtree as mtree\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f094dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../\")\n",
    "sys.path.append(path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from line_profiler import profile\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "import src.data_processing.ImageCalculations as imgcalc\n",
    "import src.visualization.ImagePlots as imgplt\n",
    "import src.helpers.ModelUtilities as models\n",
    "import src.helpers.TCNNModelUtilities as tcnnmodels\n",
    "import src.data_processing.Utilities as utils\n",
    "import src.helpers.FilepathUtils as Futils\n",
    "import src.data_processing.EmbeddingFunctions as embedfunc\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "import src.helpers.MetricUtilities as metrics\n",
    "\n",
    "from src.visualization import BFmethod\n",
    "from functools import partial\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers import get_logits_model, PolyphaseInvariantDown2D, LPS\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers.polydown import set_pool\n",
    "\n",
    "from mtree.mtree import MTree\n",
    "import mtree.mtree as mtree\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: Put these in a separate file for MTreeUtils...\n",
    "def getKNearestNeighbours(tree, point, k):\n",
    "    l = tree.search(point, k)\n",
    "    imgs = list(l)\n",
    "    return imgs\n",
    "\n",
    "def getMTree(data, k):\n",
    "    # k: desired number of nearest neighbours\n",
    "    tree = MTree(metrics.distance, max_node_size=k)\n",
    "    tree.add_all(data)\n",
    "    return tree\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"../data/imdb_wiki/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.images = []\n",
    "        for class_path in file_list:\n",
    "            for dir_path in glob.glob(class_path + \"/*\"):\n",
    "                for img_path in glob.glob(dir_path + \"/*.jpg\"):\n",
    "                    self.images.append(img_path)\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy()\n",
    "\n",
    "class CustomDatasetMStar(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"../data/mstar/Padded_imgs/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.JPG\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        #print(self.data)\n",
    "        self.class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        class_id = self.class_map[data_path[1]]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), class_id\n",
    "\n",
    "def get_data(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDataset(transform)\n",
    "\n",
    "def get_data_MStar(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDatasetMStar(transform)\n",
    "\n",
    "def mtree_ncc_query_sample_size(max_node_size=12, image_size=32, k=7, runs=2, sample_sizes=[1]):\n",
    "    total_time_ncc = 0\n",
    "    total_time_mtree = 0\n",
    "    avg_times_ncc = []\n",
    "    avg_times_mtree = []\n",
    "\n",
    "    data = get_data_MStar(image_size)\n",
    "\n",
    "    print(f\"Average runtime of querying mtree and ncc for {k} NN over {runs} runs with image size {image_size} and max node size {max_node_size} and variable sample size\")\n",
    "    for i in range(len(sample_sizes)):\n",
    "        print(f\"NOW TRYING sample size: {sample_sizes[i]}\")\n",
    "        total_time_ncc = 0\n",
    "        total_time_mtree = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "        testSample = [item[0] for item in sampled_test_data]\n",
    "        \n",
    "\n",
    "        tree = getMTree(testSample, max_node_size)\n",
    "\n",
    "        # trans = transforms.Compose([transforms.Resize(img_sizes[i])])\n",
    "        # t_MNIST_data = trans(MNIST_data)\n",
    "\n",
    "        # for img in MNIST_data:\n",
    "        #     img = trans(img)\n",
    "\n",
    "        for _ in range(runs):\n",
    "            index1 = np.random.randint(len(data))\n",
    "            #input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "            unseen_image = data[index1][0]\n",
    "\n",
    "            start_time = time.time()\n",
    "            arr = []\n",
    "            for j in range(len(testSample)):\n",
    "                result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "                arr.append(result)\n",
    "            \n",
    "            unseen_img_arr = np.array(arr)\n",
    "            #print(unseen_img_arr)\n",
    "            imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time_ncc += end_time - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            imgs = getKNearestNeighbours(tree, unseen_image, k+1)\n",
    "            end_time = time.time()\n",
    "            total_time_mtree += end_time - start_time\n",
    "\n",
    "        avg_ncc = total_time_ncc / runs\n",
    "        avg_mtree = total_time_mtree / runs\n",
    "        avg_times_ncc.append(avg_ncc)\n",
    "        avg_times_mtree.append(avg_mtree)\n",
    "\n",
    "        # with open(\"test_mtree_query_sample_sizes_avg_times_ncc_.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_ncc)}\")\n",
    "        \n",
    "        # with open(\"test_mtree_query_sample_sizes_avg_times_mtree_.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_mtree)}\")\n",
    "\n",
    "        print(f\"Average runtime of ncc search: {avg_ncc:.6f} seconds\")\n",
    "        print(f\"Average runtime of mtree search: {avg_mtree:.6f} seconds\")\n",
    "    # plot_data_mtree_ncc(x_axis=\"Sample size\", title=f\"Average runtime of finding {k} neighbours with image size {image_size} against sample sizes\", filename=\"test_mtree_query_sample_sizes.png\", \n",
    "    #                     varied_arr=sample_sizes, data1=avg_times_ncc, data2=avg_times_mtree, max_node_size=max_node_size)\n",
    "\n",
    "\n",
    "def mtree_init_sample_size(max_node_size=12, image_size=32, k=7, runs=2, sample_sizes=[1]):\n",
    "    total_time_mtree_init = 0\n",
    "    avg_times_mtree_init = []\n",
    "\n",
    "    data = get_data_MStar(image_size)\n",
    "\n",
    "    print(f\"Average runtime of initialising mtree over {runs} runs with max node size {max_node_size} and image size {image_size} and variable sample sizes\")\n",
    "\n",
    "    for i in range(len(sample_sizes)):\n",
    "        print(f\"NOW TRYING sample size: {sample_sizes[i]}\")\n",
    "        total_time_mtree_init = 0\n",
    "\n",
    "        sample_indices = random.sample(range(len(data)), sample_sizes[i])\n",
    "        sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "        testSample = [item[0] for item in sampled_test_data]\n",
    "        #sampled_test_data[:,0]\n",
    "\n",
    "        for _ in range(runs):\n",
    "            start_time = time.time()\n",
    "            tree = getMTree(testSample, max_node_size)\n",
    "            end_time = time.time()\n",
    "            total_time_mtree_init += end_time - start_time\n",
    "            print(f\"Finished one run of mtree init...\")\n",
    "\n",
    "        \n",
    "        avg_mtree_init = total_time_mtree_init / runs\n",
    "        avg_times_mtree_init.append(avg_mtree_init)\n",
    "\n",
    "        \n",
    "        # with open(\"test_mtree_init_sample_sizes_avg_times_mtree_init.txt\", \"w\") as file:\n",
    "        #     file.write(f\"avg times: {str(avg_times_mtree_init)}\")\n",
    "\n",
    "        print(f\"Average runtime of mtree init: {avg_mtree_init:.6f} seconds\")\n",
    "    \n",
    "    # plot_data_mtree_init(x_axis=\"Sample size\", title=f\"Average runtime of initialising mtree with image size {image_size}, max node size {max_node_size} against sample sizes\", filename=\"test_mtree_init_sample_sizes.png\", \n",
    "    #                     varied_arr=sample_sizes, data=avg_times_mtree_init, max_node_size=max_node_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5946879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average runtime of querying mtree and ncc for 7 NN over 10 runs with image size 368 and max node size 15 and variable sample size\n",
      "NOW TRYING sample size: 10\n",
      "Average runtime of ncc search: 0.365737 seconds\n",
      "Average runtime of mtree search: 0.357674 seconds\n",
      "NOW TRYING sample size: 100\n",
      "Average runtime of ncc search: 3.548275 seconds\n",
      "Average runtime of mtree search: 3.337489 seconds\n",
      "NOW TRYING sample size: 1000\n",
      "Average runtime of ncc search: 36.673873 seconds\n",
      "Average runtime of mtree search: 34.534373 seconds\n",
      "NOW TRYING sample size: 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 9466 samples\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# note MStar is 368 by 368...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmtree_ncc_query_sample_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_node_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m368\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mmtree_ncc_query_sample_size\u001b[39m\u001b[34m(max_node_size, image_size, k, runs, sample_sizes)\u001b[39m\n\u001b[32m    151\u001b[39m sampled_test_data = Subset(data, sample_indices)\n\u001b[32m    153\u001b[39m testSample = [item[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sampled_test_data]\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m tree = \u001b[43mgetMTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestSample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_node_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# trans = transforms.Compose([transforms.Resize(img_sizes[i])])\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# t_MNIST_data = trans(MNIST_data)\u001b[39;00m\n\u001b[32m    160\u001b[39m \n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# for img in MNIST_data:\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m#     img = trans(img)\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(runs):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mgetMTree\u001b[39m\u001b[34m(data, k)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetMTree\u001b[39m(data, k):\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# k: desired number of nearest neighbours\u001b[39;00m\n\u001b[32m     58\u001b[39m     tree = MTree(metrics.distance, max_node_size=k)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:220\u001b[39m, in \u001b[36mMTree.add_all\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m#TODO: implement using the bulk-loading algorithm\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:211\u001b[39m, in \u001b[36mMTree.add\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[32m    208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[33;03m    Add an object into the M-tree\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m.size += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:596\u001b[39m, in \u001b[36mInternalNode.add\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m entry\n\u001b[32m    594\u001b[39m entry = find_best_entry_requiring_no_covering_radius_increase() \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m    595\u001b[39m     find_best_entry_minimizing_radius_increase()\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:596\u001b[39m, in \u001b[36mInternalNode.add\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m entry\n\u001b[32m    594\u001b[39m entry = find_best_entry_requiring_no_covering_radius_increase() \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m    595\u001b[39m     find_best_entry_minimizing_radius_increase()\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent_node\n",
      "    \u001b[31m[... skipping similar frames: add at line 596 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:596\u001b[39m, in \u001b[36mInternalNode.add\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m entry\n\u001b[32m    594\u001b[39m entry = find_best_entry_requiring_no_covering_radius_increase() \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m    595\u001b[39m     find_best_entry_minimizing_radius_increase()\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:493\u001b[39m, in \u001b[36mLeafNode.add\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m.entries.add(new_entry)\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:733\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(existing_node, entry, d)\u001b[39m\n\u001b[32m    730\u001b[39m     new_node.parent_node = new_root_node\n\u001b[32m    731\u001b[39m     new_root_node.add_entry(new_node_entry)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     mtree.root = new_root_node\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    735\u001b[39m     parent_node = existing_node.parent_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:446\u001b[39m, in \u001b[36mAbstractNode.set_entries_and_parent_entry\u001b[39m\u001b[34m(self, new_entries, new_parent_entry)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28mself\u001b[39m.parent_entry = new_parent_entry\n\u001b[32m    445\u001b[39m \u001b[38;5;28mself\u001b[39m.parent_entry.radius = \u001b[38;5;28mself\u001b[39m.covering_radius_for(\u001b[38;5;28mself\u001b[39m.parent_entry.obj)\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_entries_distance_to_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mtree/mtree.py:454\u001b[39m, in \u001b[36mAbstractNode._update_entries_distance_to_parent\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parent_entry:\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.entries:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m         entry.distance_to_parent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent_entry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/helpers/MetricUtilities.py:15\u001b[39m, in \u001b[36mdistance\u001b[39m\u001b[34m(image1, image2)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistance\u001b[39m(image1, image2):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m#print(image1)\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m#print(image2)\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#print(min(ImageProducts.ncc(image1, image2), 1))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     ncc = \u001b[38;5;28mmin\u001b[39m(\u001b[43mImageProducts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mncc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     ncc = \u001b[38;5;28mmax\u001b[39m(ncc, \u001b[32m0\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m#print(math.acos(ImageProducts.ncc(image1, image2)))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/data_processing/ImageProducts.py:160\u001b[39m, in \u001b[36mncc\u001b[39m\u001b[34m(mainImg, tempImg)\u001b[39m\n\u001b[32m    157\u001b[39m mainImg = np.asarray(mainImg, np.single)  \u001b[38;5;66;03m# Setting data types of array\u001b[39;00m\n\u001b[32m    158\u001b[39m tempImg = np.asarray(tempImg, np.single)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m corr = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatchTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmainImg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempImg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTM_CCORR_NORMED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(corr)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m max_val\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 9466 samples\n",
    "# note MStar is 368 by 368...\n",
    "\n",
    "mtree_ncc_query_sample_size(max_node_size=15, image_size=368, k=7, runs=10, sample_sizes=[10, 100, 1000, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d96f5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9466\n",
      "Unseen img class: 3\n",
      "Index 57, Class 4\n",
      "Index 46, Class 7\n",
      "Index 29, Class 4\n",
      "Index 74, Class 7\n",
      "Index 81, Class 7\n",
      "Index 93, Class 4\n",
      "Index 82, Class 4\n",
      "Index 52, Class 3\n",
      "1\n",
      "[52, 82, 93, 74, 29, 46, 57]\n",
      "[57 46 29 74 81 93 82 52]\n"
     ]
    }
   ],
   "source": [
    "k=7\n",
    "data = get_data_MStar(368)\n",
    "print(len(data))\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = [item[0] for item in sampled_test_data]\n",
    "classes = [item[1] for item in sampled_test_data]\n",
    "\n",
    "# class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "tree = getMTree(testSample, 12)\n",
    "\n",
    "index1 = random.choice(sample_indices)\n",
    "#input1=input_dataset[index1][0].squeeze().to('cpu')\n",
    "unseen_image = data[index1][0]\n",
    "unseen_image_class = data[index1][1]\n",
    "\n",
    "arr = []\n",
    "for j in range(len(testSample)):\n",
    "    result = ImageProducts.ncc_scaled(testSample[j], unseen_image)\n",
    "    arr.append(result)\n",
    "\n",
    "unseen_img_arr = np.array(arr)\n",
    "#print(unseen_img_arr)\n",
    "imgProd_max_index = np.argpartition(unseen_img_arr, -(k+1))[-(k+1):]\n",
    "\n",
    "num_same_class = 0\n",
    "\n",
    "print(f\"Unseen img class: {unseen_image_class}\")\n",
    "for i in imgProd_max_index:\n",
    "    img_class = classes[i]\n",
    "    print(f\"Index {i}, Class {img_class}\")\n",
    "    if (img_class == unseen_image_class):\n",
    "        num_same_class += 1\n",
    "\n",
    "print(num_same_class)\n",
    "\n",
    "\n",
    "imgs = getKNearestNeighbours(tree, unseen_image, k+1)\n",
    "\n",
    "\n",
    "def imgs_to_indices(img_arr, testSample):\n",
    "    ind_arr = []\n",
    "    for img in img_arr:\n",
    "        for i in range(len(testSample)):\n",
    "            if (metrics.distance(img, testSample[i]) < 0.00001):\n",
    "                ind_arr.append(i)\n",
    "                break\n",
    "    \n",
    "    return ind_arr\n",
    "\n",
    "ind_arr = imgs_to_indices(imgs, testSample)\n",
    "print(ind_arr)\n",
    "print(imgProd_max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdd451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
