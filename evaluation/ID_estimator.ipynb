{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b259e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../\")\n",
    "sys.path.append(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cc53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "import src.helpers.MetricUtilities as metrics\n",
    "import src.data_processing.ImageProducts as ImageProducts\n",
    "\n",
    "from mtree.mtree import MTree\n",
    "import mtree.mtree as mtree\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b523b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"../data/imdb_wiki/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.images = []\n",
    "        for class_path in file_list:\n",
    "            for dir_path in glob.glob(class_path + \"/*\"):\n",
    "                for img_path in glob.glob(dir_path + \"/*.jpg\"):\n",
    "                    self.images.append(img_path)\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy()\n",
    "\n",
    "def get_data(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDataset(transform)\n",
    "\n",
    "\n",
    "class CustomDatasetMStar(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"/home/jovyan/data/mstar/Padded_imgs/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.JPG\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        #print(self.data)\n",
    "        self.class_map = {\"2S1\" : 0, \"BRDM_2\": 1, \"BTR_60\": 2, \"D7\": 3, \"SLICY\": 4, \"T62\": 5, \"ZIL131\": 6, \"ZSU_23_4\": 7}\n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        class_id = self.class_map[data_path[1]]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), class_id\n",
    "    \n",
    "def get_data_MStar(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDatasetMStar(transform)\n",
    "\n",
    "class CustomDatasetSARDet_100k(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs_path = \"/home/jovyan/data/SARDet_100k/SARDet_100K/JPEGImages/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "\n",
    "        # with open(\"../data/SARDet_100k/SARDet_100K/mapping.json\") as annotations:\n",
    "        #     mappings = json.load(annotations)\n",
    "\n",
    "        for dir_path in file_list:\n",
    "            for img_path in glob.glob(dir_path + \"/*.png\"):\n",
    "                # self.data.append([img_path, mappings[img_path.split(\"/\")[-1]]])\n",
    "                self.data.append([img_path, \"yippee\"])\n",
    "            for img_path in glob.glob(dir_path + \"/*.jpg\"):\n",
    "                # self.data.append([img_path, mappings[img_path.split(\"/\")[-1]]])\n",
    "                self.data.append([img_path, \"yippee\"])\n",
    "        \n",
    "\n",
    "    # Defining the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Defining the method to get an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.data[index]\n",
    "        image = Image.open(data_path[0])\n",
    "        image = transforms.functional.to_grayscale(image)\n",
    "        # class_id = self.class_map[data_path[1]]\n",
    "        class_id = data_path[1]\n",
    "        # Applying the transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image.squeeze().to('cpu').numpy(), class_id\n",
    "\n",
    "def get_data_SARDet_100k(size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return CustomDatasetSARDet_100k(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a003c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def packing_dim(r_1, r_2, epsilon, testSample, d):\n",
    "    l = 1\n",
    "    L_1 = []\n",
    "    L_2 = []\n",
    "    while True:\n",
    "        S_n = np.random.permutation(testSample)\n",
    "        C = []\n",
    "        for i in range(len(S_n)):\n",
    "            badPoint = False\n",
    "            for j in range(len(C)):\n",
    "                if (d(S_n[i], C[j]) < r_1):\n",
    "                    # print(d(S_n[i], C[j]))\n",
    "                    badPoint = True\n",
    "            if (not badPoint):\n",
    "                # print(\"not badpoint\")\n",
    "                C.append(S_n[i])\n",
    "        L_1.append(math.log(len(C)))\n",
    "        # print(f\"L1: {L_1}\")\n",
    "        C = []\n",
    "        for i in range(len(testSample)):\n",
    "            badPoint = False\n",
    "            for j in range(len(C)):\n",
    "                if (d(S_n[i], C[j]) < r_2):\n",
    "                    badPoint = True\n",
    "            if (not badPoint):\n",
    "                # print(\"not badpoint\")\n",
    "                C.append(S_n[i])\n",
    "                # print(f\"C: {C}\")\n",
    "        L_2.append(math.log(len(C)))\n",
    "        # print(f\"L2: {L_2}\")\n",
    "        \n",
    "        num = (np.sum(L_1) / len(L_1)) - (np.sum(L_2) / len(L_2))\n",
    "        dem = math.log(r_2) - math.log(r_1)\n",
    "        if (num == 0):\n",
    "            D_pack = 0\n",
    "        else:\n",
    "            D_pack = num / dem\n",
    "        # D_pack = num / ()\n",
    "        # print(f\"l: {l} and D_pack: {D_pack}\")\n",
    "        if (l > 10):\n",
    "            # print(\"l > 10 now\")\n",
    "            numerator = math.sqrt(statistics.variance(L_1) + statistics.variance(L_2))\n",
    "            denom = math.sqrt(l) * (math.log(r_2) - math.log(r_1))\n",
    "            # print(numerator / denom)\n",
    "            if (1.65 * (numerator / denom) < D_pack * (1-epsilon) / 2):\n",
    "                return D_pack\n",
    "        l += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d519c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = \"/home/jovyan/data/ATRNet-STAR_annotations/list_data_all.npz\"\n",
    "\n",
    "data = np.load(list_data)\n",
    "all_data = data[\"testSample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e507744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sample_indices = np.array(random.sample(range(len(all_data), 10)))\n",
    "sample_indices = np.array(random.sample(range(len(all_data)), 100))\n",
    "# print(sample_indices)\n",
    "# sample_indices = np.array(random.sample(range(len(entry)), left_len))\n",
    "testSample = all_data[sample_indices]\n",
    "# print(testSample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8235ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "IMDB_WIKI_data = get_data(128)\n",
    "sample_indices = random.sample(range(len(IMDB_WIKI_data)), 100)\n",
    "sampled_test_data = Subset(IMDB_WIKI_data, sample_indices)\n",
    "\n",
    "testSample = np.array(sampled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e74f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = get_data_MStar(128)\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = np.array([item[0] for item in sampled_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f5af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = get_data_SARDet_100k(128)\n",
    "sample_indices = random.sample(range(len(data)), 100)\n",
    "sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "testSample = np.array([item[0] for item in sampled_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1b407ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_distance(runs=3, testSample=[]):\n",
    "    avg_dist = 0\n",
    "    for i in range(len(testSample)):\n",
    "        for j in range(len(testSample)):\n",
    "            avg_dist += metrics.dist_fft_numba(testSample[i], testSample[j])\n",
    "    \n",
    "    return avg_dist / (len(testSample) ** 2)\n",
    "\n",
    "    # for _ in range(runs):\n",
    "        \n",
    "    #     sample_indices = random.sample(range(len(testSample)), 10)\n",
    "    #     sampled_test_data = testSample[sample_indices]\n",
    "\n",
    "    #     for i in range(10):\n",
    "    #         for j in range(10):\n",
    "    #             avg_dist += metrics.dist_fft_numba(sampled_test_data[i], sampled_test_data[j])\n",
    "    \n",
    "    # return avg_dist / ((10 ** 2) * runs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa8e4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "amt = get_avg_distance(runs=3, testSample=testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2479bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.366688015469522\n"
     ]
    }
   ],
   "source": [
    "print(amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf0677f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3346942111177769\n"
     ]
    }
   ],
   "source": [
    "r_1 = 0.25\n",
    "r_2 = 0.3\n",
    "epsilon = 0.01\n",
    "\n",
    "D_pack = packing_dim(r_1, r_2, epsilon, testSample, metrics.dist_fft_numba)\n",
    "print(D_pack)\n",
    "\n",
    "# these are r = 0.1m r2 = 0.2 or r=0.01, r2= 0.02 i forgot, epsilon 0.5\n",
    "# r=0.1 r2=0.2 works...\n",
    "# For ATRNET 100 its 3.4 in 19s\n",
    "# For IMDB its 0.18 omg IT CORRELATES!!! in 1m\n",
    "# For MStar its 0.99 oh thats cool! in 32s\n",
    "# For SARDET its 0.60 oh funsies. in 24s\n",
    "# Ok it all actl correlates perfectly lovelyyyy\n",
    "# Mayb I shld j download random ass datasets to try\n",
    "\n",
    "# epsilon = 0.01... seems a bit sus. I think epsilon should be 0.99 for 99% accuracy instead thats why its taking such a short while OH WAIT nvm 0.01 is correct\n",
    "# these results are so wrong TT\n",
    "# for IMDB w average distancing r and epsilon 0.01... r1=0.35519913309165985, r2= r1 / 2,  ss=100, 2.06670927032519\n",
    "# for MSTAR w average distancing r and epsilon 0.01... r1=0.2874618664410695, r2= r1 / 2,  ss=100, 0.924799078470416\n",
    "# for SARDET w average distancing r and epsilon 0.01... r1=0.27816828240437047, r2= r1 / 2,  ss=100, 1.1172678641872351\n",
    "# for ATRNET w average distancing r and epsilon 0.01... r1=0.11801859617485709, r2= r1 / 2,  ss=100, 1.4447639205568288\n",
    "\n",
    "\n",
    "# IMDB epsilon=0.95, 0.3762601202257626, 3.2288575970384\n",
    "# MSTAR 0.245971864754331, 1.1003957200143155\n",
    "# SARDET 0.23582069057274893, 0.6553306229013383\n",
    "# ATRNET 0.11826122486459371, 1.6479560136159335\n",
    "\n",
    "# IMDB epsilon = 0.01, r1=0.366688015469522 again ss=100\n",
    "\n",
    "# for IMDB 100 r1=0.2, r2=0.3, epsilon=0.01, 1.9178145789982848\n",
    "# for ATRNET 100 r1=0.2, r2=0.3, epsilon=0.01, 2.709511291351456\n",
    "\n",
    "# I should plot w consectuive values of a sequence of r plotted at halfway or smt like that... maybs the rs can be 0.05, 0.1, 0.15, 0.2, 0.25, 0.3\n",
    "\n",
    "# for ATRNET 100 epsilon=0.01,???????????\n",
    "# r_1=0.05, r_2=0.1 0.7951654750241648\n",
    "# r_1=0.1, r_2=0.15 2.396874022214933\n",
    "# r_1=0.15, r_2=0.2 3.8137837643096293\n",
    "# r_1=0.2, r_2=0.25 7.1641061097411445\n",
    "# r_1=0.25, r_2=0.3 0.7361277852123475\n",
    "\n",
    "# for IMDB 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 0.15505280747294337\n",
    "# r_1=0.15, r_2=0.2 0.40399419654591423 \n",
    "# r_1=0.2, r_2=0.25 1.0072751096835382\n",
    "# r_1=0.25, r_2=0.3 2.4780407589364066\n",
    "\n",
    "\n",
    "# for MSTAR 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 \n",
    "# r_1=0.15, r_2=0.2 \n",
    "# r_1=0.2, r_2=0.25 \n",
    "# r_1=0.25, r_2=0.3 \n",
    "\n",
    "# for SARDET 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 \n",
    "# r_1=0.15, r_2=0.2 \n",
    "# r_1=0.2, r_2=0.25 \n",
    "# r_1=0.25, r_2=0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a039ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_packings_imdb(r1, r2, epsilon):\n",
    "    IMDB_WIKI_data = get_data(128)\n",
    "    sample_indices = random.sample(range(len(IMDB_WIKI_data)), 100)\n",
    "    sampled_test_data = Subset(IMDB_WIKI_data, sample_indices)\n",
    "\n",
    "    testSample = np.array(sampled_test_data)\n",
    "\n",
    "    return packing_dim(r1, r2, epsilon, testSample, metrics.dist_fft_numba)\n",
    "\n",
    "def calc_packings_MSTAR(r1,r2,epsilon):\n",
    "    data = get_data_MStar(128)\n",
    "    sample_indices = random.sample(range(len(data)), 100)\n",
    "    sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "    testSample = np.array([item[0] for item in sampled_test_data])\n",
    "\n",
    "    return packing_dim(r1, r2, epsilon, testSample, metrics.dist_fft_numba)\n",
    "\n",
    "def calc_packings_SARDET(r1,r2,epsilon):\n",
    "    data = get_data_SARDet_100k(128)\n",
    "    sample_indices = random.sample(range(len(data)), 100)\n",
    "    sampled_test_data = Subset(data, sample_indices)\n",
    "\n",
    "    testSample = np.array([item[0] for item in sampled_test_data])\n",
    "    \n",
    "    return packing_dim(r1, r2, epsilon, testSample, metrics.dist_fft_numba)\n",
    "\n",
    "def calc_packings_ATRNET(r1,r2,epsilon):\n",
    "    list_data = \"/home/jovyan/data/ATRNet-STAR_annotations/list_data_all.npz\"\n",
    "    data = np.load(list_data)\n",
    "    all_data = data[\"testSample\"]\n",
    "    sample_indices = np.array(random.sample(range(len(all_data)), 100))\n",
    "    testSample = all_data[sample_indices]\n",
    "    \n",
    "    return packing_dim(r1, r2, epsilon, testSample, metrics.dist_fft_numba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_packings(r=[], epsilon=0.01, runs=5, filename=\"\"):\n",
    "    avgs_ATRNET = []\n",
    "    avgs_IMDB = []\n",
    "    avgs_MSTAR = []\n",
    "    avgs_SARDET = []\n",
    "\n",
    "    for i in range(1, len(r)):\n",
    "        avg_ATRNET = 0\n",
    "        avg_IMDB = 0\n",
    "        avg_MSTAR = 0\n",
    "        avg_SARDET = 0\n",
    "        for j in range(runs):\n",
    "            avg_ATRNET += calc_packings_ATRNET(r[i-1],r[i],epsilon)\n",
    "            avg_IMDB += calc_packings_imdb(r[i-1],r[i],epsilon)\n",
    "            avg_MSTAR += calc_packings_MSTAR(r[i-1],r[i],epsilon)\n",
    "            avg_SARDET += calc_packings_SARDET(r[i-1],r[i],epsilon)\n",
    "\n",
    "        avg_ATRNET = avg_ATRNET / runs\n",
    "        avg_IMDB = avg_IMDB / runs\n",
    "        avg_MSTAR = avg_MSTAR / runs\n",
    "        avg_SARDET = avg_SARDET / runs\n",
    "\n",
    "        avgs_ATRNET.append(avg_ATRNET)\n",
    "        avgs_IMDB.append(avg_IMDB)\n",
    "        avgs_MSTAR.append(avg_MSTAR)\n",
    "        avgs_SARDET.append(avg_SARDET)\n",
    "    \n",
    "    print(f\"ATRNET: {avgs_ATRNET}\")\n",
    "    print(f\"IMDB: {avgs_IMDB}\")\n",
    "    print(f\"MSTAR: {avgs_MSTAR}\")\n",
    "    print(f\"SARDET: {avgs_SARDET}\")\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"r: {r}\\n\")\n",
    "        f.write(f\"ATRNET: {avgs_ATRNET}\\n\")\n",
    "        f.write(f\"IMDB: {avgs_IMDB}\\n\")\n",
    "        f.write(f\"MSTAR: {avgs_MSTAR}\\n\")\n",
    "        f.write(f\"SARDET: {avgs_SARDET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ae1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4780407589364066\n"
     ]
    }
   ],
   "source": [
    "r_1 = 0.25\n",
    "r_2 = 0.3\n",
    "epsilon = 0.01\n",
    "\n",
    "D_pack = packing_dim(r_1, r_2, epsilon, testSample, metrics.dist_fft_numba)\n",
    "print(D_pack)\n",
    "\n",
    "# these are r = 0.1m r2 = 0.2 or r=0.01, r2= 0.02 i forgot, epsilon 0.5\n",
    "# r=0.1 r2=0.2 works...\n",
    "# For ATRNET 100 its 3.4 in 19s\n",
    "# For IMDB its 0.18 omg IT CORRELATES!!! in 1m\n",
    "# For MStar its 0.99 oh thats cool! in 32s\n",
    "# For SARDET its 0.60 oh funsies. in 24s\n",
    "# Ok it all actl correlates perfectly lovelyyyy\n",
    "# Mayb I shld j download random ass datasets to try\n",
    "\n",
    "# epsilon = 0.01... seems a bit sus. I think epsilon should be 0.99 for 99% accuracy instead thats why its taking such a short while OH WAIT nvm 0.01 is correct\n",
    "# these results are so wrong TT\n",
    "# for IMDB w average distancing r and epsilon 0.01... r1=0.35519913309165985, r2= r1 / 2,  ss=100, 2.06670927032519\n",
    "# for MSTAR w average distancing r and epsilon 0.01... r1=0.2874618664410695, r2= r1 / 2,  ss=100, 0.924799078470416\n",
    "# for SARDET w average distancing r and epsilon 0.01... r1=0.27816828240437047, r2= r1 / 2,  ss=100, 1.1172678641872351\n",
    "# for ATRNET w average distancing r and epsilon 0.01... r1=0.11801859617485709, r2= r1 / 2,  ss=100, 1.4447639205568288\n",
    "\n",
    "\n",
    "# IMDB epsilon=0.95, 0.3762601202257626, 3.2288575970384\n",
    "# MSTAR 0.245971864754331, 1.1003957200143155\n",
    "# SARDET 0.23582069057274893, 0.6553306229013383\n",
    "# ATRNET 0.11826122486459371, 1.6479560136159335\n",
    "\n",
    "# IMDB epsilon = 0.01, r1=0.366688015469522 again ss=100\n",
    "\n",
    "# for IMDB 100 r1=0.2, r2=0.3, epsilon=0.01, 1.9178145789982848\n",
    "# for ATRNET 100 r1=0.2, r2=0.3, epsilon=0.01, 2.709511291351456\n",
    "\n",
    "# I should plot w consectuive values of a sequence of r plotted at halfway or smt like that... maybs the rs can be 0.05, 0.1, 0.15, 0.2, 0.25, 0.3\n",
    "\n",
    "# for ATRNET 100 epsilon=0.01,???????????\n",
    "# r_1=0.05, r_2=0.1 0.7951654750241648\n",
    "# r_1=0.1, r_2=0.15 2.396874022214933\n",
    "# r_1=0.15, r_2=0.2 3.8137837643096293\n",
    "# r_1=0.2, r_2=0.25 7.1641061097411445\n",
    "# r_1=0.25, r_2=0.3 0.7361277852123475\n",
    "\n",
    "# for IMDB 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 0.15505280747294337\n",
    "# r_1=0.15, r_2=0.2 0.40399419654591423 \n",
    "# r_1=0.2, r_2=0.25 1.0072751096835382\n",
    "# r_1=0.25, r_2=0.3 \n",
    "\n",
    "\n",
    "# for MSTAR 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 \n",
    "# r_1=0.15, r_2=0.2 \n",
    "# r_1=0.2, r_2=0.25 \n",
    "# r_1=0.25, r_2=0.3 \n",
    "\n",
    "# for SARDET 100 epsilon=0.01\n",
    "# r_1=0.05, r_2=0.1 \n",
    "# r_1=0.1, r_2=0.15 \n",
    "# r_1=0.15, r_2=0.2 \n",
    "# r_1=0.2, r_2=0.25 \n",
    "# r_1=0.25, r_2=0.3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
