{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2b1d4-13ca-4ef3-8365-11ac32346de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.abspath(\"../../VecRepV3\") \n",
    "sys.path.append(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c4123-7d8e-40b5-8a5f-3e657ad699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Sampler, random_split, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from line_profiler import profile\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "import src.data_processing.BruteForceEstimator as bfEstimator\n",
    "import src.data_processing.ImageCalculations as imgcalc\n",
    "import src.visualization.ImagePlots as imgplt\n",
    "import src.helpers.ModelUtilities as models\n",
    "import src.data_processing.Utilities as utils\n",
    "import src.helpers.FilepathUtils as Futils\n",
    "\n",
    "from src.visualization import SamplingMethod, BFmethod\n",
    "from src.data_processing.SampleEstimator import SampleEstimator\n",
    "from functools import partial\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers import get_logits_model, PolyphaseInvariantDown2D, LPS\n",
    "from learnable_polyphase_sampling.learn_poly_sampling.layers.polydown import set_pool\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f17c94-c8f0-421a-be0f-2ece071ead62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------Image Input----------------------------------\n",
    "IMAGE_TYPES = [\"NbinMmax_ones\", \"Nbin\", \"triangles\", \"triangle_mean_subtracted\"]\n",
    "\n",
    "IMAGE_FILTERS = [\"unique\", \"Nmax_ones\", \"one_island\"]\n",
    "\n",
    "IMAGE_PRODUCT_TYPES = [\"ncc\", \"ncc_scaled\"]\n",
    "\n",
    "EMBEDDING_TYPES = [\"pencorr_D\"]\n",
    "\n",
    "dimensions = 32\n",
    "\n",
    "imageType = \"shapes_3_dims_6_3\" #6x6 triangle in 12x12 matrix shapes_3_dims_6_3\n",
    "filters = [\"unique\"]\n",
    "imageProductType = \"ncc_scaled_-1\"\n",
    "overwrite = {\"imgSet\": False, \"imgProd\": False, \"embedding\": False}\n",
    "weight = None\n",
    "embeddingType = f\"pencorr_{dimensions}\"\n",
    "k=5\n",
    "percentage = 0.1\n",
    "\n",
    "imageSet = utils.generate_filtered_image_set(imageType, filters, Futils.get_image_set_filepath(imageType, filters))\n",
    "\n",
    "imageSet = np.array(imageSet)\n",
    "\n",
    "testSize = int(percentage * len(imageSet)) \n",
    "trainingSize = len(imageSet) - testSize\n",
    "testSample, trainingSample = SamplingMethod.generate_random_sample(imageSet, testSize, trainingSize)\n",
    "sampleName = f\"{imageType} {filters} {percentage} sample\"\n",
    "\n",
    "sampleEstimator = SampleEstimator(sampleName=sampleName, embeddingType=embeddingType, imageProductType=imageProductType)\n",
    "\n",
    "index1 = np.random.randint(len(testSample))\n",
    "index2 = np.random.randint(len(testSample))\n",
    "\n",
    "input1=testSample[index1]\n",
    "input2=testSample[index2]\n",
    "\n",
    "imgplt.plot_original_images(input1, input2, index1, index2)\n",
    "\n",
    "# ------------------------- Preprocessing Dataset ------------------------\n",
    "input_dataset = []\n",
    "for img in testSample:\n",
    "    img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "    input_dataset.append(img_tensor)\n",
    "input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "\n",
    "stacked_tensor = torch.stack(input_dataset)\n",
    "input_dataset = stacked_tensor.cpu().numpy()      \n",
    "input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f837a-2450-4b5a-9af0-919ddc7e2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SimpleCNN(dimensions=dimensions, padding_mode='circular').to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), \"model\", \n",
    "                                               f'best_model_{imageType}_{dimension}d.pt')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10228c-7eb7-4619-861e-768c94b38933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Metric 1 - Loss Calculation-----------------\n",
    "NCC_scaled_value = imgcalc.get_NCC_score(input1, input2)\n",
    "print(\"\\nLoss Calculation\")\n",
    "print(\"\\nScaled NCC: \",NCC_scaled_value)\n",
    "\n",
    "embedded_vector_image1 = model(input_dataset[index1])\n",
    "embedded_vector_image2 = model(input_dataset[index2])\n",
    "\n",
    "dot_product_value = imgcalc.get_dp_score(embedded_vector_image1, embedded_vector_image2)\n",
    "\n",
    "print(\"Dot product value of model: \", dot_product_value.item())\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(dot_product_value, NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "\n",
    "matrixG = sampleEstimator.imageProductMatrix  \n",
    "matrixA = sampleEstimator.embeddingMatrix \n",
    "dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "dot_product_value_Pencorr = dot_product_matrix[index1][index2]\n",
    "difference = abs(dot_product_value_Pencorr - dot_product_value)\n",
    "\n",
    "print(\"\\nDot product value of BF Method: \", dot_product_value_Pencorr)\n",
    "\n",
    "train_loss_value = imgcalc.get_loss_value(torch.tensor(dot_product_value_Pencorr), NCC_scaled_value) \n",
    "print(\"Loss: \", train_loss_value)\n",
    "print(\"Difference in values of BF Method and Model Method: \", difference.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5c82c-09bb-4629-8477-c1f01a2c004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Metric 2 - KNNIoU-----------------\n",
    "print(\"----------------\\nBrute Force Method -- KNN-IOU score\\n----------------\")\n",
    "vectorb_bf1 = matrixG[index1]\n",
    "vectorc_bf1 = imgcalc.get_vectorc_brute(index1, matrixA)\n",
    "imgplt.display_and_plot_results(vectorb_bf1, vectorc_bf1, \"Brute Force\", index1, k, testSample)\n",
    "\n",
    "vectorb_bf2 = matrixG[index2]\n",
    "vectorc_bf2 = imgcalc.get_vectorc_brute(index2, matrixA)\n",
    "imgplt.display_and_plot_results(vectorb_bf2, vectorc_bf2, \"Brute Force\", index2, k, testSample)\n",
    "\n",
    "print(\"----------------\\nModel Method -- KNN-IOU score\\n----------------\")\n",
    "vectorb_model1 = matrixG[index1]\n",
    "vectorc_model1 = imgcalc.get_vectorc_model(index1, model, input_dataset)\n",
    "imgplt.display_and_plot_results(vectorb_model1, vectorc_model1, \"Model\", index1, k, testSample)\n",
    "\n",
    "vectorb_model2 = matrixG[index2]\n",
    "vectorc_model2 = imgcalc.get_vectorc_model(index2, model, input_dataset)\n",
    "imgplt.display_and_plot_results(vectorb_model2, vectorc_model2, \"Model\", index2, k, testSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8d7f1-be62-4447-8314-ba8daa794bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Visualisation across dataset-------------------\n",
    "#input(array_dataset, tensor_dataset)\n",
    "kscores, losses, ncc_loss_dict = imgcalc.kscore_loss_evaluation(testSample, input_dataset, model, k)\n",
    "\n",
    "imgcalc.loss_per_ncc_score(ncc_loss_dict)\n",
    "imgplt.plot_score_distribution(kscores, \"K-Score\")\n",
    "imgplt.plot_score_distribution(losses, \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c7817-2b0a-4313-b106-7e92c6640c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Visualisation across dataset across dimensions-------------------\n",
    "dimensions = [32, 64, 128, 192, 256, 384, 512]\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"Dimension {dim}\")\n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "    sampleName = f\"{imageType} {filters} {percentage} sample\"\n",
    "\n",
    "    sampleEstimator = SampleEstimator(sampleName=sampleName, embeddingType=embeddingType, imageProductType=imageProductType)\n",
    "    \n",
    "    input_dataset = []\n",
    "    for img in testSample:\n",
    "        img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "        img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "        input_dataset.append(img_tensor)\n",
    "    input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "    stacked_tensor = torch.stack(input_dataset)\n",
    "    input_dataset = stacked_tensor.cpu().numpy()      \n",
    "    input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "    num = len(input_dataset)\n",
    "    \n",
    "    model = models.SimpleCNN(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), \"model\", \n",
    "                     f'best_model_{imageType}_{dimension}d.pt'), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    kscores, losses, ncc_loss_dict = imgcalc.kscore_loss_evaluation(testSample, input_dataset, model, k)\n",
    "   \n",
    "    imgcalc.loss_per_ncc_score(ncc_loss_dict)\n",
    "    imgplt.plot_score_distribution(kscores, \"K-Score\")\n",
    "    imgplt.plot_score_distribution(losses, \"Loss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca911d4d-5fbe-4d5d-b8ed-6bcc32f35e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Before transforming output embedding matrices-----------------------------\n",
    "# ----------- Preprocessing dataset -------------\n",
    "matrixG = sampleEstimator.imageProductMatrix  \n",
    "matrixA = sampleEstimator.embeddingMatrix \n",
    "dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, dot_product_matrix)\n",
    "print(\"Mean Squared Difference of Pencorr (A'A) and NCC (G):\", mean_squared_difference)\n",
    "\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_matrix = imgcalc.get_matrix_embeddings(input_dataset, model_vectors)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, model_matrix.detach().cpu().numpy())\n",
    "print(\"\\nMean Squared Difference of Model and NCC (G):\", mean_squared_difference)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(dot_product_matrix, model_matrix.detach().cpu().numpy())\n",
    "print(\"Mean Squared Difference of Model and Pencorr (A'A):\", mean_squared_difference)\n",
    "    \n",
    "print(f\"\\nPrinting matrices\")\n",
    "print(f\"\\nMatrix G: {matrixG}\")\n",
    "print(f\"\\nMatrix A'A (Pencorr): {dot_product_matrix}, {dot_product_matrix.shape}\")\n",
    "print(f\"\\nMatrix A'A (Model): {model_matrix.detach().cpu().numpy()}, {model_matrix.detach().cpu().numpy().shape}\")\n",
    "\n",
    "print(f\"\\nPrinting vectors\")\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "print(f\"Embedding of image {index1} for Model: {model_vectors[index1]}\")\n",
    "\n",
    "print(f\"\\nDifferences between vector embeddings\")\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_vectors[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Pencorr (A) and Model: {mean_squared_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4b7c8-62cc-434b-9f9c-e5cc8d1eb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------After transforming output embedding matrices via Orthorgonal Procrustes ----------------\n",
    "print(f\"Difference between matrices\")\n",
    "model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "model_matrix_transformed = imgcalc.get_matrix_embeddings(input_dataset, model_transformed)\n",
    "\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixG, model_matrix_transformed.detach().cpu().numpy())\n",
    "print(\"\\nMean Squared Difference of Model and NCC (G):\", mean_squared_difference)\n",
    "        \n",
    "mean_squared_difference = imgcalc.get_MSE(dot_product_matrix, model_matrix_transformed.detach().cpu().numpy())\n",
    "print(\"Mean Squared Difference of Model and Pencorr (A'A):\", mean_squared_difference)\n",
    "     \n",
    "\n",
    "print(f\"\\nPrinting matrices after transformation\")\n",
    "print(f\"\\nMatrix G: {matrixG}\")\n",
    "print(f\"\\nMatrix A'A (Pencorr): {dot_product_matrix}, {dot_product_matrix.shape}\")\n",
    "print(f\"\\nMatrix A'A (Model): {model_matrix_transformed}, error: {error_model}\")\n",
    "\n",
    "print(f\"\\nPrinting vectors \")\n",
    "print(f\"\\nEmbedding of image {index1} for Pencorr (A'A): {matrixA[:,index1]}\")\n",
    "print(f\"Embedding of image {index1} for Model: {model_transformed[index1]}\")\n",
    "\n",
    "print(f\"\\nDifferences between vector embeddings\")\n",
    "mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], model_transformed[index1].detach().cpu().numpy())\n",
    "print(f\"Mean Squared Difference of Pencorr (A) and Transformed Model: {mean_squared_difference}\")\n",
    "\n",
    "# magnitude = np.linalg.norm(model_transformed[index1].detach().cpu().numpy())\n",
    "# print(\"magnitude: \", magnitude)\n",
    "# vector = model_transformed[index1].detach().cpu().numpy()/magnitude\n",
    "# print(\"vector after normalisation: \", vector)\n",
    "\n",
    "# mean_squared_difference = imgcalc.get_MSE(matrixA[:,index1], vector)\n",
    "# print(f\"Mean Squared Difference of Pencorr (A) and Normalised TransformedModel: {mean_squared_difference}\")\n",
    "# magnitude = np.linalg.norm(vector)\n",
    "# print(\"magnitude: \", magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81d30e-5f70-4fe8-9629-978e890b6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ MSE of embedding across dataset -----------------------------\n",
    "num = len(input_dataset)\n",
    "MSE_transformed = []\n",
    "MSE_original = []\n",
    "print(f\"\\nMSE between vector embeddings of Pencorr (A) and Model:\")   \n",
    "for i in range(num):\n",
    "    difference_squared = (matrixA[:,i] - model_transformed[i].detach().cpu().numpy()) ** 2\n",
    "    mean_squared_difference = np.sum(difference_squared) / difference_squared.size\n",
    "    #print(f\"Transformed matrix of Index {i}: {mean_squared_difference}\")\n",
    "    MSE_transformed.append(mean_squared_difference)\n",
    "    difference_squared = (matrixA[:,i] - model_vectors[i].detach().cpu().numpy()) ** 2\n",
    "    mean_squared_difference = np.sum(difference_squared) / difference_squared.size\n",
    "    #print(f\"Original matrix of Index {i}: {mean_squared_difference}\")\n",
    "    MSE_original.append(mean_squared_difference)\n",
    "    \n",
    "imgplt.plot_score_distribution(MSE_transformed, \"MSE of vector embeddings for Transformed Model\")\n",
    "imgplt.plot_score_distribution(MSE_original, \"MSE of vector embeddings for Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffc108-949d-4bdf-9ee3-515a7614065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ MSE of embedding across dimensions across dataset -----------------------------\n",
    "dimensions = [32, 64, 128, 192, 256, 384, 512]\n",
    "for dim in dimensions:\n",
    "    \n",
    "    embeddingType = f\"pencorr_{dim}\"\n",
    "    sampleName = f\"{imageType} {filters} {percentage} sample\"\n",
    "\n",
    "    sampleEstimator = SampleEstimator(sampleName=sampleName, embeddingType=embeddingType, imageProductType=imageProductType)\n",
    "    input_dataset = []\n",
    "    for img in testSample:\n",
    "        img_tensor = torch.from_numpy(np.array(img, dtype=np.float64))\n",
    "        img_tensor = img_tensor.unsqueeze(0).unsqueeze(0).to(device).double()\n",
    "        input_dataset.append(img_tensor)\n",
    "    input_dataset = [tensor.float() for tensor in input_dataset] \n",
    "    stacked_tensor = torch.stack(input_dataset)\n",
    "    input_dataset = stacked_tensor.cpu().numpy()      \n",
    "    input_dataset = [torch.tensor(data).to(device).float() for data in input_dataset]\n",
    "    num = len(input_dataset)\n",
    "    \n",
    "    model = models.SimpleCNN(dimensions=dim, padding_mode='circular').to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(os.path.abspath(\"../../VecRepV3\"), \"model\", \n",
    "                     f\"best_model_batch_greyscale_8bin_LPS_circular_{dim}d.pt\"), weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    matrixG = sampleEstimator.imageProductMatrix  \n",
    "    matrixA = sampleEstimator.embeddingMatrix \n",
    "    dot_product_matrix = np.dot(matrixA.T, matrixA)\n",
    "    \n",
    "    model_vectors = imgcalc.get_vector_embeddings(input_dataset, model)\n",
    "    model_transformed, error_model = imgcalc.get_orthogonal_transformation(model_vectors, matrixA) #transposed\n",
    "    model_transformed = [torch.tensor(row, dtype=torch.float32, device= device).unsqueeze(0).requires_grad_() for row in model_transformed]\n",
    "    \n",
    "    MSE_transformed = []\n",
    "    MSE_original = []\n",
    "    print(f\"\\nMSE between vector embeddings of dimension {dim} for Pencorr (A) and Model:\")   \n",
    "    for i in range(num):\n",
    "        difference_squared = (matrixA[:,i] - model_transformed[i].detach().cpu().numpy()) ** 2\n",
    "        mean_squared_difference = np.sum(difference_squared) / difference_squared.size\n",
    "        #print(f\"Transformed matrix of Index {i}: {mean_squared_difference}\")\n",
    "        MSE_transformed.append(mean_squared_difference)\n",
    "        difference_squared = (matrixA[:,i] - model_vectors[i].detach().cpu().numpy()) ** 2\n",
    "        mean_squared_difference = np.sum(difference_squared) / difference_squared.size\n",
    "        #print(f\"Original matrix of Index {i}: {mean_squared_difference}\")\n",
    "        MSE_original.append(mean_squared_difference)\n",
    "        \n",
    "    imgplt.plot_score_distribution(MSE_transformed, f\"MSE of Transformed Model\")\n",
    "    imgplt.plot_score_distribution(MSE_original, f\"MSE of Original Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d394bfd-1058-41a4-8e50-d369a2066769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3dfb6-12b7-4ee0-a86a-802a5128dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
