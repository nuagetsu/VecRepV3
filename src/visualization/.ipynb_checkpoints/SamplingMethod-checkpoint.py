import logging
import sys

import numpy as np
from matplotlib import pyplot as plt
from numpy.typing import NDArray
import pandas as pd
from line_profiler import profile

import src.data_processing.Utilities as utils
import src.helpers.FilepathUtils as fputils
import src.visualization.Metrics as metrics
from src.data_processing.SampleEstimator import SampleEstimator
from src.data_processing.SampleTester import SampleTester
from visualization import GraphEstimates

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)


def investigate_tester_rank_constraint(*, imageSet: NDArray, imageProductType: str, sampleSize: int, testSize: int,
                                       testPrefix: str, startingConstr: int, endingConstr: int, increment=1,
                                       specifiedKArr=None, plotFrob=True, weight="", trials=5):
    """
    :param specifiedKArr: value of k for the k neighbour score
    :param imageSet: Set of images used to the test and sample image sets. Currently, the training set takes from the front
    of the image set, and the test set takes from the tail of the image set.
    :param imageProductType: Image product type to investigate
    :param startingConstr: Starting lowest rank constraint to start the sweep inclusive
    :param endingConstr: Final largest rank constraint to end the sweep inclusive
    :param increment: Increment in the rank constraint sweep
    :param testPrefix: Used as a prefix to all the test names
    :param testSize: Size of the test set
    :param sampleSize: Size of the sample set
    :param plotFrob: If True, also plots frob error against rank
    :return: Uses the penncorr method to generate embeddings for different rank constraints
    Makes a graph of the average neighbour score against rank_constraint and
    average frobenius distance against rank_constraint
    Remember to use plt.show() to display plots

    Aims to answer the question: How does the rank constraint affect the error of the embeddings generated by penncorr?
    """
    if startingConstr >= endingConstr:
        raise ValueError("Starting rank constraint must be lower than ending constraint")
    if specifiedKArr is None:
        specifiedKArr = [5]

    aveFrobDistanceArr = []
    # A list of k neighbour plotting data, for each of the k in specified K array
    allAveNeighArr = [[] for i in specifiedKArr]
    rankConstraints = list(range(startingConstr, endingConstr + 1, increment))

    # For each rank in the sweep, generate a SampleTester and add its results to the array
    for rank in rankConstraints:
        logging.info("Investigating rank " + str(rank) + " of " + str(endingConstr))
        embType = "pencorr_" + str(rank)
        sampleName = testPrefix + "_sample_" + str(rank) + " of " + str(endingConstr)
        testName = testPrefix + "_test_" + str(rank) + " of " + str(endingConstr)

        aveNeighArr = [[] for i in specifiedKArr]
        frobDistanceArr = []
        # Trials for random sampling
        for j in range(trials):
            # Taking training and testing samples as random samples of the image set
            testSample, trainingSample = generate_random_sample(imageSet, testSize, sampleSize, seed=j)

            # Generating a sampleEstimator and SampleTester with the input parameters
            sampleEstimator = SampleEstimator(sampleName=sampleName + "_" + str(j), trainingImageSet=trainingSample, embeddingType=embType,
                                          imageProductType=imageProductType, weight=weight)
            sampleTester = SampleTester(testImages=testSample, sampleEstimator=sampleEstimator, testName=testName)


            # For each k to be investigated, append the respective k neighbour score
            for i in range(len(specifiedKArr)):
                k = specifiedKArr[i]
                aveNeighArr[i].append(metrics.get_mean_normed_k_neighbour_score(sampleTester.matrixG,
                                                                                   sampleTester.matrixGprime, k))
            frobDistanceArr.append(sampleTester.aveFrobDistance)
        for l in range(len(specifiedKArr)):
            allAveNeighArr[l].append(sum(aveNeighArr[l]) / len(aveNeighArr[l]))
        aveFrobDistanceArr.append(sum(frobDistanceArr) / len(frobDistanceArr))

    if plotFrob:
        rankFig, axArr = plt.subplots(1, len(specifiedKArr) + 1)
        frobAx = axArr[-1]
        neighAx = axArr[:-1]
        GraphEstimates.plot_frob_error_against_rank_constraint(frobAx, rankConstraints, aveFrobDistanceArr)
    else:
        rankFig, neighAx = plt.subplots(1, len(specifiedKArr))
    GraphEstimates.plot_error_against_rank_constraint(neighAx, rankConstraints, allAveNeighArr, specifiedKArr)

def investigate_training_size(*, imageSet: NDArray, imageProductType: str, embeddingType:str, startingTrainingSize: int,
                              endingTrainingSize: int, increment=50, testSize: int,
                              testPrefix: str, specifiedKArr=None, plotFrob=True, trials=5, weight=""):
    """
    :param imageSet: Set of images used to the test and sample image sets. Currently, the training set takes from the front
    of the image set, and the test set takes from the tail of the image set.
    :param imageProductType: Image product type to investigate
    :param embeddingType: Embedding type to investigate
    :param startingTrainingSize: Starting point for the sweep (inclusive)
    :param endingTrainingSize: Ending point for the sweep
    :param increment: Increment in the sweep
    :param testSize: Size of the test set
    :param testPrefix: Prefix to the test names
    :param specifiedKArr: Specified k to plot for the neighbour graphs
    :param plotFrob: If true, also plot the frob error against sample size
    :return: Generates a graph for the k neighbour score against the size of the test sample.
    For now, training set is obtained from the start of imageSet and testing from the end of imageSet.
    IMPORTANT: Hence as the sample size increases, the training set remains mostly the same, and new images are added
    """
    if startingTrainingSize > endingTrainingSize:
        raise ValueError("Starting sample size must be lower than ending")
    sampleSizeArr = list(range(startingTrainingSize, endingTrainingSize, increment))
    # A list of k neighbour plotting data, for each of the k in specified K array
    allAveNeighArr = [[] for i in specifiedKArr]
    aveFrobDistanceArr = []
    for sampleSizeTested in sampleSizeArr:
        logging.info("Investigating sample size " + str(sampleSizeTested) + " of " + str(endingTrainingSize))

        sampleName = testPrefix + "_sample_" + str(sampleSizeTested) + " of " + str(endingTrainingSize)
        testName = testPrefix + "_test_" + str(sampleSizeTested) + " of " + str(endingTrainingSize)
        frobDistanceArr = []
        aveNeighArr = [[] for i in specifiedKArr]

        # Trials for random sampling
        for j in range(trials):
            # Taking random training and testing samples
            testSample, trainingSample = generate_random_sample(imageSet, testSize, sampleSizeTested, seed=j)

            sampleEstimator = SampleEstimator(sampleName=sampleName + "_" + str(j), trainingImageSet=trainingSample,
                                            embeddingType=embeddingType, imageProductType=imageProductType, weight=weight)
            sampleTester = SampleTester(testImages=testSample, sampleEstimator=sampleEstimator, testName=testName)

            # For each value of k, add the result to the array
            for i in range(len(specifiedKArr)):
                k = specifiedKArr[i]
                aveNeighArr[i].append(metrics.get_mean_normed_k_neighbour_score(sampleTester.matrixG,
                                                                               sampleTester.matrixGprime, k))
            frobDistanceArr.append(sampleTester.aveFrobDistance)
        for l in range(len(specifiedKArr)):
            allAveNeighArr[l].append(sum(aveNeighArr[l]) / len(aveNeighArr[l]))
        aveFrobDistanceArr.append(sum(frobDistanceArr) / len(frobDistanceArr))

    if plotFrob:
        trainingFig, axArr = plt.subplots(1, len(specifiedKArr) + 1)
        frobAx = axArr[-1]
        neighAx = axArr[:-1]
        GraphEstimates.plot_frob_error_against_training_size(frobAx, sampleSizeArr, aveFrobDistanceArr)
    else:
        trainingFig, neighAx = plt.subplots(1, len(specifiedKArr))
    GraphEstimates.plot_error_against_sample_size(neighAx, sampleSizeArr, allAveNeighArr, specifiedKArr)

def generate_random_sample(imageSet: NDArray, testSampleSize: int, trainingSampleSize: int, seed=0):
    """
    Separates an image set into a test and training set randomly. Consider looking into n-folds method as an alternative
    :param imageSet: Image set to separate.
    :param testSampleSize: Size of the test sample.
    :param trainingSampleSize: Size of the training sample.
    :param seed: Seed for random sampling to ensure results are consistent and can be retested if needed.
    :return: Random test and training samples.
    """
    if testSampleSize + trainingSampleSize > len(imageSet):
        raise ValueError("Training and test sample size too large! Use size less than " + str(len(imageSet)) + ".")
    rng = np.random.default_rng(seed=500 + seed)
    trainingSample = rng.choice(imageSet, trainingSampleSize, replace=False)
    remaining = np.asarray([image for image in imageSet.tolist() if image not in trainingSample.tolist()])
    testSample = rng.choice(remaining, testSampleSize, replace=False)
    return testSample, trainingSample

def investigate_training_size_for_image_products(*, imageSet: NDArray, imageProductTypes: list, embeddingType:str,
                                                 startingTrainingSize: int, endingTrainingSize: int, increment=50,
                                                 testSize: int, testPrefix: str, specifiedKArr=None, plotFrob=False,
                                                 weights=None, trials=5):
    """
    :param imageSet: Set of images used to the test and sample image sets. Currently, the training set takes from the front
    of the image set, and the test set takes from the tail of the image set.
    :param imageProductTypes: Image product types to investigate
    :param embeddingType: Embedding type to investigate
    :param startingTrainingSize: Starting point for the sweep (inclusive)
    :param endingTrainingSize: Ending point for the sweep
    :param increment: Increment in the sweep
    :param testSize: Size of the test set
    :param testPrefix: Prefix to the test names
    :param specifiedKArr: Specified k to plot for the neighbour graphs
    :param plotFrob: If true, also plot the frob error against sample size
    :return: Generates a graph for the k neighbour score against the size of the test sample.
    For now, training set is obtained from the start of imageSet and testing from the end of imageSet.
    IMPORTANT: Hence as the sample size increases, the training set remains mostly the same, and new images are added
    """
    if startingTrainingSize > endingTrainingSize:
        raise ValueError("Starting sample size must be lower than ending")
    sampleSizeArr = list(range(startingTrainingSize, endingTrainingSize, increment))
    if weights is None:
        weights = ["" for image_product_type in imageProductTypes]
    # A list of k neighbour plotting data, for each of the k in specified K array
    allAveNeighArr = [[[] for imageProductType in imageProductTypes] for i in specifiedKArr]
    aveFrobDistanceArr = [[] for imageProductType in imageProductTypes]
    for sampleSizeTested in sampleSizeArr:
        logging.info("Investigating sample size " + str(sampleSizeTested) + " of " + str(endingTrainingSize))

        sampleName = testPrefix + "_sample_" + str(sampleSizeTested) + " of " + str(endingTrainingSize)
        testName = testPrefix + "_test_" + str(sampleSizeTested) + " of " + str(endingTrainingSize)

        frobDistanceArr = [[] for imageProductType in imageProductTypes]
        aveNeighArr = [[[] for imageProductType in imageProductTypes] for i in specifiedKArr]

        for index in range(len(imageProductTypes)):
            imageProductType = imageProductTypes[index]
            weight = weights[index]
            for j in range(trials):
                # Taking random training and testing samples
                testSample, trainingSample = generate_random_sample(imageSet, testSize, sampleSizeTested, seed=j)

                sampleEstimator = SampleEstimator(sampleName=sampleName + "_" + str(j), trainingImageSet=trainingSample,
                                                  embeddingType=embeddingType, imageProductType=imageProductType, weight=weight)
                sampleTester = SampleTester(testImages=testSample, sampleEstimator=sampleEstimator, testName=testName)

                # For each value of k, add the result to the array
                for i in range(len(specifiedKArr)):
                    k = specifiedKArr[i]
                    aveNeighArr[i][index].append(metrics.get_mean_normed_k_neighbour_score(sampleTester.matrixG,
                                                                                    sampleTester.matrixGprime, k))
                frobDistanceArr[index].append(sampleTester.aveFrobDistance)
            for l in range(len(specifiedKArr)):
                allAveNeighArr[l][index].append(sum(aveNeighArr[l][index]) / len(aveNeighArr[l][index]))
            aveFrobDistanceArr[index].append(sum(frobDistanceArr[index]) / len(frobDistanceArr[index]))

    if plotFrob:
        trainingFig, axArr = plt.subplots(1, len(specifiedKArr) + 1)
        frobAx = axArr[-1]
        neighAx = axArr[:-1]
        #GraphEstimates.plot_frob_error_against_training_size(frobAx, sampleSizeArr, aveFrobDistanceArr)
    else:
        trainingFig, neighAx = plt.subplots(1, len(specifiedKArr))
    if type(neighAx) is not list:
        neighAx = [neighAx]
    GraphEstimates.plot_error_against_sample_size_for_image_types(neighAx, sampleSizeArr, allAveNeighArr, specifiedKArr,
                                                                  imageProductTypes, weights)

def investigate_tester_rank_constraint_for_image_products(*, imageSet: NDArray, imageProductTypes: list, sampleSize: int,
                                                          testSize: int, testPrefix: str, startingConstr: int,
                                                          endingConstr: int, increment=1, specifiedKArr=None,
                                                          plotFrob=False, trials=5, weights=None, progressive=False,
                                                          embeddings=None):
    """
    :param specifiedKArr: value of k for the k neighbour score
    :param imageSet: Set of images used to the test and sample image sets. Currently, the training set takes from the front
    of the image set, and the test set takes from the tail of the image set.
    :param imageProductTypes: Image product types to investigate
    :param startingConstr: Starting lowest rank constraint to start the sweep inclusive
    :param endingConstr: Final largest rank constraint to end the sweep inclusive
    :param increment: Increment in the rank constraint sweep
    :param testPrefix: Used as a prefix to all the test names
    :param testSize: Size of the test set
    :param sampleSize: Size of the sample set
    :param plotFrob: If True, also plots frob error against rank
    :return: Uses the penncorr method to generate embeddings for different rank constraints
    Makes a graph of the average neighbour score against rank_constraint and
    average frobenius distance against rank_constraint
    Remember to use plt.show() to display plots

    Aims to answer the question: How does the rank constraint affect the error of the embeddings generated by penncorr?
    """
    if startingConstr >= endingConstr:
        raise ValueError("Starting rank constraint must be lower than ending constraint")
    if specifiedKArr is None:
        specifiedKArr = [5]
    if weights is None:
        weights = ["" for image_product_type in imageProductTypes]
    if embeddings is None:
        embeddings = ["pencorr" for image_product_type in imageProductTypes]

    aveFrobDistanceArr = [[] for imageProductType in imageProductTypes]
    # A list of k neighbour plotting data, for each of the k in specified K array
    allAveNeighArr = [[[] for imageProductType in imageProductTypes] for i in specifiedKArr]
    rankConstraints = list(range(startingConstr, endingConstr + 1, increment))
    if progressive:
        rankConstraints = metrics.get_progressive_range(startingConstr, endingConstr + 1, increment)


    # For each rank in the sweep, generate a SampleTester and add its results to the array
    for rank in rankConstraints:
        logging.info("Investigating rank " + str(rank) + " of " + str(endingConstr))

        for index in range(len(imageProductTypes)):
            imageProductType = imageProductTypes[index]
            embType = embeddings[index] + "_" + str(rank)
            weight = weights[index]
            sampleName = testPrefix + "_sample_" + str(rank) + " of " + str(endingConstr)
            testName = testPrefix + "_test_" + str(rank) + " of " + str(endingConstr)

            aveNeighArr = [[] for i in specifiedKArr]
            frobDistanceArr = []
            for j in range(trials):
                # Taking training and testing samples as random samples of the image set
                testSample, trainingSample = generate_random_sample(imageSet, testSize, sampleSize, seed=j)

                # Generating a sampleEstimator and SampleTester with the input parameters
                sampleEstimator = SampleEstimator(sampleName=sampleName + "_sample_" + str(j), trainingImageSet=trainingSample, embeddingType=embType,
                                                  imageProductType=imageProductType, weight=weight)
                sampleTester = SampleTester(testImages=testSample, sampleEstimator=sampleEstimator, testName=testName)


                # For each k to be investigated, append the respective k neighbour score
                for i in range(len(specifiedKArr)):
                    k = specifiedKArr[i]
                    aveNeighArr[i].append(metrics.get_mean_normed_k_neighbour_score(sampleTester.matrixG,
                                                                                    sampleTester.matrixGprime, k))
                frobDistanceArr.append(sampleTester.aveFrobDistance)
            for l in range(len(specifiedKArr)):
                allAveNeighArr[l][index].append(sum(aveNeighArr[l]) / len(aveNeighArr[l]))
            aveFrobDistanceArr[index].append(sum(frobDistanceArr) / len(frobDistanceArr))

    if plotFrob:
        rankFig, axArr = plt.subplots(1, len(specifiedKArr) + 1)
        frobAx = axArr[-1]
        neighAx = axArr[:-1]
        GraphEstimates.plot_frob_error_against_rank_constraint(frobAx, rankConstraints, aveFrobDistanceArr)
    else:
        rankFig, neighAx = plt.subplots(1, len(specifiedKArr))
    if type(neighAx) is not list:
        neighAx = [neighAx]
    GraphEstimates.plot_error_against_rank_constraint_for_image_products(neighAx, rankConstraints, allAveNeighArr, specifiedKArr,
                                                                         imageProducts=imageProductTypes, weights=weights)

def investigate_sample_and_test_sets(*, trainingSet: str, testSet: str, trainingSize: int, testSize: int,
                                     imageProductTypes: list, testPrefix: str,
                                     startingConstr: int, endingConstr: int, increment=1, specifiedKArr=None, trials=5,
                                     weights=None, progressive=False, embeddings=None, filters=None, plotFrob=False):
    """
    Function to investigate sweeping the rank constraint on a training and test set.
    :param trainingSet: Training set to use.
    :param testSet: Test set to use.
    :param trainingSize: Size of the training set.
    :param testSize: Size of the test set.
    :param imageProductTypes: Image product type to use.
    :param testPrefix: Name of the folder to save the test data.
    :param startingConstr: Starting rank constraint.
    :param endingConstr: Ending rank constraint.
    :param increment: Rank constraint increment.
    :param specifiedKArr: Specified values of k to use.
    :param trials: Number of random trials to conduct.
    :param weights: Weighting types to use for each set.
    :param progressive: Whether to use progressive increase in rank constraint. Useful when training set is very large.
    :param embeddings: Embedding type to use.
    :param filters: Filters to use.
    :param plotFrob: Whether to plot the Frobenius error. May or may not be currently broken.
    :return: Graph of k-scores against rank constraints.
    """
    if startingConstr >= endingConstr:
        raise ValueError("Starting rank constraint must be lower than ending constraint")
    if specifiedKArr is None:
        specifiedKArr = [5]
    if weights is None:
        weights = ["" for image_product_type in imageProductTypes]
    if embeddings is None:
        embeddings = ["pencorr" for image_product_type in imageProductTypes]
    if filters is None:
        filters = []

    training_set_filepath = fputils.get_image_set_filepath(trainingSet, filters)
    full_training_image_set = utils.generate_filtered_image_set(trainingSet, filters, training_set_filepath)

    if trainingSet == testSet:
        sameSet = True
    else:
        sameSet = False
        test_set_filepath = fputils.get_image_set_filepath(testSet, filters)
        full_test_image_set = utils.generate_filtered_image_set(testSet, filters, test_set_filepath)


    aveFrobDistanceArr = [[] for imageProductType in imageProductTypes]
    # A list of k neighbour plotting data, for each of the k in specified K array
    allAveNeighArr = [[[] for imageProductType in imageProductTypes] for i in specifiedKArr]
    rankConstraints = list(range(startingConstr, endingConstr + 1, increment))
    if progressive:
        rankConstraints = metrics.get_progressive_range(startingConstr, endingConstr + 1, increment)


    # For each rank in the sweep, generate a SampleTester and add its results to the array
    for rank in rankConstraints:
        logging.info("Investigating rank " + str(rank) + " of " + str(endingConstr))

        for index in range(len(imageProductTypes)):
            imageProductType = imageProductTypes[index]
            embType = embeddings[index] + "_" + str(rank)
            weight = weights[index]
            sampleName = testPrefix + "_sample_" + str(rank) + " of " + str(endingConstr)
            testName = testPrefix + "_test_" + str(rank) + " of " + str(endingConstr)

            aveNeighArr = [[] for i in specifiedKArr]
            frobDistanceArr = []
            for j in range(trials):
                # Taking training and testing samples as random samples of the image set
                if sameSet:
                    testSample, trainingSample = generate_random_sample(full_training_image_set, testSize, trainingSize, seed=j)
                else:
                    disregard, trainingSample = generate_random_sample(full_training_image_set, 0, trainingSize, seed=j)
                    testSample, disregard = generate_random_sample(full_test_image_set, testSize, 0, seed=j)

                # Generating a sampleEstimator and SampleTester with the input parameters
                sampleEstimator = SampleEstimator(sampleName=sampleName + "_sample_" + str(j), trainingImageSet=trainingSample, embeddingType=embType,
                                                  imageProductType=imageProductType, weight=weight)
                sampleTester = SampleTester(testImages=testSample, sampleEstimator=sampleEstimator, testName=testName)


                # For each k to be investigated, append the respective k neighbour score
                for i in range(len(specifiedKArr)):
                    k = specifiedKArr[i]
                    aveNeighArr[i].append(metrics.get_mean_normed_k_neighbour_score(sampleTester.matrixG,
                                                                                    sampleTester.matrixGprime, k))
                frobDistanceArr.append(sampleTester.aveFrobDistance)
            for l in range(len(specifiedKArr)):
                allAveNeighArr[l][index].append(sum(aveNeighArr[l]) / len(aveNeighArr[l]))
            aveFrobDistanceArr[index].append(sum(frobDistanceArr) / len(frobDistanceArr))

    if plotFrob:
        rankFig, axArr = plt.subplots(1, len(specifiedKArr) + 1)
        frobAx = axArr[-1]
        neighAx = axArr[:-1]
        GraphEstimates.plot_frob_error_against_rank_constraint(frobAx, rankConstraints, aveFrobDistanceArr)
    else:
        rankFig, neighAx = plt.subplots(1, len(specifiedKArr))
    if type(neighAx) is not list:
        neighAx = [neighAx]
    GraphEstimates.plot_error_against_rank_constraint_for_image_products(neighAx, rankConstraints, allAveNeighArr, specifiedKArr,
                                                                         imageProducts=imageProductTypes, weights=weights)


@profile
def investigate_sample_plateau_rank(*, training_sets: list, test_set: str, test_size: int,
                                    image_product: str, test_prefix: str, embedding: str, weight: str,
                                    filters: list, k=5, trials=5, prox=3,):
    """
    Function to investigate the plateau rank on sampling method.
    :param training_sets: Training sets to analyze. May or may not contain the test set which will be separated first
    :param test_set: Test set to analyze.
    :param test_size: Size of the test set. If smaller than the test set, takes a random sample.
    :param image_product: Image product type used.
    :param test_prefix: Name of the test sample folder to save the data.
    :param embedding: Embedding method to use.
    :param weight: Weighting matrix to be used. Currently unavailable for server/python pencorr testing.
    :param filters: Filters to be used.
    :param k: k value for k-score calculated
    :param trials: number of random trials to conduct. Average values of data will be returned.
    :param prox: Proximity of binary search to find goal/minimum k-score. Search terminates when search
    range is within this proximity.
    :return: Graph of goal/minimum rank constraint at which a k-score is achieved against the
    """

    if filters is None:
        filters = []

    data = {"Training Set": training_sets, "Test Set": [test_set for i in training_sets], "Image Size": [],
            "Image Products": [image_product for i in training_sets], "Embeddings": [embedding for i in training_sets],
            "Weights": [weight for i in training_sets],
            "K_scores": [], "Training Set Size": [], "Non_zero": [], "Plateau Rank": []}
    set_data = {"K_scores": {}, "Training Set Size": {}, "Non_zero": {}, "Plateau Rank": {}}

    for trial in range(trials):
        test_set_filepath = fputils.get_image_set_filepath(test_set, filters)
        full_test_image_set = utils.generate_filtered_image_set(test_set, filters, test_set_filepath)
        data["Image Size"] = [len(full_test_image_set[0]) for i in training_sets]
        test_images = generate_random_sample(full_test_image_set, test_size, 0, seed=trial)[0]

        for index, training_set in enumerate(training_sets):

            training_set_filepath = fputils.get_image_set_filepath(training_set, filters)
            full_training_image_set = utils.generate_filtered_image_set(training_set, filters,
                                                                        training_set_filepath)
            training_image_set = [image for image in full_training_image_set.tolist() if
                                  image not in test_images.tolist()]
            training_image_set = np.asarray(training_image_set)
            training_size = len(training_image_set)

            if training_set not in set_data["Training Set Size"]:
                set_data["Training Set Size"][training_set] = []
            set_data["Training Set Size"][training_set].append(len(training_image_set[0]))

            # Loop variables
            high = training_size
            low = 0
            selected_rank = high
            goal_k_score = 2
            iterations = 0
            same_rank = 0
            score_change = False
            max_score_rank = []

            while high - low > prox:
                logging.info("Starting iteration " + str(iterations + 1))
                selected_embedding = embedding + "_" + str(selected_rank)
                sampleName = test_prefix + "_" + training_set + "_constraint_" + str(selected_rank)
                sampleEstimator = SampleEstimator(sampleName=sampleName + "_sample_" + str(trial),
                                                  trainingImageSet=training_image_set, embeddingType=selected_embedding,
                                                  imageProductType=image_product, weight=weight)
                testName = test_prefix + "_test"
                sample_tester = SampleTester(testImages=test_images, sampleEstimator=sampleEstimator, testName=testName)

                k_score = metrics.get_mean_normed_k_neighbour_score(sample_tester.matrixG, sample_tester.matrixGprime, k)
                if not score_change:
                    # k_score is the same as previous tested rank constraint
                    if iterations == 0:
                        # First iteration, no rank constraint placed
                        max_k_score = k_score  # k_score value where plateau occurs
                        if training_set not in set_data["Max_K_scores"]:
                            set_data["K_scores"][training_set] = []
                        set_data["K_scores"][training_set].append(max_k_score)
                        nonzero = np.count_nonzero(np.array([np.max(b) - np.min(b) for b in sampleEstimator.embeddingMatrix]))
                        if training_set not in set_data["Non_zero"]:
                            set_data["Non_zero"][training_set] = []
                        set_data["Non_zero"][training_set].append(nonzero)
                        # Number of nonzero eigenvalues after pencorr acts as upper bound
                        high = training_size
                        low = training_size // 2
                    elif k_score == max_k_score:
                        # Not first iteration, k_score has yet to change. Continue lowering rank constraint.
                        high = low
                        low = high // 2
                    else:
                        # Not first iteration, k_score has changed. Begin looking for plateau rank.
                        score_change = True
                        low = ((high - low) // 2) + low
                elif k_score != max_k_score:
                    # Before plateau area. Raise rank constraint.
                    low = ((high - low) // 2) + low
                    max_score_rank = []
                    same_rank = 0
                elif same_rank == 2:
                    # Successively within plateau area. Break loop
                    high = max_score_rank[0]
                    iterations += 1
                    logging.info("Finishing iteration" + str(iterations))
                    break
                else:
                    # Within plateau area. Raise rank constraint slowly in case plateau rank not yet reached.
                    max_score_rank.append(low)
                    diff = (high - low) // 4
                    low += diff
                    same_rank += 1
                logging.info("k score is " + str(k_score))

                # Test next iteration at low estimate
                selected_rank = low
                iterations += 1
                logging.info("Finishing iteration" + str(iterations))
                logging.info("Next Rank " + str(low))

            # Once loop ends, save high estimate plateau rank for currently tested image set
            logging.info("Plateau rank " + str(high))
            if training_set not in set_data["Plateau Rank"]:
                set_data["Plateau Rank"][training_set] = []
            set_data["Plateau Rank"][training_set].append(high)

        for param in set_data:
            for training_set in set_data[param]:
                set_data[param][training_set] = sum(set_data[param][training_set]) / len(set_data[param][training_set])
                data[param].append(set_data[param][training_set])
    df = pd.DataFrame(data)
    return df


def investigate_goal_k_score_rank(*, training_sets: list, test_set: str, test_size: int,
                                  image_product_type: str, test_prefix: str,
                                  k=5, trials=5, prox=3,
                                  weight=None, embedding=None, filters=None):
    """
    WIP function for investigating the minimum rank at which the Sampling method achieves a k-score
    :param training_sets: Training sets to analyze. May or may not contain the test set which will be separated first
    :param test_set: Test set to analyze.
    :param test_size: Size of the test set. If smaller than the test set, takes a random sample.
    :param image_product_type: Image product type used.
    :param test_prefix: Name of the test sample folder to save the data.
    :param k: k value for k-score calculated
    :param trials: number of random trials to conduct. Average values of data will be returned.
    :param prox: Proximity of binary search to find goal/minimum k-score. Search terminates when search
    range is within this proximity.
    :param weight: Weighting matrix to be used. Currently unavailable for server/python pencorr testing.
    :param embedding: Embedding method to use.
    :param filters: Filters to be used.
    :return: Graph of goal/minimum rank constraint at which a k-score is achieved against the
    """
    # Idea: For a set of images to be used as a test set, if remove it from all training sets and use sample estimation,
    # At what k_score do you reach certain accuracy?
    if weight is None:
        weight = ""
    if embedding is None:
        embedding = "pencorr_python"
    if filters is None:
        filters = ["unique"]

    data = {"Training Set": training_sets, "Test Set": [test_set for training_set in training_sets],
            "Image Products": [image_product_type for training_Set in training_sets],
            "Embeddings": [embedding for training_set in training_sets],
            "Weights": [weight for training_set in training_sets],
            "Max_K_scores": [], "Training Set Size": [], "Non_zero": [], "Goal Rank": []}
    set_data = {"Max_K_scores": {}, "Training Set Size": {}, "Non_zero": {}, "Goal Rank": {}}

    for trial in range(trials):
        test_set_filepath = fputils.get_image_set_filepath(test_set, filters)
        full_test_image_set = utils.generate_filtered_image_set(test_set, filters, test_set_filepath)
        test_images = generate_random_sample(full_test_image_set, test_size, 0, seed=trial)[0]

        for index, training_set in enumerate(training_sets):

            training_set_filepath = fputils.get_image_set_filepath(training_set, filters)
            full_training_image_set = utils.generate_filtered_image_set(training_set, filters, training_set_filepath)
            training_image_set = [image for image in full_training_image_set.tolist() if image not in test_images.tolist()]
            training_image_set = np.asarray(training_image_set)
            training_size = len(training_image_set)

            if training_set not in set_data["Training Set Size"]:
                set_data["Training Set Size"][training_set] = []
            set_data["Training Set Size"][training_set].append(len(training_image_set[0]))

            # Loop variables
            high = training_size
            low = 0
            selected_rank = high
            goal_k_score = 2
            iterations = 0
            same_rank = 0
            score_change = False
            max_score_rank = []

            if index == 0:
                # Begin binary search. Search ends when high estimate is within "prox" of low estimate
                # For the first set, do plateau rank search and look for when max k_score is achieved
                while high - low > prox:
                    logging.info("Starting iteration " + str(iterations + 1))
                    selected_embedding = embedding + "_" + str(selected_rank)
                    sample_name = test_prefix + "_" + training_set + "_constraint_" + str(selected_rank)
                    sample_estimator = SampleEstimator(sampleName=sample_name, trainingImageSet=training_image_set,
                                                       embeddingType=selected_embedding, imageProductType=image_product_type,
                                                       weight=weight)

                    test_name = test_prefix + "_test"
                    sample_tester = SampleTester(testImages=test_images, sampleEstimator=sample_estimator,
                                                 testName=test_name)

                    k_score = metrics.get_mean_normed_k_neighbour_score(sample_tester.matrixG,
                                                                        sample_tester.matrixGprime, k)

                    if not score_change:
                        # k_score is the same as previous tested rank constraint
                        if iterations == 0:
                            # First iteration, no rank constraint placed
                            goal_k_score = k_score  # k_score value where plateau occurs
                            if training_set not in set_data["Max_K_scores"]:
                                set_data["Max_K_scores"][training_set] = []
                            set_data["Max_K_scores"][training_set].append(goal_k_score)
                            nonzero = np.count_nonzero(
                                np.array([np.max(b) - np.min(b) for b in sample_estimator.embeddingMatrix]))
                            if training_set not in set_data["Non_zero"]:
                                set_data["Non_zero"][training_set] = []
                            set_data["Non_zero"][training_set].append(nonzero)  # Number of nonzero eigenvalues after pencorr acts as upper
                            high = training_size
                            low = training_size // 2
                        elif k_score == goal_k_score:
                            # Not first iteration, k_score has yet to change. Continue lowering rank constraint.
                            high = low
                            low = high // 2
                        else:
                            # Not first iteration, k_score has changed. Begin looking for plateau rank.
                            score_change = True
                            low = ((high - low) // 2) + low
                    elif k_score != goal_k_score:
                        # Before plateau area. Raise rank constraint.
                        low = ((high - low) // 2) + low
                        max_score_rank = []
                        same_rank = 0
                    elif same_rank == 2:
                        # Successively within plateau area. Break loop
                        high = max_score_rank[0]
                        iterations += 1
                        logging.info("Finishing iteration" + str(iterations))
                        break
                    else:
                        # Within plateau area. Raise rank constraint slowly in case plateau rank not yet reached.
                        max_score_rank.append(low)
                        diff = (high - low) // 4
                        low += diff
                        same_rank += 1
                    logging.info("k score is " + str(k_score))

                    # Test next iteration at low estimate
                    selected_rank = low
                    iterations += 1
                    logging.info("Finishing iteration" + str(iterations))
                    logging.info("Next Rank " + str(low))

                # Once loop ends, save high estimate plateau rank for currently tested image set
                logging.info("Goal rank " + str(high))
                if training_set not in set_data["Goal Rank"]:
                    set_data["Goal Rank"][training_set] = []
                set_data["Goal Rank"][training_set].append(high)
            else:
                while high - low > prox:
                    logging.info("Starting iteration " + str(iterations + 1))
                    selected_embedding = embedding + "_" + str(selected_rank)
                    sample_name = test_prefix + "_" + training_set + "_constraint_" + str(selected_rank)
                    sample_estimator = SampleEstimator(sampleName=sample_name, trainingImageSet=training_image_set,
                                                       embeddingType=selected_embedding, imageProductType=image_product_type,
                                                       weight=weight)

                    test_name = test_prefix + "_test"
                    sample_tester = SampleTester(testImages=test_images, sampleEstimator=sample_estimator,
                                                 testName=test_name)

                    k_score = metrics.get_mean_normed_k_neighbour_score(sample_tester.matrixG,
                                                                        sample_tester.matrixGprime, k)

                    if not score_change:
                        # k_score is the same as previous tested rank constraint
                        if iterations == 0:
                            # First iteration, no rank constraint placed
                            max_k_score = k_score  # k_score value where plateau occurs
                            if training_set not in set_data["Max_K_scores"]:
                                set_data["Max_K_scores"][training_set] = []
                            set_data["Max_K_scores"][training_set].append(max_k_score)
                            nonzero = np.count_nonzero(
                                np.array([np.max(b) - np.min(b) for b in sample_estimator.embeddingMatrix]))
                            if training_set not in set_data["Non_zero"]:
                                set_data["Non_zero"][training_set] = []
                            set_data["Non_zero"][training_set].append(
                                nonzero)  # Number of nonzero eigenvalues after pencorr acts as upper
                            high = training_size
                            low = training_size // 2
                            if max_k_score < goal_k_score:
                                logging.info("Goal k-score cannot be reached!")
                                break
                        elif k_score > goal_k_score:
                            # Not first iteration, k_score still higher. Continue lowering rank constraint.
                            high = low
                            low = high // 2
                        elif k_score == goal_k_score:
                            high = low
                            break
                        else:
                            # Not first iteration, k_score is lower. Begin looking for goal rank.
                            score_change = True
                            low = ((high - low) // 2) + low
                    elif k_score < goal_k_score:
                        # Before goal score. Raise rank constraint.
                        low = ((high - low) // 2) + low
                    elif k_score == goal_k_score:
                        # Successively found goal k-score. Break loop
                        high = low
                        iterations += 1
                        logging.info("Finishing iteration" + str(iterations))
                        break
                    else:
                        # Higher than goal. Raise rank constraint slowly in case plateau rank not yet reached.
                        high = low
                        low += 3 * (low // 4)
                    logging.info("k score is " + str(k_score))

                    # Test next iteration at low estimate
                    selected_rank = low
                    iterations += 1
                    logging.info("Finishing iteration" + str(iterations))
                    logging.info("Next Rank " + str(low))

                # Once loop ends, save high estimate plateau rank for currently tested image set
                logging.info("Goal rank " + str((high + low) / 2))
                if training_set not in set_data["Goal Rank"]:
                    set_data["Goal Rank"][training_set] = []
                set_data["Goal Rank"][training_set].append(high)

        for param in set_data:
            for training_set in set_data[param]:
                set_data[param][training_set] = sum(set_data[param][training_set]) / len(set_data[param][training_set])
                data[param].append(set_data[param][training_set])

    df = pd.DataFrame(data)
    return df
